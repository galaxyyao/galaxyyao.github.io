{"meta":{"title":"Galaxy","subtitle":"姚皓的技术博客-一杯咖啡，一首音乐，一台电脑，编程","description":"姚皓的技术博客","author":"姚皓(Galaxy Yao)","url":"https://galaxyyao.github.io","root":"/"},"pages":[{"title":"关于我","date":"2024-08-30T01:45:35.940Z","updated":"2024-08-30T01:45:35.940Z","comments":true,"path":"about/index.html","permalink":"https://galaxyyao.github.io/about/index.html","excerpt":"","text":"关于博客我是姚皓。这个博客命名为Galaxy，是因为我喜欢星空。还在大学时，我心中理想的工作与生活，就是一杯咖啡、一首音乐、一台电脑、编程。一晃20年过去了。不知道接下来的20年，是否依然有幸能继续这样的生活？有兴趣沟通的话可以通过邮件联系我：galaxyyao[at]live.com 个人简介15+年工作经验。当前在阿里云技术服务团队。个人经历： 10+年开发&amp;架构。 多年金融行业经验。 多年项目管理经验和技术管理经验。 技术支持。技术方面擅长Java、数据库、大数据、DevOps和云原生，也了解Python和Shell。在阿里的前四年的工作内容除了交付外，也负责云巧组装式开发的售前、生态赋能运营、数据运营等。 Hello World(2019-03-21)陈皓在极客时间的专栏中提出过一个ARTS运动： Algorithm：每周至少做一道leetcode算法题 Review：每周阅读并点评至少一篇英文技术文章 Tip：每周学习至少一个技术技巧 Share：每周分享一篇有观点和思考的技术文章 我曾经在cnblog上写过博客，也在前司里做过一个名为技术沙龙的专栏，每2周写一个主题。虽然保持更新不轻松，但回想起来，有技术输出的那段时间也是技术上成长最快的时期。虽然大多数内容是蜻蜓点水，但也会尽量使博客的内容不是简单的copy &amp; paste。希望这个博客里有一点内容能对你有帮助，save your days。 Resume(2024-08-15)从上一次中止博客更新，已经有整整4年了。上一次更新是2020年8月。而现在，我即将迎来入职4周年。在这4年里，肯定不是没有成长。不如说，在阿里度过的这4年，体感时间远远超过4年。我给自己起的花名，期望自己能够做不一样的事情。从结果来看，也是实现了我自己的目标。但不得不承认离我原先的立身之本：技术，有些距离远。打开PPT和Word的时间更多。但由于阿里云的方向发生了变化，为了“活下去”，经过考虑后，还是重新选择了方向，更加贴近“云”的方向。过去的这些经验对未来的工作没有直接的帮助。基本是从零开始。不过，那没有虚度的4年的经验依然能成为我的养料。我也相信自己依然保持快速学习能力。 回看博客的第一篇，有些感慨提出ARTS的陈皓的故去。按照自己的技术方向，调整一下ARTS的目标： Algorithm：每周至少写一段代码，保持编程手感 Review：每周阅读并点评至少一篇英文技术文章 Tip：每周学习至少一个技术技巧 Share：每周分享一篇有观点和思考的技术文章 回归技术立命后，重启这个博客的更新。记录一下这个重新起航的时刻。"},{"title":"categories","date":"2024-08-28T11:49:14.098Z","updated":"2024-08-28T11:49:14.098Z","comments":true,"path":"categories/index.html","permalink":"https://galaxyyao.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2024-08-28T11:49:26.420Z","updated":"2024-08-28T11:49:26.420Z","comments":true,"path":"tags/index.html","permalink":"https://galaxyyao.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"DNS常用知识点","slug":"DNS常用知识点","date":"2025-06-11T14:03:38.000Z","updated":"2025-06-11T14:03:38.124Z","comments":true,"path":"2025/06/11/DNS常用知识点/","permalink":"https://galaxyyao.github.io/2025/06/11/DNS%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%E7%82%B9/","excerpt":"1. 层级划分 com：顶级域名 aliyuncs.com：二级域名 oss-cn-shanghai.aliyuncs.com：三级域名 xxx.oss-cn-shanghai.aliyuncs.com：四级域名 1.1 本章节Q&amp;AQ：是否可以有五级或更多层级域名？A：DNS 协议本身不限制域名层级数量。理论上可以无限添加子域名层级。但有限制： ICANN 规定：域名的总长度不能超过 253 个字符。 兼容性考量：需考虑客户端和服务器的兼容性 管理复杂性：多级域名需要逐层配置 DNS 记录（如 A、CNAME 等），管理成本高 缓存与性能：每增加一层域名，DNS 查询次数可能增加，影响解析效率。 Q：多级域名如何配置A：配置多级域名：为每一层级的域名单独配置 DNS 记录（如 A、CNAME）。如： 123level1.com. IN A 192.0.2.1level2.level1.com. IN CNAME level1.com.level3.level2.level1.com. IN A 192.0.2.2 2. 分布式系统","text":"1. 层级划分 com：顶级域名 aliyuncs.com：二级域名 oss-cn-shanghai.aliyuncs.com：三级域名 xxx.oss-cn-shanghai.aliyuncs.com：四级域名 1.1 本章节Q&amp;AQ：是否可以有五级或更多层级域名？A：DNS 协议本身不限制域名层级数量。理论上可以无限添加子域名层级。但有限制： ICANN 规定：域名的总长度不能超过 253 个字符。 兼容性考量：需考虑客户端和服务器的兼容性 管理复杂性：多级域名需要逐层配置 DNS 记录（如 A、CNAME 等），管理成本高 缓存与性能：每增加一层域名，DNS 查询次数可能增加，影响解析效率。 Q：多级域名如何配置A：配置多级域名：为每一层级的域名单独配置 DNS 记录（如 A、CNAME）。如： 123level1.com. IN A 192.0.2.1level2.level1.com. IN CNAME level1.com.level3.level2.level1.com. IN A 192.0.2.2 2. 分布式系统单台服务器承受不了全网的带宽，所以DNS使用分布在各地的域名服务器分区委派。 根域名服务器：13组IPV4根域名服务器（A~M） 顶级域名服务器：负责管理在该顶级域名服务器注册的二级域名 权威域名服务器：负责管理某个区的域名 2.1 LocalDNS不属于上述等级结构： 本地域名服务器LocalDNS（简称LDNS）：起到代理作用，转发DNS请求报文。需要直接配置在需要域名解析的主机中。 LDNS通常是由用户的网络服务提供商或者所在网络的管理员来管理和配置的： 在企业内部网络中，LocalDNS服务器可能由企业自身的IT部门维护 在家庭网络环境中，LocalDNS服务器地址通常由ISP分配，一般都是通过DHCP协议动态分配给用户的路由器或直接分配给用户的计算机。ISP分配时通常为本地运营商的LDNS，同时也支持手动设置。 假设LDNS配置错误或者与CIP相距较远，就会存在访问失败或者访问时间长的问题。 2.2 根域名服务器分布 美国主导：8组，A、B、C、D、E、F、H、J 根服务器由美国机构运营。 欧洲参与：3组，G、I、K 根服务器由德国、英国、荷兰运营。 亚洲和中立国：2组，M 根服务器位于日本，L 根服务器位于瑞士（由 ICANN 管理）。 2.3 相关机构 IANA（互联网数字分配机构）：负责管理根服务器的 IP 地址分配和协调。 ICANN（互联网名称与数字地址分配机构）：制定域名系统规则，并授权Verisign和 CNNIC等注册局管理特定顶级域名。 注册局：顶级域管理者，注册局主要有VeriSign(负责.com .net .cc等) 和 CNNIC(负责.cn)。 2.4 顶级域名服务器的所有者大部分顶级域名由盈利性公司运营，通过域名注册和续费盈利。 .com&#x2F;.net：Verisign，商业公司 .org：Public Interest Registry (PIR)，非营利组织 .cn：CNNIC（中国互联网络信息中心），政府自主的非营利组织 .top：由江苏邦宁科技有限公司负责运营管理的一个新通用顶级域名 2.5 知名公共LDNS 阿里 AIiDNS：223.5.5.5, 223.6.6.6 Google DNS：8.8.8.8, 8.8.4.4 Cloudflare：1.1.1.1 114 DNS：114.114.114.114, 114.114.115.115 百度 BaiduDNS：180.76.76.76 DNSPod DNS+：119.29.29.29, 182.254.116.116 CNNIC SDNS：1.2.4.8, 210.2.4.8 oneDNS：112.124.47.27, 114.215.126.16 DNS派电信&#x2F;移动&#x2F;铁通：101.226.4.6,218.30.118.6 DNS派联通：123.125.81.6,140.207.198.6 OpenDNS：208.67.222.222, 208.67.220.220 2.6 本章节Q&amp;AQ：根节点服务器都在国外，会不会访问很慢？A：每组根服务器通过 Anycast 技术 全球部署多个镜像站点，实现高可用性和低延迟。Anycast指的是每个根服务器的镜像站点共享相同的 IP 地址，客户端会自动连接到最近的镜像节点。中国没有运营根服务器，但通过 Anycast 镜像 接入全球根服务器网络。境内设有多个根服务器镜像节点（如 A、F、J 根服务器的镜像）。 Q：根域名服务器能不能扩更多？A：不能。源于早期 DNS 协议设计时的技术约束和历史背景。DNS 响应数据包的大小不能超过 512 字节（除非启用 EDNS0 扩展）。根服务器的 IP 地址需要通过 DNS 响应返回给客户端。每个根服务器的信息需要占用一定字节数。 A 记录格式（IPv4 地址）： 域名（如 a.root-servers.net）：压缩后约 2 字节。 类型（A 记录）：2 字节。 类（IN）：2 字节。 TTL（生存时间）：4 字节。 数据长度（4 字节的 IPv4 地址）：2 字节。 IPv4 地址：4 字节。总长度：约 16 字节&#x2F;条。512 字节 &#x2F; 16 字节 ≈ 32 条记录。由于需要包含其他信息（如 NS 记录、额外字段），实际最多只能容纳 13 条 A 记录。 3. DNS信息3.1 DNS常用记录 A记录：IPV4 AAAA记录：IPV6 CNAME：创建域名别名 NS记录：指定该域名的权威DNS服务器，说明哪些DNS服务器负责解析该域名的DNS记录 TXT记录：证明域名所有权 3.2 CNAME记录的常用场景 多域名&#x2F;子域管理：CNAME记录允许将多个域名或子域指向同一目标域名，简化管理。例如，将www.example.com、blog.example.com等子域统一指向主域名example.com CDN加速：CDN中，CNAME记录用于将用户域名（如cdn.example.com）指向CDN服务商分配的加速域名（如cdn.provider.com） 流量管理与负载均衡：CNAME记录可用于分发流量至不同服务器集群，实现负载均衡。 A&#x2F;B测试与灰度发布:通过CNAME记录将特定流量段指向不同版本的网站（如test.example.com和prod.example.com），支持A&#x2F;B测试或逐步发布新功能 灾备：在服务器故障时，CNAME记录可快速将流量重定向至备份服务器，确保高可用性。例如，将api.example.com指向主服务器域名，故障时切换至备用域名 3.3 域名的DNS服务器通常指的是权威域名服务器。通常会配置2条或2条以上。域名注册人可以自行修改。 4. DNS扩展协议–EDNSEDNS（Extension Mechanisms for DNS,DNS扩展机制）是一种扩展DNS协议的方法，解决了一些DNS协议的限制，RFC2671首次提出，RFC6891更新。目前大多数CDN厂商都支持了EDNS Client Subnet，但是运营商的LDNS都不支持，所以现在还未正式被广泛使用。 5. DNS相关操作5.1 本地查看DNS解析 dig（Windows默认不支持） nslookup 5.2 修改LocalDNSWindows：步骤1:打开网络设置 1.右键点击任务栏的网络图标(Wi-Fi或以太网),选择”打开网络和共享中心”。 或通过控制面板进入: 控制面板&gt;网络和 Internet&gt;网络和共享中心。步骤2:进入网络适配器属性 1.点击当前连接的网络名称(如”以太网”或”Wi-Fi”)。 2.在弹出的窗口中点击”属性”。 若需要管理员权限,点击”继续”。步骤3:修改IPv4 DNS 设置 在列表中选择”Internet协议版本4(TCP&#x2F;IPv4)“,点击”属性”。选择”使用下面的DNS服务器地址”,输入: ꔷ 首选DNS服务器:223.5.5.5 ꔷ 备用DNS服务器(可选):223.6.6.6勾选”退出时验证设置”,点击”确定”保存。 MacOS：步骤1:打开系统偏好设置 点击屏幕左上角苹果菜单,选择”系统偏好设置”&gt;“网络”。步骤2:选择网络连接类型 在左侧选择当前连接的网络(如Wi-Fi或以太网)),点击右下角”高级”。步骤3:配置DNS服务器 1.切换到”DNS”选项卡。 2.点击”+“添加DNS服务器, 输入: 223.5.5.5。 可添加备用DNS(如223.6.6.6)。 3.点击”确定”&gt;“应用”保存设置。 更多其他系统可参考https://www.alidns.com/knowledge?type=SETTING_DOCS 5.2 修改hosts文件 Linux&#x2F;MacOS：&#x2F;etc&#x2F;hosts Windows：C:\\Windows\\System32\\drivers\\etc\\hosts 5.3 本地刷新DNS缓存 Windows：ipconfig /flushdns MacOS：sudo dscacheutil -flushcache; sudo killall -HUP mDNSResponder Linux：sudo nscd -i hostsLinux系统上的DNS缓存服务可能因发行版和配置而异，使用不同服务作为DNS缓存守护进程的命令不同，取决于所使用的网络管理工具或DNS解析器。 5.4 查看全局DNS解析拨测工具https://boce.aliyun.com/detect/DNS 6. DNS劫持DNS劫持是指在域名解析过程中，通过技术手段篡改或干扰正常的DNS解析结果，使域名指向一个未经授权或错误的IP地址的行为。这通常会导致用户尝试访问一个网站时，被重定向到另一个未预期的网站，可能涉及恶意目的，如广告展示、钓鱼攻击或数据窃取等。 6.1 形式1：个别LocalDNS劫持通常情况为，该LocalDNS的解析结果与NS不一致，与全局其他LocalDNS解析也不一致。例如：客户已经将域名CNAME到阿里CDN，但是某个LocalDNS没有CNAME到阿里。 解决思路： 定位问题LDNS，通过客户反馈的情况和拨测，找到对应解析出错的LDNS； 客户与对应ISP报障或修改DNS记录，如果不配合可联系通信管理局进行申诉。 6.2 形式2：全局大面积解析异常可能是因为客户业务违规，被XX了。以www.fxxxbook.com为例，海外地区会正常解析，CNAME到star-mini.c10r.fxxxbook.com，并且给出A记录。","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://galaxyyao.github.io/tags/%E7%BD%91%E7%BB%9C/"}]},{"title":"向量化与向量数据库入门","slug":"向量化与向量数据库入门","date":"2025-06-11T01:14:18.000Z","updated":"2025-06-11T05:16:20.171Z","comments":true,"path":"2025/06/11/向量化与向量数据库入门/","permalink":"https://galaxyyao.github.io/2025/06/11/%E5%90%91%E9%87%8F%E5%8C%96%E4%B8%8E%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%85%A5%E9%97%A8/","excerpt":"1. 引子近10年，开发过一个很简陋的舆情监控平台。大致的架构就是用爬虫从新浪新闻、微博、微信公众号等平台爬取新闻，存储到ElasticSearch中。对于每一条新闻的内容，基于一个负面词库（比如“爆雷”、“下跌”等词），一个公司名+公司高管名字的词库，根据一个自创的算法计算在新闻中的出现次数和关联性，由此判断这篇新闻是否是公司相关的负面舆情新闻。现在想来，其实就是用稀疏向量检索的方式，企图模拟稠密向量检索的语义识别效果。当时实测下来整体效果还不错。但对于包含了否定词，以及讽刺、反话的内容，识别效果不佳。 本文先比较稀疏向量和稠密向量的差别，然后介绍稠密向量的生成、相似度比较，以及检索方式。然后介绍了阿里云上的向量数据库。 2. 传统关键词检索与稀疏向量2.1 传统检索方式在信息检索领域，“传统”方式是通过关键词进行信息检索，其大致过程为： 对原始语料（如网页）进行关键词抽取。 建立关键词和原始语料的映射关系。常见的方法有倒排索引、TF-IDF、BM25等方法，其中TF-IDF、BM25通常用稀疏向量（Sparse Vector）来表示词频。 检索时，对检索语句进行关键词抽取，并通过步骤2中建立的映射关系召回关联度最高的TopK原始语料。 2.2 常见稀疏向量算法","text":"1. 引子近10年，开发过一个很简陋的舆情监控平台。大致的架构就是用爬虫从新浪新闻、微博、微信公众号等平台爬取新闻，存储到ElasticSearch中。对于每一条新闻的内容，基于一个负面词库（比如“爆雷”、“下跌”等词），一个公司名+公司高管名字的词库，根据一个自创的算法计算在新闻中的出现次数和关联性，由此判断这篇新闻是否是公司相关的负面舆情新闻。现在想来，其实就是用稀疏向量检索的方式，企图模拟稠密向量检索的语义识别效果。当时实测下来整体效果还不错。但对于包含了否定词，以及讽刺、反话的内容，识别效果不佳。 本文先比较稀疏向量和稠密向量的差别，然后介绍稠密向量的生成、相似度比较，以及检索方式。然后介绍了阿里云上的向量数据库。 2. 传统关键词检索与稀疏向量2.1 传统检索方式在信息检索领域，“传统”方式是通过关键词进行信息检索，其大致过程为： 对原始语料（如网页）进行关键词抽取。 建立关键词和原始语料的映射关系。常见的方法有倒排索引、TF-IDF、BM25等方法，其中TF-IDF、BM25通常用稀疏向量（Sparse Vector）来表示词频。 检索时，对检索语句进行关键词抽取，并通过步骤2中建立的映射关系召回关联度最高的TopK原始语料。 2.2 常见稀疏向量算法2.2.1 词袋（BOW）模型词袋模型(Bag-of-words model，BOW)，BOW模型假定对于一个文档，忽略它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的，不依赖于其它单词是否出现。以下面的句子为例： 12John likes to watch movies. Mary likes tooJohn also likes to watch football games. 将上面的两句话中看作一个文档集，列出文档中出现的所有单词（忽略大小写与标点符号），构造一个字典： 1&#123;&quot;John&quot;: 1, &quot;likes&quot;: 2, &quot;to&quot;: 3, &quot;watch&quot;: 4, &quot;movies&quot;: 5, &quot;also&quot;: 6, &quot;football&quot;: 7, &quot;games&quot;: 8, &quot;Mary&quot;: 9, &quot;too&quot;: 10&#125; 将句子向量化，维数和字典大小一致，第 ii 维上的 数值 代表 ID 为 ii 的词语在这个句子里出现的频次： 1234# 第一个文本[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]# 第二个文本[1, 1, 1, 1, 0, 1, 1, 1, 0, 0] 缺点： 不能保留语义：不能保留词语在句子中的位置信息，“你爱我” 和 “我爱你” 在这种方式下的向量化结果依然没有区别。“我喜欢北京” 和 “我不喜欢北京” 这两个文本语义相反，利用这个模型得到的结果却能认为它们是相似的文本。 维数高和稀疏性：当语料增加时，那么维数也会不可避免的增大，一个文本里不出现的词语就会增多，导致矩阵稀疏 2.2.2 TF-IDFTF-IDF（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用加权技术。TF意思是词频(Term Frequency)，IDF意思是逆文本频率指数(Inverse Document Frequency)。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。公式： 1TF-IDF(t,d)=TF(t,d)×IDF(t) 参数： TF（词频）：词 tt 在文档 dd 中出现的频率。 IDF（逆文档频率）：衡量词 tt 的普遍重要性（越少文档包含该词，IDF 越高）。 2.2.3 BM25BM25（Best Matching 25）是一种基于统计学的文本相似度评分算法，用于计算查询和文档之间的相关性。在早期研究中，研究人员尝试了多种参数组合和模型变体，并为不同版本赋予了编号（如 BM11、BM24、BM25 等）。BM25 是其中表现最优的版本之一，因此被广泛采用并成为标准算法名称。 参数： f(t,d)：词 tt 在文档 dd 中的频率。 ∣d∣：文档 dd 的长度。 avgdl：文档集合的平均长度。 k1​,b：可调参数（通常 k1​≈1.2−2.0，b≈0.75）。 BM25 对 TF-IDF 的改进： 文档长度归一化：BM25 通过 b 参数控制长文档的惩罚（避免长文档因词多而被误判为更相关）。 词频饱和：BM25 通过 k1​ 参数限制词频的权重增长（避免高频词过度影响结果）。 动态调整：BM25 的参数 k1​、b 允许针对不同数据集优化，而 TF-IDF 固定不变。BM25 是对 TF-IDF 的扩展，针对检索排序问题进行了优化，是现代搜索引擎（如 Elasticsearch）的基础算法之一。 2.3 稀疏向量的存储方式稀疏向量只存储非零元素及其维度的索引，通常以{ index: value} 的键值对表示。如： 1[&#123;2: 0.2&#125;, ..., &#123;9997: 0.5&#125;, &#123;9999: 0.7&#125;] 2.4 稀疏向量特征总结 原理：映射成一个高维（维度一般就是 vocabulary 空间大小）向量 数据特征：向量的大部分元素都是 0 检索机制：只为那些输入文本中出现过的 token 计算权重。非零值表明 token 在特定文档中的相对重要性。 检索特征：检索速度快 适用场景：需要精确匹配关键词或短语 局限性：无法对语义进行理解。例如，检索语句为“浙一医院”，经过分词后成为“浙一”和“医院”，这两个关键词都无法有效的命中用户预期中的“浙江大学医学院附属第一医院”这个目标。 3. 稠密向量3.1 Embedding模型与稠密向量Embedding模型的核心，是在将高维数据（如文本、图像、音频等）映射到低维的连续向量空间中。 举一个简单的例子：可以将狗的品种按照平均大小、毛色，毛发量、听话程度等，都以数值形式打个分。这样每个狗品种就可以形成一个数组，如：拉布拉多：[0.9, 0.8, -0.5,0.7]相近品种的狗特征相近，那么在多维空间的距离也就接近。 vector embedding，或称向量表示&#x2F;向量嵌入，通过 Embedding 模型生成的数据表示。狭义上特指 稠密（Dense） 向量。vector embedding通常还包含作为后续任务（如分类、聚类、检索）用途的含义。关键特征： 高维空间：一个维度能代表一个特征或属性，高维意味着分辨率高，能区分细微的语义差异； 数值表示：一个 embedding 一般就是一个浮点数数组，所以方便计算。 3.2 稠密向量特征总结 数据特征：embedding 向量的大部分字段都非零 维度特征：相比 sparse embedding 维度要低很多在这个多维空间中，意义相关的词被放置得更近。 稠密向量本质上是对语义的压缩，具有语义理解能力。 3.3 稠密向量生成模型原理：BERT &amp; OpenAI text-embedding-3从稠密向量模型的训练方法，可以大致理解为什么稠密向量具有语义理解能力。BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer架构的预训练语言模型，通过双向上下文建模和自监督学习，能够生成高质量的文本嵌入。其训练过程分为两个主要阶段：预训练（Pre-training） 和 微调（Fine-tuning）。 其中的预训练部分中，BERT通过两个自监督任务在大规模无标签语料上进行预训练，目标是让模型学习语言的通用表示。Masked Language Model (MLM) 目标：预测被随机遮蔽的词。 实现方式：输入句子中随机遮蔽15%的词（例如，将“the cat is on the mat”变成“the [MASK] is on the mat”）。模型根据上下文预测被遮蔽词的原始内容。 被遮蔽词中有80%替换为[MASK]，10%保留原词，10%替换为随机词（防止模型过度依赖[MASK]标记）。 Next Sentence Prediction (NSP) 目标：判断两段文本是否连续。（50% 正例，50% 负例）。 实现方式：输入句子对（Sentence A, Sentence B），其中50%的时间B是A的下一句，50%是随机句子。模型通过[CLS]标记的输出判断两段是否连续。 OpenAI的text-embedding-3还增加了对比学习任务、多任务学习等，更注重语义对齐和推理效率，无需微调。 3.4 百炼text-embedding-v3https://help.aliyun.com/zh/model-studio/embedding，是通义实验室基于LLM底座的多语言文本统一向量模型。灵活的向量维度选择：提供 1024、768、512、256、128和64六种维度选择，维度越高，语义表达精度越高。支持稠密向量（dense）和离散向量（sparse）。 Java SDK调用范例： 123456789101112131415public List&lt;Float&gt; textEmbed(String text) &#123; TextEmbeddingParam param = TextEmbeddingParam .builder() .model(TextEmbedding.Models.TEXT_EMBEDDING_V3) .texts(Collections.singletonList(text)).build(); TextEmbedding textEmbedding = new TextEmbedding(); TextEmbeddingResult result; try &#123; result = textEmbedding.call(param); &#125; catch (NoApiKeyException e) &#123; throw new RuntimeException(e); &#125; List&lt;Double&gt; doubleList = result.getOutput().getEmbeddings().get(0).getEmbedding(); return doubleList.stream().map(Double::floatValue).toList();&#125; 3.5 其他文本向量化模型魔搭社区-CoROM文本向量模型ID：damo&#x2F;nlp_corom_sentence-embedding_chinese-base维度：768度量方式：Cosine适用领域：中文-通用领域-base更多信息参考：https://modelscope.cn/models/iic/nlp_corom_sentence-embedding_chinese-base 魔搭社区-GTE文本向量模型ID：damo&#x2F;nlp_corom_sentence-embedding_chinese-base维度：768度量方式：Cosine适用领域：中文-通用领域-base更多信息参考：https://modelscope.cn/models/iic/gte_sentence-embedding_multilingual-base范例在上一章已提供，可参考。 魔搭社区-Undever多语言通用文本表示模型模型ID：damo&#x2F;udever-bloom-560m维度：1024度量方式：Cosine更多信息参考：https://modelscope.cn/models/iic/udever-bloom-560m 魔搭社区-StructBERT FAQ问答模型ID：damo&#x2F;nlp_structbert_faq-question-answering_chinese-base维度：768度量方式：Cosine适用领域：中文-通用领域-base更多信息参考：https://modelscope.cn/models/iic/nlp_structbert_faq-question-answering_chinese-base Jina Embeddings v2模型模型名称：jina-embeddings-v2-small-en维度：768度量方式：Cosinehttps://jina.ai/embeddings/ 3.6 多模态向量化模型此外，也有多模态也可以使用multimodal-embedding-v1。详情参考：https://help.aliyun.com/zh/model-studio/developer-reference/multimodal-embedding-api-reference 文本语义识别体验下面的代码是使用GTE文本向量化模型生成向量，归一化后进行相似度打分。 123456789101112131415161718192021222324252627import torch.nn.functional as Ffrom modelscope import AutoModel, AutoTokenizerinput_texts = [ &quot;吃完海鲜可以喝牛奶吗？&quot;, &quot;牛奶&quot;, &quot;海鲜&quot;, &quot;常识&quot;, &quot;吃完苹果可以吃香蕉吗？&quot;, &quot;可以喝牛奶&quot;, &quot;不可以喝牛奶&quot;]model_name_or_path = &#x27;iic/gte_sentence-embedding_multilingual-base&#x27;tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True)# Tokenize the input textsbatch_dict = tokenizer(input_texts, max_length=8192, padding=True, truncation=True, return_tensors=&#x27;pt&#x27;)outputs = model(**batch_dict)dimension = 768embeddings = outputs.last_hidden_state[:, 0][:dimension]embeddings = F.normalize(embeddings, p=2, dim=1)scores = (embeddings[:1] @ embeddings[1:].T)print(scores.tolist()) 结果是： 1[[0.6987984776496887, 0.694797933101654, 0.47736865282058716, 0.6109740734100342, 0.7921578288078308, 0.7704585790634155]] 我们将结果从低到高排个序： 常识：0.47736865282058716 吃完苹果可以吃香蕉吗？：0.6109740734100342 海鲜：0.694797933101654 牛奶：0.6987984776496887 不可以喝牛奶：0.7704585790634155 可以喝牛奶：0.7921578288078308从结果可以看到，对原始问题的回答得到了高分，并且符合生活常识的回答得到了最高分。 4. 向量相似度度量相似度量用于衡量向量之间的相似性。主要的度量方式为3种： 余弦相似度 欧式距离 点积相似度 4.1 欧式距离（Euclidean Distance）欧式距离可解释为连接两个点的线段的长度。欧式距离公式非常简单，使用勾股定理从这些点的笛卡尔坐标计算距离。 4.2 余弦相似度（Cosine Similarity）余弦相似度是指两个向量夹角的余弦。特点：如果将向量归一化为长度均为 1 的向量，则向量的点积也相同。余弦相似度经常被用作抵消高维欧式距离问题。当我们对高维数据向量的大小不关注时，可以使用余弦相似度。场景：对于文本分析，当数据以单词计数表示时，经常使用此度量。适用于不同长度的文本计算语义相似性。 4.3 点积相似度通过将两个向量的对应分量相乘后，将每个计算值相加求和得出结果，从而进行相似度检验。向量a和b的点积计算公式如下： 点积会受到向量长度和方向的影响。当两个向量长度相同但方向不同时：如果两个向量方向相同，则点积计算结果较大；如果两个向量方向相反，则点积计算结果较小。 4.4 Milvus中的度量类型Milvus 以上几种度量类型都支持，但名称有些不同： 欧氏距离：L2 内积&#x2F;点积相似度：IP 余弦相似度：COSINE可以在搜索的时候设置search_params来指定度量类型：123456789search_params = &#123; &quot;metric_type&quot;: &quot;L2&quot;&#125;result = milvus_client.search( collection_name=collection_name, data=[vectors_to_search], limit=1, search_params=search_params, output_fields=[&quot;id&quot;, &quot;text&quot;]) 5. 检索算法与向量索引5.1 KNNk近邻算法全称是 k-nearest neighbors algorithm，简称kNN。KNN是一种暴力检索算法。按距离排序，选取前 K 个。优点： 绝对最近缺点： 计算复杂度高：需遍历所有训练样本，时间复杂度为 O(n)。 内存消耗大：需存储整个训练集。 在人脸识别的场景中经常要求100%的召回率，这种情况下一般直接暴力计算。 5.2 ANN与通常非常耗时的精确检索相比，ANNS 的核心理念不再局限于返回最精确的结果，而是只搜索目标的近邻。ANNS 通过在可接受的范围内牺牲精确度来提高检索效率。 以其中常见的基于图的检索方法为例。 已有的向量数据库内容就是图中的点，ANN的任务就是对给定一个点找到距离最近的点。那么如果每个点都知道离自己近的点，那么是不是就可以沿着这个连接线找到相近的点了。这样就避免了与所有数据计算距离。这就是基于图算法出发点。 NSW（navigate small world）,漫游小世界算法。对于每个新的传入元素，我们从结构中找到其最近邻居的集合（近似的 Delaunay 图， 就是上面的右图）。该集合连接到元素。随着越来越多的元素被插入到结构中，以前用作短距离边现在变成长距离边，形成可导航的小世界。圆（顶点）是度量空间中的数据，黑边是近似的 Delaunay 图，红边是用于对数缩放的长距离边。箭头显示从入口点到查询的贪心算法的示例路径（显示为绿色）。 HNSW（Hierarchical Navigable Small World）是对 NSW 的一种改进。HNSW 借鉴了跳表的思想，根据连接的长度（距离）将连接划分为不同的层，然后就可以在多层图中进行搜索。在这种结构中，搜索从较长的连接（上层）开始，贪婪地遍历所有元素直到达到局部最小值，之后再切换到较短的连接（下层），然后重复该过程，如下图所示： 5.3 Milvus支持的索引 平面: 数据集相对较小，需要 100% 的召回率 IVF_FLAT: 高速查询，要求尽可能高的召回率 IVF_SQ8: 极高速查询，内存资源有限，可接受召回率略有下降 IVF_PQ: 高速查询，内存资源有限，可略微降低召回率 HNSW: 极高速查询，要求尽可能高的召回率，内存资源大 HNSW_SQ: 非常高速的查询，内存资源有限，可略微降低召回率 HNSW_PQ: 中速查询，内存资源非常有限，在召回率方面略有妥协 HNSW_PRQ: 中速查询，内存资源非常有限，召回率略有下降 SCANN: 极高速查询，要求尽可能高的召回率，内存资源大 6. 向量数据库主要概念6.1 基于稠密向量的语义检索 6.2 向量数据库与传统关系性数据库概念对比无论DashVector，还是Milvus，本质上更接近数据库。主要能力包含几项： 创建实例&#x2F;DB 创建表（Collection） 插入数据（实体&#x2F;Doc），支持分区 创建索引加速查询 搜索，支持标量字段过滤 DB: DB Collection: 表（Table）, Collection 是一个二维表，具有固定的列和变化的行。每列代表一个字段，每行代表一个实体。 Partition: 分区表（Partitioned Table）, 将 Collection 分割为多个子集，便于管理和查询性能优化（如按时间或业务逻辑分区）。 Field: 列（Column）, Milvus的Field包括标量字段（如字符串、整数）和向量字段。 Document: 行（Row）, 表中的一条记录，对应 Milvus 中的一组向量字段值+标量字段值。 Vector Index: B-Tree 索引, 用于加速向量相似性搜索的索引结构（如 HNSW、IVF-PQ）； 7. 向量检索服务比较7.1 自建Milvus vs 云MilvusMilvus架构：对于自建Milvus和云Milvus，官方的比较参考：[https://help.aliyun.com/zh/milvus/product-overview/comparison-between-alibaba-cloud-milvus-and-ecs-self-built-milvus] 其实也和云数据的优点类似： 开箱即用，拉起方便 弹性伸缩 白屏化运维和管控 稳定性 产研支持 7.2 云Milvus vs DashVectorDashVector相比Milvus： 简化概念，包括Schema、索引等 面向用户：做快速PoC，数据量小，性能要求不特别高 灵活度: 低。没有索引方式选择。 上手难度: 低之前DashVector还有一个优势：除了包年包月之外，支持按量付费。而之前云Milvus只有包年包月，最低规格每个月约5K，对开发者上手还是有些门槛和肉疼的。不过当前云Milvus也支持按量付费了。 8. 主要场景8.1 简单语义匹配将用户输入的文字和数据库里的文字进行语义匹配，寻找最接近的匹配结果的场景。 8.2 多模态检索 以文搜图：输入文本查询，搜索最相似的图片。 以文搜文：输入文本查询，搜索最相似的图片描述。 以图搜图：输入图片查询，搜索最相似的图片。 以图搜文：输入图片查询，搜索最相似的图片描述。 更多参见：[https://help.aliyun.com/zh/milvus/use-cases/multi-modal-search-through-alibaba-cloud-milvus-and-tongyi-qianwen] 8.3 RAG智能问答向量化的最通用业务场景之一，就是RAG知识库。先将文档切分为文本块，然后将文本块生成Embedding模型。 知识库预处理：您可以借助LangChain SDK对文本进行分割，作为Embedding模型的输入数据。 知识库存储：选定的Embedding模型（DashScope）负责将输入文本转换为向量，并将这些向量存入阿里云Milvus的向量数据库中。 向量相似性检索：Embedding模型处理用户的查询输入，并将其向量化。随后，利用阿里云Milvus的索引功能来识别出相应的Retrieved文档集。 RAG（Retrieval-Augmented Generation）对话验证：您使用LangChain SDK，并将相似性检索的结果作为上下文，将问题导入到LLM模型（本例中用的是阿里云PAI EAS），以产生最终的回答。此外，结果可以通过将问题直接查询LLM模型得到的答案进行核实。 提示词范例： 12345from dashscope import Generationdef answer_question(question, context): prompt = f&#x27;&#x27;&#x27;请基于```内的内容回答问题。&quot; &#123;context&#125; 12345我的问题是：&#123;question&#125;。&#x27;&#x27;&#x27;rsp = Generation.call(model=&#x27;qwen-turbo&#x27;, prompt=prompt)return rsp.output.text 适用场景： 客户希望更多把控全链路 客户希望进一步降低响应RT 8.4 更多场景想象 加速药物发现：在制药行业，向量嵌入可以编码化合物的化学结构，通过测量其与目标蛋白质的相似性来识别有前景的药物候选物。这加速了药物发现过程，通过专注于最有可能的线索来节省时间和资源。 异常检测：在欺诈检测、网络安全和工业监控等领域，向量嵌入在识别异常模式方面起着重要作用。通过将数据点表示为嵌入，可以通过计算距离或不相似性来检测异常，从而实现对潜在问题的早期识别和预防措施。 9. 参考资料 云Milvus官方文档 DashVector官方文档 Milvus官方文档 文本向量化（理论篇） 大模型 RAG 基础：信息检索、文本向量化及 BGE-M3 embedding 实践 20分钟速成 RAG &amp; 向量数据库核心概念 【上集】向量数据库技术鉴赏_哔哩哔哩_bilibili 【下集】向量数据库技术鉴赏_哔哩哔哩_bilibili","categories":[],"tags":[{"name":"ai","slug":"ai","permalink":"https://galaxyyao.github.io/tags/ai/"}]},{"title":"GTC2025信息脑图及观后感","slug":"GTC2025信息脑图及观后感","date":"2025-04-07T01:29:28.000Z","updated":"2025-04-07T01:29:28.234Z","comments":true,"path":"2025/04/07/GTC2025信息脑图及观后感/","permalink":"https://galaxyyao.github.io/2025/04/07/GTC2025%E4%BF%A1%E6%81%AF%E8%84%91%E5%9B%BE%E5%8F%8A%E8%A7%82%E5%90%8E%E6%84%9F/","excerpt":"来源：NVIDIA CEO 黄仁勋主题演讲 | GTC 2025 GTC2025信息脑图 片头 Token开拓了新边界：突出了token与物理世界的联系 Geforce 5090显卡开场 GTC的渊源从Geforce开始：Geforce带来CUDA，CUDA促进AI，AI反过来促进计算机图形学 AI促进计算机图形学的范例：对每个像素预测15个像素，并保持时间稳定性 AI的历史 感知式AI：计算机视觉、语音识别 生成式AI：在多模态之间转换 除了文本、图像、视频，还包含氨基酸到蛋白质、特性到化学物质 从检索式计算模型，转变为生成式计算模型。以前都是预先创建多个版本的内容。现在不再检索数据而是生成答案，根本上改变了计算方式 自主智能AI：具备自主性（has agency）的AI 能推理如何解决，并能采取行动。 物理AI：能理解物理世界，如摩擦、惯性、因果 对于Nvidia合作方的意义：每个阶段都开启了更多机遇，更大的图景诞生了更多的合作方 贯穿AI每个阶段的三个基本问题 问题1：数据问题 人类历史已经积累了数百问题空间，生成数百万个不同示例。如勾股定律、数独、益智游戏。这些会生成万亿个token 问题2：训练问题 无需人工干预，借助强化学习生成 问题3：规模化问题 投入的资源更多，AI越聪明。 今年预估的资源比去年这时候预期的要多至少100倍 为什么需要更多资源的逻辑 过去：ChatGPT采取了“一击即中（One Shot）”的方式，所以回答问题很可能会出错，效果不佳 token数增加：现在不仅生成一个个token或单词，而是生成代表推理步骤的单词序列，生成token数大幅增加 步骤增加：推理，可能会尝试多种方法后选择最佳方法，可能会用多种方法解决后做一致性检查，可能得出答案后将答案代回方程验证正确性。 计算时效性要求不变，因此计算速度需要提高 数据证明：四大云服务运营商的Blackwell和Hooper出货量对比 AI工厂 变化趋势：增长加速、软件的未来需要资本投入 手工编码的通用计算到了尽头。计算机成为了软件token生成器，而非文件检索工具。 将生成的token重构为音乐、文字、视频、研究成果、蛋白质 软件栈：各行各业的900多个CUDA-X库，实现计算加速 在CUDA上还有各行各业的AI库（物理学、生物学、光刻）来搭建AI框架，提供感知、学习、推理能力 每个工厂需要两个“工厂”。例如一个工厂制造晶圆，另一个工厂制造晶圆所需的信息 行业1举例：光刻 行业2举例：无线网络通信（5G） 行业3举例：基因测序分析 行业4举例：计算机辅助工程（CAE） AI对于各行业 云服务商（CSP）：GPU云，托管GPU 边缘计算：6G无线网络 AI-RAN 价值：通过上下文和先验知识，改善不同环境下的大规模MIMO（多输入多输出）。（原理类似前文的像素点预测） 自动驾驶 制造3种计算机：训练、仿真、自动驾驶 HALOS：汽车安全。对每一行代码安全评估。 Cosmos + Omniverse：AI创造AI，包括模型蒸馏、闭环训练（由Cosmos评分）、合成数据生成（Omniverse神经重建技术，将日志转为4D驾驶环境，并创建变体） 数据中心 前提：未来每个数据中心都会受到电力限制 向上扩展（Scale Up） 在横向扩展（Scale Out）之前，先需要向上扩展（Scale Up） 难点：无法使用类似Hadoop的方式复用现有服务器，电力成本会过高。 Blackwell的硬件设计 Blackwell源于Range。 上一代Scale Up的极限：HGX。8个GPU，连接到NVLink 8交换机。然后通过PCI Express连接到CPU机架，最终形成AI超级计算机。 Range在HGX的基础上扩展了4倍，对接NVLink 32。Range证明方向正确，但规模过大。于是进行了重新设计。重构方式：解构了NVLink，放在机箱中心 强大的计算能力是用于解决看似简单的终极问题：推理 为什么推理是终极问题：工厂所依赖的推理的效率决定了工厂的盈亏。工厂生成的token越多，AI越能给出聪明的答案；但时间过长会贻误时机。 token数与响应时间依赖大量计算能力，所以就需要Blackwell 数据证明：安排婚礼座位，传统LLM采用one shot，消耗了439个token，得到了错误的答案（白白浪费了token）；DeepSeek R1会尝试不同场景，返回检验答案，最终消耗了8559个token，得到争取到的答案。即20多倍的token数，150多倍的计算量 NVLink的价值：为什么推理需要NVLink： DS R1运行时，需要将工作负载（数万亿个参数和模型）分布到整个GPU系统中。Blackwell的NVLink 72架构的优势在于每个GPU都可以执行推理所需的批处理和聚合。 预填充阶段：推理模型需要进行思考、进行阅读网站和看视频以消化信息，这些信息消化和上下文处理非常依赖浮点运算。 解码阶段：需要浮点运算更需要巨大带宽。数万亿个参数输入，每秒TB级的数据仅仅为了生成一个token。 Dynamo（直译为发电机）：AI工厂的操作系统 需求：支持动态分配不同的GPU数量给预填充和解码，动态适应思考（更需要预填充）和聊天（更需要解码）等不同场景的需求 为什么称之为操作系统：以前操作系统是协调应用程序运行。未来是协调Agents智能体 Blackwell对AI工厂的价值估算 Blackwell + Dynamo + NVL72 + FP4：在最大吞吐率和最高质量之间寻找平衡点。 Blackwell方案的ISO功耗是Hooper的25倍。对于一个数据中心，Blackwell的token生产效率是Hooper的40倍，每秒生成12,000,000,000个token 未来规划：Blackwell Ultra、Vera Rubin（2026）、Rubin Ultra（2027） 横向扩展（Scale Out） 挑战：收发器会消耗大量能源，将电信号转换为光信号 25万个GPU中的每一个都需要6个收发器，使每个GPU增加180瓦的能耗 解决方案：共封装光学（CPO） 基于微环谐振器调制器（MRM），解决如何扩展到数百万个GPU的问题 最终方案：将硅光和光电一体化封装方案结合，不再需要收发器，光纤直接连入512端口交换机，节省数十兆瓦的能量。 未来规划：下一代产品命名为Feynman 企业计算 AI与机器学习重塑了计算机技术栈：处理器、操作系统、应用程序、应用的运行方式、编排方式都不再相同 范例：对数据将不再精确检索，而是阅读并尝试理解，直接给出答案 未来的个人电脑：DGX Station工作站 计算机三大支柱：计算、网络、存储 重新设计存储：不再是基于检索的存储系统，而是基于语义。只需要与之交互。 机器人技术 背景：全世界缺少5000万个工人 三大基础问题同样适用 数据问题：互联网规模的数据提供了常识和推理能力。基于Cosmos + Omniverse生成海量合成行动和控制数据。 Omniverse：物理AI操作系统 Cosmos：理解物理世界。用Omniverse调节Cosmos，用Cosmos生成无限数量的环境 训练问题： Newton：能训练触觉反馈、精细动作的物理引擎。将物理定律作为可验证奖励。 规模化问题： Nvidia Isaac Groot N1：人形机器人通才基础模型 双系统架构，用于快思考和慢思考。 慢思考用于感知和推理，规划行动 快思考转化为精确而连续的动作 泛化能力：操作常见物体、协同执行多步骤 感受GTC2025演讲内容的逻辑性老黄的演讲中的确不乏画饼的成分，以部分抵消DeepSeek对于Scaling Law的挑战。不过对于我们有价值的，还是他是如何从逻辑上说圆自洽。 为什么投资者还应该继续看好Nvidia：我们处于时代拐点 什么时代拐点：已经从生成式AI进入了自主能推理的AI，并向物理AI时代进发 自主推理AI与Nvidia有什么关系：推理产生更多的token，要求更高的计算速度 自主推理AI带来的革新：计算机将变成token生成器，token与物理世界关联 为什么token能与物理世界关联的基础：各行各业的CUDA-X Nvidia如何支撑产生更多的token：新技术Blackwell和CPO reinvent computer","text":"来源：NVIDIA CEO 黄仁勋主题演讲 | GTC 2025 GTC2025信息脑图 片头 Token开拓了新边界：突出了token与物理世界的联系 Geforce 5090显卡开场 GTC的渊源从Geforce开始：Geforce带来CUDA，CUDA促进AI，AI反过来促进计算机图形学 AI促进计算机图形学的范例：对每个像素预测15个像素，并保持时间稳定性 AI的历史 感知式AI：计算机视觉、语音识别 生成式AI：在多模态之间转换 除了文本、图像、视频，还包含氨基酸到蛋白质、特性到化学物质 从检索式计算模型，转变为生成式计算模型。以前都是预先创建多个版本的内容。现在不再检索数据而是生成答案，根本上改变了计算方式 自主智能AI：具备自主性（has agency）的AI 能推理如何解决，并能采取行动。 物理AI：能理解物理世界，如摩擦、惯性、因果 对于Nvidia合作方的意义：每个阶段都开启了更多机遇，更大的图景诞生了更多的合作方 贯穿AI每个阶段的三个基本问题 问题1：数据问题 人类历史已经积累了数百问题空间，生成数百万个不同示例。如勾股定律、数独、益智游戏。这些会生成万亿个token 问题2：训练问题 无需人工干预，借助强化学习生成 问题3：规模化问题 投入的资源更多，AI越聪明。 今年预估的资源比去年这时候预期的要多至少100倍 为什么需要更多资源的逻辑 过去：ChatGPT采取了“一击即中（One Shot）”的方式，所以回答问题很可能会出错，效果不佳 token数增加：现在不仅生成一个个token或单词，而是生成代表推理步骤的单词序列，生成token数大幅增加 步骤增加：推理，可能会尝试多种方法后选择最佳方法，可能会用多种方法解决后做一致性检查，可能得出答案后将答案代回方程验证正确性。 计算时效性要求不变，因此计算速度需要提高 数据证明：四大云服务运营商的Blackwell和Hooper出货量对比 AI工厂 变化趋势：增长加速、软件的未来需要资本投入 手工编码的通用计算到了尽头。计算机成为了软件token生成器，而非文件检索工具。 将生成的token重构为音乐、文字、视频、研究成果、蛋白质 软件栈：各行各业的900多个CUDA-X库，实现计算加速 在CUDA上还有各行各业的AI库（物理学、生物学、光刻）来搭建AI框架，提供感知、学习、推理能力 每个工厂需要两个“工厂”。例如一个工厂制造晶圆，另一个工厂制造晶圆所需的信息 行业1举例：光刻 行业2举例：无线网络通信（5G） 行业3举例：基因测序分析 行业4举例：计算机辅助工程（CAE） AI对于各行业 云服务商（CSP）：GPU云，托管GPU 边缘计算：6G无线网络 AI-RAN 价值：通过上下文和先验知识，改善不同环境下的大规模MIMO（多输入多输出）。（原理类似前文的像素点预测） 自动驾驶 制造3种计算机：训练、仿真、自动驾驶 HALOS：汽车安全。对每一行代码安全评估。 Cosmos + Omniverse：AI创造AI，包括模型蒸馏、闭环训练（由Cosmos评分）、合成数据生成（Omniverse神经重建技术，将日志转为4D驾驶环境，并创建变体） 数据中心 前提：未来每个数据中心都会受到电力限制 向上扩展（Scale Up） 在横向扩展（Scale Out）之前，先需要向上扩展（Scale Up） 难点：无法使用类似Hadoop的方式复用现有服务器，电力成本会过高。 Blackwell的硬件设计 Blackwell源于Range。 上一代Scale Up的极限：HGX。8个GPU，连接到NVLink 8交换机。然后通过PCI Express连接到CPU机架，最终形成AI超级计算机。 Range在HGX的基础上扩展了4倍，对接NVLink 32。Range证明方向正确，但规模过大。于是进行了重新设计。重构方式：解构了NVLink，放在机箱中心 强大的计算能力是用于解决看似简单的终极问题：推理 为什么推理是终极问题：工厂所依赖的推理的效率决定了工厂的盈亏。工厂生成的token越多，AI越能给出聪明的答案；但时间过长会贻误时机。 token数与响应时间依赖大量计算能力，所以就需要Blackwell 数据证明：安排婚礼座位，传统LLM采用one shot，消耗了439个token，得到了错误的答案（白白浪费了token）；DeepSeek R1会尝试不同场景，返回检验答案，最终消耗了8559个token，得到争取到的答案。即20多倍的token数，150多倍的计算量 NVLink的价值：为什么推理需要NVLink： DS R1运行时，需要将工作负载（数万亿个参数和模型）分布到整个GPU系统中。Blackwell的NVLink 72架构的优势在于每个GPU都可以执行推理所需的批处理和聚合。 预填充阶段：推理模型需要进行思考、进行阅读网站和看视频以消化信息，这些信息消化和上下文处理非常依赖浮点运算。 解码阶段：需要浮点运算更需要巨大带宽。数万亿个参数输入，每秒TB级的数据仅仅为了生成一个token。 Dynamo（直译为发电机）：AI工厂的操作系统 需求：支持动态分配不同的GPU数量给预填充和解码，动态适应思考（更需要预填充）和聊天（更需要解码）等不同场景的需求 为什么称之为操作系统：以前操作系统是协调应用程序运行。未来是协调Agents智能体 Blackwell对AI工厂的价值估算 Blackwell + Dynamo + NVL72 + FP4：在最大吞吐率和最高质量之间寻找平衡点。 Blackwell方案的ISO功耗是Hooper的25倍。对于一个数据中心，Blackwell的token生产效率是Hooper的40倍，每秒生成12,000,000,000个token 未来规划：Blackwell Ultra、Vera Rubin（2026）、Rubin Ultra（2027） 横向扩展（Scale Out） 挑战：收发器会消耗大量能源，将电信号转换为光信号 25万个GPU中的每一个都需要6个收发器，使每个GPU增加180瓦的能耗 解决方案：共封装光学（CPO） 基于微环谐振器调制器（MRM），解决如何扩展到数百万个GPU的问题 最终方案：将硅光和光电一体化封装方案结合，不再需要收发器，光纤直接连入512端口交换机，节省数十兆瓦的能量。 未来规划：下一代产品命名为Feynman 企业计算 AI与机器学习重塑了计算机技术栈：处理器、操作系统、应用程序、应用的运行方式、编排方式都不再相同 范例：对数据将不再精确检索，而是阅读并尝试理解，直接给出答案 未来的个人电脑：DGX Station工作站 计算机三大支柱：计算、网络、存储 重新设计存储：不再是基于检索的存储系统，而是基于语义。只需要与之交互。 机器人技术 背景：全世界缺少5000万个工人 三大基础问题同样适用 数据问题：互联网规模的数据提供了常识和推理能力。基于Cosmos + Omniverse生成海量合成行动和控制数据。 Omniverse：物理AI操作系统 Cosmos：理解物理世界。用Omniverse调节Cosmos，用Cosmos生成无限数量的环境 训练问题： Newton：能训练触觉反馈、精细动作的物理引擎。将物理定律作为可验证奖励。 规模化问题： Nvidia Isaac Groot N1：人形机器人通才基础模型 双系统架构，用于快思考和慢思考。 慢思考用于感知和推理，规划行动 快思考转化为精确而连续的动作 泛化能力：操作常见物体、协同执行多步骤 感受GTC2025演讲内容的逻辑性老黄的演讲中的确不乏画饼的成分，以部分抵消DeepSeek对于Scaling Law的挑战。不过对于我们有价值的，还是他是如何从逻辑上说圆自洽。 为什么投资者还应该继续看好Nvidia：我们处于时代拐点 什么时代拐点：已经从生成式AI进入了自主能推理的AI，并向物理AI时代进发 自主推理AI与Nvidia有什么关系：推理产生更多的token，要求更高的计算速度 自主推理AI带来的革新：计算机将变成token生成器，token与物理世界关联 为什么token能与物理世界关联的基础：各行各业的CUDA-X Nvidia如何支撑产生更多的token：新技术Blackwell和CPO reinvent computer演讲中个人觉得最有意思的部分莫过于重新定义了计算机，类似乔布斯reinvent phone，老黄也在宣言reinvent computer。 对计算机的使用方式变了：从检索数据到交互式提问 开发应用程序变为了编写和编排智能体对于新的应用程序开发模式，是否有更加适合的开发工具和开发方式？ 更核心的转变是通过token的多模态转换，将token变成了计算机科学中的0和1，构成万物的原子。虽然数字孪生的概念提出很多年了，但从物理AI的概念中看到了真正实现的可行性。 对于云计算的影响是否会淘汰一部分云计算产品，新诞生无需与人交互，专供AI和机器学习使用的云计算产品？对于Omniverse生成的多模态数据，是否有更适合存放的数据库？对于AI生产的海量数据存放，是否云存储有更高效和低成本的存放方式？","categories":[],"tags":[{"name":"ai","slug":"ai","permalink":"https://galaxyyao.github.io/tags/ai/"},{"name":"hardware","slug":"hardware","permalink":"https://galaxyyao.github.io/tags/hardware/"}]},{"title":"Review/Share-2024-08技术文章分享","slug":"Review-Share-2024-08技术文章分享","date":"2024-08-29T16:00:00.000Z","updated":"2024-08-30T06:01:47.432Z","comments":true,"path":"2024/08/30/Review-Share-2024-08技术文章分享/","permalink":"https://galaxyyao.github.io/2024/08/30/Review-Share-2024-08%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0%E5%88%86%E4%BA%AB/","excerpt":"ARTS中的 Review英文技术文章点评 和 Share技术文章分享，先试点按照月为单位，每月发一篇汇总吧。每篇写一小段感想。关于英文技术文章来源，从v2ex的推荐，找到了DevURLs – A neat developer news aggregator的文章聚合网站。但感觉这些文章大多很短，看得不太过瘾。所以在尝试了两周后，改为根据本周关注的技术点，用英文关键字Google，选择搜索结果中质量比较高的。 RevieweBPF for Cloud Computing - DZonehttps://dzone.com/articles/ebpf-for-cloud-computing eBPF这个术语看到过好几次，找了一篇入门介绍。主要是针对eBPF和K8S结合的场景，包括网络可观测性、安全、性能监控等用途。eBPF的主要优势在于不用开发Linux内核模块，也能在内核执行。这点使其很适合K8S，用于包含Calico、Cilium在内的开源项目。 Improving language understanding by generative pre-training[J].https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf 在B站李沐视频和内网解读文章的辅助下，读了GPT-1的论文。除了论文本身的知识点外，有2个感受：","text":"ARTS中的 Review英文技术文章点评 和 Share技术文章分享，先试点按照月为单位，每月发一篇汇总吧。每篇写一小段感想。关于英文技术文章来源，从v2ex的推荐，找到了DevURLs – A neat developer news aggregator的文章聚合网站。但感觉这些文章大多很短，看得不太过瘾。所以在尝试了两周后，改为根据本周关注的技术点，用英文关键字Google，选择搜索结果中质量比较高的。 RevieweBPF for Cloud Computing - DZonehttps://dzone.com/articles/ebpf-for-cloud-computing eBPF这个术语看到过好几次，找了一篇入门介绍。主要是针对eBPF和K8S结合的场景，包括网络可观测性、安全、性能监控等用途。eBPF的主要优势在于不用开发Linux内核模块，也能在内核执行。这点使其很适合K8S，用于包含Calico、Cilium在内的开源项目。 Improving language understanding by generative pre-training[J].https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf 在B站李沐视频和内网解读文章的辅助下，读了GPT-1的论文。除了论文本身的知识点外，有2个感受： 学习了标准论文的结构。 好的配图对于理解很重要。我指的就是Transformer原理的那张经典架构图。 A Beginner’s Guide to Vector Embeddingshttps://www.timescale.com/blog/a-beginners-guide-to-vector-embeddings/这周在做灵积的DashScope和DashVector的Demo，对其中用到的embeddings概念感兴趣，就找了一篇相关的科普。记录一下科普文中比较感兴趣的点： Embeddings和Vector的关系：Embeddings可以表现为Vectors，可以生成Vectors，但Vectors本身的概念范围更加广泛。 Embeddings的类型，除了Word外，还包含Sentence、Document、Graph、Image、Audio，还有Product。Product可以用于产品推荐，应该在淘宝中已经运用了。 Vector Embeddings如何存储：作者推荐使用PG 关于Embeddings、Zero Shot、Few Shot、Fine tunning的使用场景，也可以参考下图 Share【哔哩哔哩技术】7.2 我们机房断网了！https://mp.weixin.qq.com/s/KD1cvp6thMXM8LkrRUoslw B站技术的文章通常干货不少。之前在网络方面的常识比较缺乏，借这篇也补习了下POP（网络接入点）的概念。也重温了下南北向流量管控和东西向流量管控的策略区别： 南北向流量：指的是从外部（互联网）进入数据中心或云平台内部，以及从内部向外发出的流量。 东西向流量：东西向流量指的是在同一数据中心内部或云平台内不同服务器、服务之间流动的流量。 南北向包含： 防火墙与安全组 入侵检测与防御系统（IDS&#x2F;IPS） SSL&#x2F;TLS卸载与检查 边界网关协议（BGP）路由控制 DDoS防护 东西向包含： 软件定义网络（SDN） 网络策略控制器：如Kubernetes中的NetworkPolicy 服务网格（Service Mesh）：如Istio、Linkerd等 API网关与服务间认证 人工智能 LLM 革命前夜：一文读懂横扫自然语言处理的 Transformer 模型 &#x2F; 人工智能 LLM 革命破晓：一文读懂当下超大语言模型发展现状https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/https://www.mikecaptain.com/2023/03/06/captain-aigc-2-llm/ 这两篇并在一起讲。在查阅GPT的发展历程的过程中，这两篇原阿里同学写的文章，是最详尽的。通过这两篇，加上B站李沐的视频，我的主要收获是基本把LLM中几个重要概念有了初步了解。我个人重点关注的概念包括： Transformer（含编码器与解码器） 注意力（Attension）与自注意力机制 GPT与BERT的关系 Fine tuning&#x2F;Zero shot&#x2F;Few Shot的区别 以前读的几篇GPT的介绍文，我个人最不满意的就是将Few Shot描述得像魔法一样。的确由于LLM的黑盒特性，无法给出非常确切的结论，但业界肯定是有假设的。采用这种神乎其神的介绍方式只会降低GPT的可信度。而《人工智能 LLM 革命破晓：一文读懂当下超大语言模型发展现状》这篇中，给出了ICL能力的底层假设是贝叶斯推理，以及提到了很多隐式马尔科夫模型的混合体的数学框架假设。 Way To Prompt系列(1): 为什么大模型连”Strawberry”的”r”都数不对？一招“理由先行”显著提升模型思考能力这篇是内网的文章，不过命题的来源是外网的几篇文章： 为什么AI数不清Strawberry里有几个 r？Karpathy：我用表情包给你解释一下 | 机器之心：https://www.jiqizhixin.com/articles/2024-07-27-5 大模型智障检测+1：Strawberry有几个r纷纷数不清，最新最强Llama3.1也傻了-36氪：https://36kr.com/p/2877379178156681 一记惊雷：改一下Prompt的输出顺序，就能显著影响LLM的评估结果：https://mp.weixin.qq.com/s/GxX0brkFjlUN9z8oRWJUNw 这几篇深入简出地解释了Tokenization，以及所带来的问题。也介绍了Few Shot和Zero Shot的分别解决办法： Few Shot：添加Prompt：“请你先将单词拆分成一个个字母，再用0和1分别标记字母列表中非r和r的位置，数一下一共有几个r” Zero Shot：添加Prompt“请一步步思考，以逐级复杂的原则思考问题，最后才得到答案。”，即思维链（Chain-of-Thought，简称CoT） 不用CoT加以限制的的情况下，大模型倾向于先给出一个答案，再给出理由。这个理由很可能是为了圆那个答案编造出来的。这个引导方式也不禁让我想到教女儿的时候，如果引导她一步步说出自己的思考过程，答案的正确率的确会提高。总结成四个字：“理由先行”。","categories":[],"tags":[{"name":"review","slug":"review","permalink":"https://galaxyyao.github.io/tags/review/"},{"name":"share","slug":"share","permalink":"https://galaxyyao.github.io/tags/share/"}]},{"title":"大数据的因果性和相关性","slug":"大数据的因果性和相关性","date":"2024-08-22T16:00:00.000Z","updated":"2024-08-23T03:06:24.467Z","comments":true,"path":"2024/08/23/大数据的因果性和相关性/","permalink":"https://galaxyyao.github.io/2024/08/23/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E5%9B%A0%E6%9E%9C%E6%80%A7%E5%92%8C%E7%9B%B8%E5%85%B3%E6%80%A7/","excerpt":"大数据的相关性特性挺巧的是，最近从不同来源听到了这个理论两次。 第一次是ACE题库里的一道题：“美国海军军官莫里通过对前人航海日志的分析，绘制了新的航海日志图，表明了大风与洋流可能发生的地点，这体现了大数据分析理念中的：”这道题的标准答案是：在分析方法上，更注重相关分析而不是因果分析。 第二次是听播客“纵横四海”，讲瑞幸数字化。瑞幸除了把甜度、浓稠度等饮料的指标做了数字化之外，也将其和销量的反馈结合起来。但在结合的时候，并不会试图推断出甜度与销量的因果关系，而是相信数据给出的指引。播客里还举了几个典型的案例，比如沃尔玛发现飓风前人们喜欢屯蛋挞。至于为什么是蛋挞，而不是薯片，也不是可乐，从因果性没法给出很好的解释。还有亚马逊的案例中书籍推荐之间的相关性，医疗保险方面买车与遵医嘱信用分的相关性。这时候就要反人类的本能，放弃对因果性的追究，去全身心拥抱大数据。 从某个角度，LLM中对Few Shot有效性的解释，也可以从这个角度来理解。Few Shot本身并没有改变模型，但得到的结果就是更好了。即Few Shot与结果的有效性之间存在相关性。但当前的理论模型只能给出一些猜测，还无法完美证明。 个人对因果性与相关性的看法对于这个现象，我的个人看法分为两点： 接受这个事实，从实用性角度先利用起来。 关注业界对于其背后原理的研究进展。","text":"大数据的相关性特性挺巧的是，最近从不同来源听到了这个理论两次。 第一次是ACE题库里的一道题：“美国海军军官莫里通过对前人航海日志的分析，绘制了新的航海日志图，表明了大风与洋流可能发生的地点，这体现了大数据分析理念中的：”这道题的标准答案是：在分析方法上，更注重相关分析而不是因果分析。 第二次是听播客“纵横四海”，讲瑞幸数字化。瑞幸除了把甜度、浓稠度等饮料的指标做了数字化之外，也将其和销量的反馈结合起来。但在结合的时候，并不会试图推断出甜度与销量的因果关系，而是相信数据给出的指引。播客里还举了几个典型的案例，比如沃尔玛发现飓风前人们喜欢屯蛋挞。至于为什么是蛋挞，而不是薯片，也不是可乐，从因果性没法给出很好的解释。还有亚马逊的案例中书籍推荐之间的相关性，医疗保险方面买车与遵医嘱信用分的相关性。这时候就要反人类的本能，放弃对因果性的追究，去全身心拥抱大数据。 从某个角度，LLM中对Few Shot有效性的解释，也可以从这个角度来理解。Few Shot本身并没有改变模型，但得到的结果就是更好了。即Few Shot与结果的有效性之间存在相关性。但当前的理论模型只能给出一些猜测，还无法完美证明。 个人对因果性与相关性的看法对于这个现象，我的个人看法分为两点： 接受这个事实，从实用性角度先利用起来。 关注业界对于其背后原理的研究进展。 背后的逻辑是： 承认人类对于现实世界的理解是有限的。的确存在只发现了现象，但暂时无法完全探明其原理的事务。 时间是有限的，机会是稍纵即逝的。很多时候如果一定要掌握因果性才做决策，机会点就丢失了。务实为先。 但不盲信，不将现象宗教化。坚信其中被后的原理早晚是会被人类解答的。 舍恩伯格在《大数据时代》中的观点是放弃对因果性的理解：我们没有必要非得知道现象背后的原因，而是要让数据自己发声；其译者周涛的观点是不能放弃：放弃了对因果的追求，就是放弃了人凌驾计算机之上的智力优势，是人类自身的放纵和堕落。 我个人还是认同不能放弃对因果性追求的观点。即使是从实用性角度，了解数据之间的因果关系，也有利于建立起更好的模型，避免明显的伪相关性。典型的案例就是“冰淇淋销售量和溺水次数”的相关性，与“降雨和骑自行车的人数”的相关性。溺水预测模型明显应该排除冰激凌销量的数据，而降雨预测模型也应该排除骑自行车人数。 在搜索相关观点的过程中，甚至看到了一些比较极端的观点，比如：“世界可能根本没有因果性”。对这种观点就不予置评了。 总结对这个topic也只是在听完播客后的理解巩固。在工作中暂时没有实际的运用场景。就先推荐一下相关的不错的深入阅读文章吧。 参考资料7.8 相关关系、因果关系和预测 | 预测：方法与实践（第三版）https://otexts.com/fpp3cn/causality-cn.html这本书在澳大利亚莫纳什大学用作商学本科三年级的课程教材，还贴心地有中文翻译，值得后续进一步详细阅读。在B站上还有教师英文讲课视频可以锻炼英语听力。 相关性不一定等于因果性：从 Yule-Simpson’s Paradox 讲起 - 郑瀚Andrew - 博客园https://www.cnblogs.com/LittleHann/p/11992311.html这篇文章对相关性与因果性的相关理论总结得很不错，详略得当，并加入了自己对于安全领域实践的理解。看了下作者，是阿里云安全团队的。对我司的人员能力还真的是可以信任的。 EP06 瑞幸：数字化重生之路，一切无关于咖啡 - 纵横四海丨Ready Go | 小宇宙 - 听播客，上小宇宙https://www.xiaoyuzhoufm.com/episode/66b97c3733591c27be654491","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://galaxyyao.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Terraform IaC 学习Tips","slug":"Terraform-IaC-学习Tips","date":"2024-08-06T16:00:00.000Z","updated":"2024-09-04T03:35:03.639Z","comments":true,"path":"2024/08/07/Terraform-IaC-学习Tips/","permalink":"https://galaxyyao.github.io/2024/08/07/Terraform-IaC-%E5%AD%A6%E4%B9%A0Tips/","excerpt":"背景在之前参与资产市场产品迭代的时候，就资产市场与研发平台的打通，研发团队就提出过使用IaC，实现“一键下行”。一键下行，指的是在下行复用组件资产的时候，将组件所依赖的资源也同时自动化开通，然后自动将组件部署在新开通的资源上。举个例子：假设4A组件需要依赖RDS、Redis、OSS，就在后台配置的云资源环境自动申请RDS、Redis、OSS。站在研发的视角，隐藏了资源申请的复杂性，加速了复用流程，提升了研发体验。在资产市场对外输出时，对接的云资源往往没有那么理想，必定是阿里云公共云资源。如果使用IaC，便于适配不同的云环境。 接下来即将开始的工作，可能也会涉及大量而频繁的云资源开通。最近在做Demo的时候，就深感开通和销毁云资源这个繁琐的事情有多浪费时间。以程序员的偷懒本性，自然想到了使用IaC来提升效率。 概念和快速入门指引可以直接查阅参考资料，就不copy &amp; paste了。只记录一些个人觉得实践时需要关注的点。由于自家的关系，以下云资源默认为阿里公共云。 2. 工作中可能用到IaC的业务场景IaC核心是版本控制和可重复，实现提效、降低误操作、一致性与合规安全。适合IaC的业务场景是什么？企业上云、环境复制、环境重建、合规管控等。 3. 学习时的几个选型3.1 Terraform vs. 阿里云ROS","text":"背景在之前参与资产市场产品迭代的时候，就资产市场与研发平台的打通，研发团队就提出过使用IaC，实现“一键下行”。一键下行，指的是在下行复用组件资产的时候，将组件所依赖的资源也同时自动化开通，然后自动将组件部署在新开通的资源上。举个例子：假设4A组件需要依赖RDS、Redis、OSS，就在后台配置的云资源环境自动申请RDS、Redis、OSS。站在研发的视角，隐藏了资源申请的复杂性，加速了复用流程，提升了研发体验。在资产市场对外输出时，对接的云资源往往没有那么理想，必定是阿里云公共云资源。如果使用IaC，便于适配不同的云环境。 接下来即将开始的工作，可能也会涉及大量而频繁的云资源开通。最近在做Demo的时候，就深感开通和销毁云资源这个繁琐的事情有多浪费时间。以程序员的偷懒本性，自然想到了使用IaC来提升效率。 概念和快速入门指引可以直接查阅参考资料，就不copy &amp; paste了。只记录一些个人觉得实践时需要关注的点。由于自家的关系，以下云资源默认为阿里公共云。 2. 工作中可能用到IaC的业务场景IaC核心是版本控制和可重复，实现提效、降低误操作、一致性与合规安全。适合IaC的业务场景是什么？企业上云、环境复制、环境重建、合规管控等。 3. 学习时的几个选型3.1 Terraform vs. 阿里云ROS对于个人而言，暂时没精力掌握两种IaC语法。考虑到后续工作会涉及跨云平台的资源编排和管理，优先选择了Terraform。 3.2 Terraform vs. AnsibleAnsible于 2015 年被Red Hat收购。从当前的厂商Redhat自己的说法，两者可以结合。 使用Terraform创建云资源：使用Terraform创建虚拟机、网络、存储等云资源，确保环境的一致性和可重复性。 使用Ansible配置软件：在创建完云资源后，使用Ansible来安装、配置和管理软件，确保每个虚拟机都具有所需的配置。如果需要在虚拟机之间进行复杂的配置和协调，可以优先考虑使用Ansible，因为它在主机内部执行操作，更适合执行复杂的系统管理任务。 不过我的个人未经实践的粗浅见解是，如果是有很强能力的互联网行业客户，可以考虑分别利用这两套解决方案各自的长处。但对于大量泛企业客户，同时运用两套解决方案对于运维的学习成本会更高（特别是对运维人员能力一般的企业）。此外，在云原生时代，直接通过镜像拉起容器，在主机内部执行操作是需要尽量减少的运维行为。以国内的运维条件，直接到主机内部执行操作肯定还是无法避免。不过如果这种是低频偶发操作，是不是靠人工+文档，比引入Ansible成本更低？如果是基于纯阿里公共云的话，还有OOS（系统运维管理）云产品可能也可以替代Ansible。 Anyhow，当前我个人测试和做demo的时候还是以纯Terraform的方案。 参考资料： Ansible与Terraform的应用场景和优劣势 - bestsrc 一文了解 Terraform 与 Ansible 的区别是什么？ 3.3 是否将Terraform用于应用部署阿里云的官方文档有使用Terraform分别在ECS和K8S集群上部署Wordpress的实践教程。典型案例里也有使用Terraform快速拉起幻兽帕鲁服务的案例。但实际搜索应用编排，主要内容是在云效的AppStack文档中。即从阿里云的业务逻辑来看，应用编排从逻辑上更贴合DevOps。我个人还是认同这个逻辑的。上面案例中，不管是部署Wordpress，还是幻兽帕鲁的服务，都是比较成熟的服务。而对于快速迭代的企业应用，还是适合配合DevOps来落地应用编排，而非ROS（资源编排服务）。不过也有对Terraform深度探索使用的企业（特别是游戏行业），不仅云资源，DevOps部署应用也使用Terraform编排完成。 讲到了应用编排，顺带一提，在查资料的时候看到了华为对其资源编排的分享的材料，发现一个挺有意思的细节。在那份材料里，华为将资源编排与应用编排分为两个不同的产品，并按照Shell脚本（命令行级别自动化）-&gt;配置管理自动化（Puppet&#x2F;Chef&#x2F;Ansible&#x2F;Salt）-&gt;资源供给自动化（Terraform&#x2F;Docker Compose）-&gt;资源编排（AWS-Cloudformation&#x2F;Ali-ROS&#x2F;HC-RTS）-&gt;应用编排（AWS-CFN&#x2F;Pivotal-BOSH&#x2F;HC-AOS）的逻辑，来描述编排服务的发展趋势来描绘云上自动化。但从这张图很容易误解资源编排和应用编排是有发展递进关系。当前，资源编排和应用编排已经都合并到华为的同一个资源编排服务（RFS）的产品里，估计也是做了云产品治理与整合。但应用编排的产品页和产品代号理论上应该随之下线，但还保留着，透露出内部组织调整的影子。 4. 几个知识点在实践了几个Hello world级的demo后，整理了几个自己关注的知识点。 4.1 身份认证在提到身份认证之前，记录一下自己遇到过的弱智问题。使用Terraform在创建VPC的时候，发现可以通过页面创建上海区域可用区的VPC，但没法使用Terraform创建，会报错：”Message: code: 400, Resource you requested is not available in this region or zone. “。但在杭州区域区域，不管是页面还是Terraform都可以成功创建。提了工单，经过售后工程师提醒才发现，Terraform初始配置的时候需要配置一个环境变量： 1export ALICLOUD_REGION=&quot;cn-hangzhou&quot; 环境变量是前一个周五配置的，隔了一个周末继续做demo的时候，自己忘记了。。。在写HCL脚本的时候还在奇怪，怎么VPC不需要指定区域的。 这里就涉及到初始化本地Terraform的时候一个设置，就是将AK和SK设置到本机的环境变量里。Terraform要操作云资源，操作交互自然是需要带身份认证的。官方文档介绍了几种方案，包括通过terraform命令传参方式（-var参数）、环境变量。如果是在ECS上执行，还可以通过ECS服务角色、角色扮演、OIDC角色扮演。这3个也是生产环境到的建议方式，无需直接暴露AK&#x2F;SK。 不过话又说回来，我能理解需要将AK和SK放到环境变量里，还是不太理解为什么默认要将区域也放到环境变量里，而不是也放到脚本里维护。盲猜是由于ROS的HCL脚本本身就是按区域分别维护的原因。 4.2 变量HCL2中，变量可以不用加花括号了。即可以直接用var.region，而不需要使用&quot;$&#123;var.region&#125;&quot;。（这是IntelliJ IDEA里提示后才知道的） 4.3 模块化Terraform也支持代码复用，形式就是模块。模块可以理解为包含一个或多个资源的模板，对应Terraform代码里的一个目录。模块使用的典型场景，包括： 需要重复创建多个相同资源（可以配合count或for each语法） 在不同的项目或环境，调用相同的模块。通过传入不同项目或环境的参数区分 有最佳实践推荐每个基础module尽可能只包含同一产品的相关资源。 阿里云的常用module其实可以直接从terraform官网引用，参见：Browse Modules | Terraform Registry，可以在线引用，例如： 12module &quot;vpc&quot; &#123; source = &quot;alibaba/vpc/alicloud&quot; 用在线的还是用自己封装的，就看个人判断了。用在线版本可能会有2个问题： init的时候有可能会连不上github导致失败 拉取时间变长阿里云的alibabacloud-automation项目里的module的代码风格也多少有些差异，猜测是没有制定统一规范，由不同云产品团队自己写的关系。个人觉得可以作为参考和学习资料。 4.4 Terraform后台（Backend）存储状态文件默认情况下，Terraform 状态文件（.tfstate，即Terraform State，保存了资源配置和生成资源的元数据）会存储在本地文件系统上。然而，在团队环境中，需要更安全、持久或可共享的状态管理。使用Git托管是不行的，会产生团队多人同时操作情况下产生的状态文件更新及时性问题。例如A刚运行完apply，本地更新了state文件，还没来得及上传git，这时候B也运行的话，就可能导致A创建的资源被销毁。最佳实践是放到OSS Bucket远程存储，配置后台通常在 Terraform 配置文件（terraform.tf 或 backend.tf）中完成。参考资料： Terraform State-阿里云帮助中心。 如何使用Terraform OSS Backend_Terraform(Terraform)-阿里云帮助中心 4.5 存量资源导入Terraform除非是新创建或新迁云的企业，不然肯定会有相当数量的存量资源没有通过Terraform管理。此外还会遇到： 虽然资源已经使用Terraform管理，但由于某些情况，是通过控制台对云资源做了属性变更 Terraform模版过于复杂而拆分 这种情况下，就需要使用Terraform的DataSource来获取资源ID，声明要导入的资源，然后通过terraform import命令导入。如果是测试使用，也可以打上被污染的标记，让terraform销毁后重新创建。 参考资料： 如何将资源导入Terraform资源栈_资源编排(ROS)-阿里云帮助中心 如何使用Terraform解决存量云资源管理难题_Terraform(Terraform)-阿里云帮助中心 按照Google的建议，要避免导入现有资源。给出的理由是：因为这样做可能会很难完全了解手动创建的资源的来源和配置。应通过 Terraform 创建新资源并删除旧资源。当然这是理想情况，Google自己也没死板地要求一定要采用。 5. 工程最佳实践虽然在demo的时候，可以用一个main.tf搞定。但既然要系统性地学习，还是学彻底。所以也整合了Terraform工程最佳实践的资料。 5.1 目录结构从 Terraform目录，可以看到一般会分根模块和子模块。每个模块里包含： CHANGELOG.md README.md locals.tf main.tf variables.tf outputs.tf此外根模块里包含providers.tf。如果资源复杂，资源配置代码较长，可以按照资源类型单独使用一个独立的 .tf 文件来存放，例如，用于 ECS 实例、OSS Bucket 和数据库的配置可以分别放在 instance.tf，oss.tf 和 database.tf 中。（这种情况是不是还不如封module？） 5.2 Google方案参考按照Google的建议，使用environments目录来方每个环境的配置。 12345678910111213141516171819202122-- SERVICE-DIRECTORY/ -- OWNERS -- modules/ -- &lt;service-name&gt;/ -- main.tf -- variables.tf -- outputs.tf -- provider.tf -- README -- ...other… -- environments/ -- dev/ -- backend.tf -- main.tf -- qa/ -- backend.tf -- main.tf -- prod/ -- backend.tf -- main.tf 此外，按照Google的风格，也是建议使用MonoRepo来管理所有的Terraform代码，并且由单个平台工程团队管理这个MonoRepo。也可以将Terraform配置拆分到不同的代码库。基础代码库：特定于应用和团队的代码库： 我将个人测试自用的项目，参考Google的MonoRepo+基础代码库+项目（服务）的目录结构方案，暂时拟定了个自用的目录结构： 1234567891011121314151617181920212223242526272829303132333435363738-- foundations/ -- vpc/ -- examples/ -- main.tf -- outputs.tf -- CHANGELOG.md -- locals.tf -- main.tf -- outputs.tf -- README.md -- variables.tf -- ecs-instance/ -- ...other/ -- template/-- projects/ -- project-1/ -- modules/ -- main.tf -- outputs.tf -- CHANGELOG.md -- environments/ -- dev/ -- backend.tf -- main.tf -- sit/ -- backend.tf -- main.tf -- prod/ -- backend.tf -- main.tf -- locals.tf -- main.tf -- outputs.tf -- provider.tf -- README.md -- variables.tf -- ...other/ -- template/ foundations（基础设施）和projects里都包含了个template目录。新建模块和子项目的时候，可以从template里快速复制出来一份。根据实际使用体验再调整。 5.3 代码格式化代码格式化的规则不用记，运行terraform fmt -recursive就可以了。 5.4 gitignore文件.gitignore文件参考：gitignore&#x2F;Terraform.gitignore at main · github&#x2F;gitignore稍微修改了一下，增加了个人用的IDE的IntelliJ Idea的gitignore配置，以及感觉也需要加入的.terraform.lock.hcl文件 12345678910111213141516171819202122232425262728293031323334353637383940# Local .terraform directories**/.terraform/*# .tfstate files*.tfstate*.tfstate.*# Crash log filescrash.logcrash.*.log# Exclude all .tfvars files, which are likely to contain sensitive data, such as# password, private keys, and other secrets. These should not be part of version# control as they are data points which are potentially sensitive and subject# to change depending on the environment.*.tfvars*.tfvars.json# Ignore override files as they are usually used to override resources locally and so# are not checked inoverride.tfoverride.tf.json*_override.tf*_override.tf.json# Ignore transient lock info files created by terraform apply.terraform.tfstate.lock.info# Include override files you do wish to add to version control using negated pattern# !example_override.tf# Include tfplan files to ignore the plan output of command: terraform plan -out=tfplan# example: *tfplan*# Ignore CLI configuration files.terraformrcterraform.rc# IDE.idea/* 注意点：.terraform.lock.hcl这个是依赖项锁文件，需要纳入版本控制，有助于跟踪和审核指定配置的提供商选择变化。 5.5 Google的其他最佳实践变量相关： 为变量设置type字段：避免变量类型错误 如果参数包含敏感信息，在其对应的变量中将sensitive设置为true 表示数值的输入、局部变量和输出（例如磁盘大小或 RAM 大小）必须使用单位命名（例如 ram_size_gb） 对于具有与环境无关的值的变量（例如磁盘大小），请提供默认值 对于具有特定于环境的值的变量（例如 project_id），请勿提供默认值。这样，调用模块必须提供有意义的值。 对于根模块，请使用 .tfvars 变量文件提供变量。为了保持一致性，请将变量文件命名为 terraform.tfvars。命令行选项是临时性的，容易忘记。使用默认变量文件更容易预测。模块相关： 在每个模块中，添加 Markdown 格式的 README.md 文件。在 README.md 文件中，添加有关模块的基本文档 使用资源各自的文件和描述性名称（例如 network.tf、instance.tf 或 loadbalancer.tf）创建资源的逻辑分组 将示例放在 examples&#x2F; 文件夹中，并为每个示例提供单独的子目录。对于每个示例，请添加详细的 README.md 文件。命令相关： 使用下划线命名所有配置对象，以分隔多个字词 将资源名称设为单数形式 在资源名称中，请不要重复资源类型。例如避免：resource &quot;google_compute_global_address&quot; &quot;main_global_address&quot; &#123; … &#125;输出： 请勿直接通过输入变量传递输出，因为这样做会阻止输出正确地添加到依赖关系图中数据源： 将数据源放在引用它们的资源旁边。 如果数据源数量很大，请考虑将它们移动到专用 data.tf 文件中。自定义脚本： 限制自定义脚本的使用。Terraform 不会考虑或管理通过脚本创建的资源的状态。 将 Terraform 调用的自定义脚本放在 scripts&#x2F; 目录中。其他： 建议指定Provider版本：防止Provider更新引入问题，保障稳定性 将静态文件放在单独的目录中 最大限度地减少每个根模块中的资源数量。一般规则：在单个状态下，最好不要超过十几个。 不要手动修改 Terraform 状态 5.6 无网络情况下运行客户专有云环境，特别是生产环境，基本都是无法连互联网的。首先在有网络环境的机器把当前目录的插件复制到特定目录： 1terraform providers mirror /data/terraform/plugins 拷贝到无网络环境的机器，运行时指定插件目录： 1terraform init -plugin-dir=/data/terraform/plugins 参考资料：《Terraform 101 从入门到实践》 第二章 Providers插件管理 - 南瓜慢说官网 5.7 Terraform故障恢复和K8S不同，Terraform 不提供自动故障恢复功能，必须依赖故障时运行的脚本。不过故障的检测能力还是具备的。可以使用terraform state命令，检查Terraform管理的当前资源状态： 1terraform state list 这将输出一个列表，其中包含了资源类型和资源的唯一标识符（例如aws_instance.example）。如果想查看某个特定资源的详细状态，可以使用terraform state show命令，并提供该资源的地址作为参数。例如，如果要查看名为example的AWS EC2实例的状态，可以执行： 1terraform state show aws_instance.example 5.8 快捷命令用得次数多了之后，也可以自己设置一些alias来简化需要敲的命令： 123456# terraformalias tfmt=&#x27;terraform fmt -recursive&#x27;alias tinit=&#x27;terraform init&#x27;alias tapply=&#x27;terraform apply -auto-approve&#x27;alias tdestroy=&#x27;terraform destroy -auto-approve&#x27;alias tplan=&#x27;terraform plan&#x27; 6. 总结以上内容都只是Demo测试+搜索后的资料整理，未经历实际项目和客户的实践验证，只能算是纸上谈兵。如果后续有机会在真实客户或项目中实践，会修正其中错误的内容。 X. 参考资料Terraform入门什么是Terraform_Terraform(Terraform)-阿里云帮助中心https://help.aliyun.com/document_detail/95820.html 为什么选择Terraform_资源编排(ROS)-阿里云帮助中心https://help.aliyun.com/zh/ros/user-guide/overview-2 如何在本地安装和配置Terraform_Terraform(Terraform)-阿里云帮助中心https://help.aliyun.com/document_detail/95825.html 快速手册Terraform有哪些常用命令_Terraform(Terraform)-阿里云帮助中心https://help.aliyun.com/document_detail/145531.html 常见的Terraform模板示例有哪些_资源编排(ROS)-阿里云帮助中心https://help.aliyun.com/zh/ros/user-guide/examples-of-terraform-templates Terraform 身份认证-阿里云帮助中心https://help.aliyun.com/document_detail/2837050.html 《Terraform 101 从入门到实践》 第五章 HCL语法 - 南瓜慢说官网https://www.pkslow.com/archives/terraform-101-hcl 最佳实践参考Terraform代码的开发方式和建议是什么_资源编排(ROS)-阿里云帮助中心https://help.aliyun.com/zh/ros/user-guide/methods-and-suggestions-for-terraform-code-development 使用 Terraform 的最佳实践 | Google Cloudhttps://cloud.google.com/docs/terraform/best-practices-for-terraform?hl=zh-cn","categories":[],"tags":[{"name":"IaC","slug":"IaC","permalink":"https://galaxyyao.github.io/tags/IaC/"}]},{"title":"金融业务-不动产保理业务入门和系统设计","slug":"金融业务-不动产保理业务入门和系统设计","date":"2020-07-31T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2020/08/01/金融业务-不动产保理业务入门和系统设计/","permalink":"https://galaxyyao.github.io/2020/08/01/%E9%87%91%E8%9E%8D%E4%B8%9A%E5%8A%A1-%E4%B8%8D%E5%8A%A8%E4%BA%A7%E4%BF%9D%E7%90%86%E4%B8%9A%E5%8A%A1%E5%85%A5%E9%97%A8%E5%92%8C%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/","excerpt":"1.保理的概念保理，从本质上来说，就是应收账款的融资服务。举个场景:某桂园向某混凝土公司A采购了2000吨水泥，应收账款100万。但由于账期原因，应收账款是按照季度结算。但公司A因为款项没有即时结清，产生了资金周转问题。于是公司A就将应收账款以折扣价转让给了保理商B。保理商B给供应公司A提供了融资，并通知某桂园回款后续不再打给公司A，而是打给保理商自己。季末某桂园将回款打给了保理商B。保理商B在融资和回款的差价里赚到了收益。整个流程可以参见下图 在这个最基础的流程中，有三方： 卖方:某混凝公司A，也可以称为债权方、上游 买方:某桂园，也可以称为债务方、下游 保理商:分为商业机构进行的商业保理和银行保理 1.1 正向保理和反向保理保理这个概念产生的时候，都是由拥有融资需要的卖方主动发起的。但卖方拿到融资却不供货转身跑路的情况，也不是不会发生。这种情况下，买方当然也不会为没有收到的货而白白付钱。保理为了尽量避坏账，会对卖方的资质和规模进行要求。但现实中很常见的情况是：上游的卖方是中小企业，无法达成资质规模要求，尽调难度也很大；而下游买方是龙头企业，拥有较高的资信程度。为了在这种场景下也能让保理商放心融资，会由卖方（混凝土公司A）找买方（某桂园）做担保，由买方主动发起保理申请。买方为了保证上游供应链的稳定，出面找保理商做担保：公司A确实是我的上游供应商，我们有商务合作。如果你能信得过我的话（大企业的授信担保），就给公司A融资，然后在一定时间段后到我这里兑回款。保理商相信了A的资信，给公司A提供了融资。在这里出现了一个核心企业的概念。核心企业是供应链中的概念,是供应链中的关键节点，资信程度较高（AA＋是基本门槛）。 在保理流程中，核心企业是保理的发起方。核心企业的类型也是区分正向保理和反向保理的关键判断因素。核心企业是卖家，就是正向保理；核心企业是买家，就是反向保理。这里的“正”和“反”指的是相关交易链的方向。","text":"1.保理的概念保理，从本质上来说，就是应收账款的融资服务。举个场景:某桂园向某混凝土公司A采购了2000吨水泥，应收账款100万。但由于账期原因，应收账款是按照季度结算。但公司A因为款项没有即时结清，产生了资金周转问题。于是公司A就将应收账款以折扣价转让给了保理商B。保理商B给供应公司A提供了融资，并通知某桂园回款后续不再打给公司A，而是打给保理商自己。季末某桂园将回款打给了保理商B。保理商B在融资和回款的差价里赚到了收益。整个流程可以参见下图 在这个最基础的流程中，有三方： 卖方:某混凝公司A，也可以称为债权方、上游 买方:某桂园，也可以称为债务方、下游 保理商:分为商业机构进行的商业保理和银行保理 1.1 正向保理和反向保理保理这个概念产生的时候，都是由拥有融资需要的卖方主动发起的。但卖方拿到融资却不供货转身跑路的情况，也不是不会发生。这种情况下，买方当然也不会为没有收到的货而白白付钱。保理为了尽量避坏账，会对卖方的资质和规模进行要求。但现实中很常见的情况是：上游的卖方是中小企业，无法达成资质规模要求，尽调难度也很大；而下游买方是龙头企业，拥有较高的资信程度。为了在这种场景下也能让保理商放心融资，会由卖方（混凝土公司A）找买方（某桂园）做担保，由买方主动发起保理申请。买方为了保证上游供应链的稳定，出面找保理商做担保：公司A确实是我的上游供应商，我们有商务合作。如果你能信得过我的话（大企业的授信担保），就给公司A融资，然后在一定时间段后到我这里兑回款。保理商相信了A的资信，给公司A提供了融资。在这里出现了一个核心企业的概念。核心企业是供应链中的概念,是供应链中的关键节点，资信程度较高（AA＋是基本门槛）。 在保理流程中，核心企业是保理的发起方。核心企业的类型也是区分正向保理和反向保理的关键判断因素。核心企业是卖家，就是正向保理；核心企业是买家，就是反向保理。这里的“正”和“反”指的是相关交易链的方向。 相比正向保理，反向保理几个优势： 上游量中小企业融资难的问题得以解决 便于建立长期的战略合作伙伴关系，供应链更加稳定 提高了保理业务的工作效率。保理商不再需要着重审核供应商的财务情况。反而需要更关注核心企业贸易背景的真实性，预防核心企业和上游供应串谋欺诈风险 核心企业的资金运用效率得以提升，减少核心企业的财务管理成本 此外，由于保理业务需要债务人确权，所以在正向保理中可能会发生债务人不愿意配合的情况。不愿意配合的主要原因是在转让前，买卖双方的应收账款仅属于商业信用，强势买方在账款到期后，有继续延期支付的可能。在这种情况下，卖方为自身经营利益考虑，也只能被动同意。但是若将应收账款转给保理商之后，商业信用负债就变成了需要刚性兑付的金融负债，到期不还款就会被起诉的可能。所以买方是很不情愿将债权转给保理商的。但反向保理中，由于本身就是债务人发起的，所以基本不会存在类似的问题。 1.2 ABS资产证券化和专项计划放款给卖方的资金可以是保理商的自有资金，但相较把应收账款捂1年等买方回款，还有其他资金周转率更高的方式，例如将应收账款进行资产证券化（ABS）。ABS，即“Asset Backed Securitization”（资产证券化），是一种非常常见的融资方式。常见的保理ABS交易架构如下： 我们扩展一下之前提到的混凝土公司A和某桂园的案例： 公司A提供给对某桂园水泥，应收账款债权100万。如果不通过保理，正常还款账期是3个月 为了更快收回款项，公司A将应收账款债权折价至94万，转让给了保理商B，使账期缩短到了半个月 保理商B以95万元的价格将应收账款债权打包发行ABS（即票面发行利率5%），设立专项计划。通过资产证券化过程中履行合同审核、确权等义务，收取1万元的中介费。如果债务人是某桂园的子公司的话，会加入母公司作为共同付款人来增信 ABS发行一年后，某桂园偿还本息100万元或更多 价差的5万，在扣除管理费&#x2F;销售成本&#x2F;增值税&#x2F;律师费&#x2F;评级费&#x2F;托管费等费用后，剩余的就是证券持有人的收益 从形式上，这个保理流程中，公司A贴息5万元，但实际由于某桂园的账期从3个月延长到了1年，所以可能会以某种形式承担部分利息，返还至公司A 归纳整个流程，实现了买方&#x2F;卖方&#x2F;保理商三方的共赢： 对卖方：实现了快速融资。换个角度，等于公司A做了2.5个月的融资，利息6万，相比从银行融资更容易成功，成本更低。并与核心企业建立了更牢固的合作伙伴关系 对买方：变相拉长了账期。换个角度，等于某桂园以极低的成本融到了9个月的100万元资金。作为牵头发起保理的代价，还可以向卖方后续要求优先供货权等利益 对保理商：获得了低风险的收益 以上范例仅为简化版的范例，实际并不会对单个供应商的应收账款建立ABS。一个ABS产品中通常会包含数十笔到数百笔不等的应收账款。资产池至少包含10个相互之间不存在关联关系的债权人（供应商）。 1.3 再保理除了将债权ABS化之外，还有另一种处理方式是将转给其他保理商，即再保理。常见的是转给银行，即银行再保理。 与国外相反，国内由大部分金融资源集中在银行，加上政策原因，所以是银行率先开展保理业务。而商业保理是从2012年之后商务部发文批准后才开始发展。由于起步较晚，商业保理公司普遍轻资产，资产负债率较高，无法直接从银行获得贷款。但如果拥有银行认可的优质保理资产，可以通过再保理的形式，将这些资产转计给银行或其他保理商，获得融资。具体流程如下： 我们从再保理的角度扩展一下混凝公司A、某桂园和保理商B的案例： 保理商B除了为混凝公司A之外，还为某桂园的一系列供应商提供保理服务，例如混凝土公司X，装修公司Y和园林公司Z 保理商B想进一步扩大业务规模，希望基于银行C与某园合作的基础上，将自己手头的公司XYZ的应收账款转给银行C，获得融资 保理商B与银行C签订再保理合作协议，向银行C提供与供应商签的保理合司和票据等材料 某桂园向银行C出具应收账款转让确认书，确认银行C为新的债权人 银行C向保理商B发放融资 保理商B将融资用于扩大业务规模，向更多的公司U公司V公司W提供保理业务 其实从标准的保理概念上，应该称之为“双保理”。即保理商B是卖方保理，银行保理C为买方保理。 2不动产保理业务和保理系统的特点对于不动产保理ABS业务，实践中通常有以下几个特点： 以反向保理为主：通常采用“1＋N”模式。1即供应链核心企业，N为上游供应商 具有类信用债特点：通常直接债务人为由核心企业针对特定地产项目创建的项目公司。对核心企业主体信用依赖强，供应商往往规模小，追索权意义不大，所以大多采用了无追索权保理的形式 平层结构为主：即通常不设外部增信。同样由于强依赖核心企业主的还款力和意愿，对于优先级证券的信用级别没有提升作用，保证担保差额补等外部措施也并不会起到增信作用 通常采用到期一次还本付息的本息兑付方式。主要是由于项目期限短（通常为1年或1年以下） 多采用了储架发行模式和“黑红池”机制。主要是由于存在期限短、单笔金额小、笔数多、同质化高的特点 储架发行，即“一次备案，多次发行” 黑红池机制中“红池”指前期审批报备阶段所使用的基础资产池，而“黑池”则是在证券发行时使用的基础资产池 这些特点使得不动产保理系统在设计过程中产生了以下几个特点： 需要包含供买方&#x2F;卖方&#x2F;核心企业使用的外部客户门户系统 核心企业成为买方和资产的必填要素 需要与核心企业进行系统对接 保理产品的设计较单纯（相较其他一些公私募产品） 尽调管理以核心企业为查询维度 还款管理和催收管理弱化 3.资产的属性及和其他业务对象的关联关系不动产保理中的核心概念是资产。资产就是应收账款。资产本身的主要因素包括:发票编号、发票金额、资产转让金额等。 资产与众多业务对象之间都有关联关系： 接下来围绕资产和不同业务对象的关联关系展开描述。 3.1 供应商&#x2F;项目公司&#x2F;核心企业这里的项目公司即上文提到的买方（债务人），供应商即卖方（债权人）。所以资产和三者的关系是： 供应商卖出资产 项目公司买入资产 核心企业增信资产 在资产导入阶段，如果资产对应的供应商和项目公司未进行准入审核，会生成审核任务，并通过短信通知供应商和项目公司登录客户门户自助补充资料（实物、银行账户、联系人等）。完成准入审核是合同签订的必要前置条件。 准入审核的关键信息包括：公司名称、统一社会信用代码、用印方式、营业执照、资质证书、法人身份证、授权用印经办人授权书影像、授权用印经办人身份证影像等。其中对非结构化数据（例如营业执照的影像资料）的审核也被称为实物审核。 在新增项目公司和供应商后，内部也会开展尽调（尽职调查）。除了对公司的尽调外，也会对资产进行尽调，以确保不是通过虚假交易骗保（理）。在企业准入成功后，会在电子签章系统里为其注册账号，用于后续的合同签署。 3.2 产品&#x2F;投资项目&#x2F;项目&#x2F;资产包产品&#x2F;投资项目&#x2F;项目&#x2F;资产包，这几个概念从上往下，从抽象到具体。 产品产品可以是针对某个核心企业的业务模式，比如“某桂园私募ABS反向保理”，“某科再保理”等。 产品的关键信息包括产品类型、还款方式、宽限期、资金规模、债权期限天数、利率范围等。其中产品类型是针对主体（正向&#x2F;反向保理）+资产处置方式类别。例如： 公募ABS反向保理 私募契约型基金反向保理 私募信托反向保理 供应链ABS正向保理 银行再保理 投资项目投资项目，是保理商公司内部进行投决（投资决策）的投资活动单位。所以投资项目的关键因素包括立项会、投决会等会议的信息、审批规模、投资团队、资金来源等。 项目资产包根据保理的不同阶段，项目可以分为保理项目和资产处置项目。保理项目是合同签署的对象单位。对于保理项目，关键信息包括签约方式，资产到期日、折现期、折价率等。资产处置项目是将资产金融化的对象单位，会补充对应的金融产品的信息，包括金融产品代码（例如基金代码）评级、服务费、律师事务所、托管人、评级机构、专项计划户名账号等等。 资产包项目和资产包对应。保理项目和资产处置项目分别对应预备资产包和（处置）资产包。资产包是应收账款的组合。会从预备资产池中选出符合要求的应收账款进行打包处理。上文也提到过，如果要包装为ABS产品的话资产包中至少需要包含10个相互之间不存在关联关系的供应商。当资产包中的资产确定，进行合同签署后，就会转为（处置）资产包。后续资产包中的资产就不可再发生变化。 3.3 中登登记《民法典草案》第七百六十八条规定：应收账款债权人同一应收账款订立多个保理合同，致使多个保理人主张权利的，已经登记的先于未登记的取得应收账款；均已经登记的，按照登记时间的先后顺序取得应收账款，均未登记的，由最先到达应收账款债务人的转让通知中载明的保理人取得应收账款；既未登记也未通知的，按照保理融资款或者服务报酬的比例取得应收账款。 这里的登记，指的就是到国家公认的中登网进行登记。中登网，全名中国人民银行征信中心动产融资统一登记平台。中登网的主要作用是用于以融资为目的的动产登记公示。对于保理业务来说，中登网的主要作用是收质押登记和应收账款转让。如果没有在中登网上进行登记，那么万一项目公司背着保理商，偷偷将应收账款进行多次质押引起纠纷，在法律上就会处于不利地位。 当资产成功导入且包含发票，就需要进行中登查询和登记。首先进行中登查询。中登查询不通过的话就会进入到退单流程；而中登查询通过的话就进入登记流程。中登登记的关键因素包括：应收账款的出让人信息（供应商企业信息）、资产信息、受让人信息（保理商企业信息）等。 3.4 合同《民法典草案》第七百六十二条规定：保理合同的内容一般包括业务类型、服务范围、服务期限、基础交易合同情况、应收账款信息、转让价款、服务报酬及支付方式等条款。合同签约包括两种方式:纸质和在线。保理商分别于如果是在线签，为了确保合同的合规，会通过第三万电子签章服务进行合同的签署和生成（如E签宝、上上签）。 3.5 放款还款当资产和实物审核通过，并在中登网登记成功，内部合同用印完毕后，即可进入向供应商的放款申请审批。由于放款申请所需的绝大部分信息在该步骤之前都已补充完华（供应商信息&#x2F;资产转让金额），所以只需要补充支付方式（商票支付&#x2F;现金支付）即可。当处置资产的到期日到达后，会创建还款登记和确认任务。还款登记的主要因素包括本次还款日期还款金额、还款凭证等。还款登记完成后，就可以进行财务流水匹配和制证处理。 4. 保理流程经过前一章的介绍，我们就可以将资产在整个保理流程中的流转过程梳理出来 首先在前置阶段，预先配置好资产所属和核心企业以及上层的产品&#x2F;投决项目信息 在投前（保理项目）阶段，进行供应商和项目公司的信息补充和审核。同步进行资产导入和审核，并将资产到中登网上进行登记 进入合同签署，资产包里的资产固定，进入投后（处置项目）阶段。在该阶段进行放款资产出售以及收款 在保理流程的不同阶段，资产包的类型也会发生变化。在投前阶段，是从预备资产池生成预备资产包，并退回审核不通过的资产；在投后阶段，预备资产包变为处置资产包 5. 参考资料正向保理和反向保理有什么不一样 供应链金融之反向保理资产证券化模式 浅析“再保理”业务 一文读懂：房地产供应链应付账款ABS","categories":[],"tags":[{"name":"金融","slug":"金融","permalink":"https://galaxyyao.github.io/tags/%E9%87%91%E8%9E%8D/"}]},{"title":"运维-运维体系标准化之故障管理","slug":"运维-运维体系标准化之故障管理","date":"2020-07-29T16:00:00.000Z","updated":"2021-05-21T05:43:19.000Z","comments":true,"path":"2020/07/30/运维-运维体系标准化之故障管理/","permalink":"https://galaxyyao.github.io/2020/07/30/%E8%BF%90%E7%BB%B4-%E8%BF%90%E7%BB%B4%E4%BD%93%E7%B3%BB%E6%A0%87%E5%87%86%E5%8C%96%E4%B9%8B%E6%95%85%E9%9A%9C%E7%AE%A1%E7%90%86/","excerpt":"本文是极客时间《赵成的运维体系管理课》的读后体会之二 。 1.对故障的认识ITIL的10个重要的IT管理关键模块之一就是故障管理。故障永远只是表面现象，其背后技术和管理上的问题才是根因 即当技术和管理上的问题积累到一定程度后，就会以故障的形式爆发出来。所以不能仅将眼光限于故障本身和直接责任人。 管理者要先自我反省:员工只是执行者，管理者的责任永远大于执行者 强调用技术解决问题，而不是单纯地靠增加管理流程和检查环节来解决问题 短期可以辅以一些管理措施，比如靠宣传学习必要的Double Check&#x2F;制定复杂操作的Checklist等。但是这些只能作为辅助手段，一定不能是常态 随着系统复杂度越来越高，迟早有一天会超出单纯人力的认知范围和掌控能力，各种人力的管理成本也会随之上升，所以最终必须将这些人为动作转化到技术平台中去 2.故障的定级故障需要有标准化的流程来指导我们的处理过程。 这里有个关键组织:故障应急小组。这个组织有4个职责：","text":"本文是极客时间《赵成的运维体系管理课》的读后体会之二 。 1.对故障的认识ITIL的10个重要的IT管理关键模块之一就是故障管理。故障永远只是表面现象，其背后技术和管理上的问题才是根因 即当技术和管理上的问题积累到一定程度后，就会以故障的形式爆发出来。所以不能仅将眼光限于故障本身和直接责任人。 管理者要先自我反省:员工只是执行者，管理者的责任永远大于执行者 强调用技术解决问题，而不是单纯地靠增加管理流程和检查环节来解决问题 短期可以辅以一些管理措施，比如靠宣传学习必要的Double Check&#x2F;制定复杂操作的Checklist等。但是这些只能作为辅助手段，一定不能是常态 随着系统复杂度越来越高，迟早有一天会超出单纯人力的认知范围和掌控能力，各种人力的管理成本也会随之上升，所以最终必须将这些人为动作转化到技术平台中去 2.故障的定级故障需要有标准化的流程来指导我们的处理过程。 这里有个关键组织:故障应急小组。这个组织有4个职责： 制定故障定级定责标准 对线上故障做出定级和定责 跟踪线上故障处理 组织故障复盘 故障应急小组需要有个负责人。在部分公司，这个负责人属于研发效率团队。定级定责标准等同于法律条款，而这个角色等同于法官。法官依法办事，做到公平公正。现实情况中，因为各方受到故障的影响不同，对故障影响的理解也不同，所以复盘过程中经常会出现下面这两种争执场景： 技术支持判定故障很严重，但是责任方认为没什么大不了的，不应该把故障等级判定到如此之高 技术支持认为故障影响较小，但是受影响方却认为十分严重，不应该将故障等级判定得这么低 所以需要有严格而明确的判定标准。故障定级标准的目标是要判定故障的影响程度，使各相关利益方能够基于统一的标准判断和评估。故障等级常见可以分为PO-P4共5个级别。PO最高P4最低。定级主要看3点：功能的核心程度，影响面，以及影响时间。核心程度有一些共通的标准（例如登录），也有各系统独有的业务衡量标准，所以需要基于每个系统与业务部门分别制定。影响时间是包含故障发生到完全解决的总时间。根据实际况，也可以调整为只计算工作时间的时间长度。如果影响时间超过一定时长，要进行故障升级。P故障通常是两个或以上P1故障的叠加造成。 2.1 故障定级范例下面是两个定级参考范例。首先是交易系统，主要以钱为衡量指标。 另一个是IM即时通信App的故障定级标准。 再次强调一下，为了避免日后引起争执，需要将定级标准在业务部门、产品团队、开发团队、测试团队和运维团队之间进行逐点的细节讨论，并达成最终一致的认可。这个标准可能覆盖不到有些特例，这个时候需要由应急小组的负责人根据已达成一致的标准＋自己的经验进行独立裁量。同时，在每季度或半年对标准进行一轮修订。需要注意的是要对故障应急小组，特别是应急小组的负责人树立绝对的话语权和决策权的制度。 3.故障应急处理故障发生后可能会产生很大的外部压力，并传递到研发团队。如果没有很好的故障应对措施，很容易陷入慌乱。 3.1 故障应急的原则在故障应急状态下，坚守的第一原则是：优先恢复业务而非定位问题。这需要事先有充足的预案准备以及故障模拟演练，也涉及各种稳定性保障措施，例如扩容，开关，限流降级等。 3.2 故障应急流程故障应急流程由故障应急小组来主导。对外同步信息，包括大致原因，影响面和预估恢复时长，同时屏蔽各方对故障处理人员的干扰；对内组织协调各团队相关人员集中处理。 故障的应急流程主要分为以下几个步骤： 确认故障的有效性 登记生产缺陷 将故障上报到可用性保障群里 故障的原因排查和讨论在该群里或者单独拉一个独立的故障处理群处理。如果相关人员比较集中在一个办公场所，则集中到会议室 对于处理时间比较长的故障，应急处理小组每隔15-30分钟对相关业务部门同步一次故障处理进程，并判断是否需要升级故障 确定故障处理方案，包括:正常业务流程处理提交数据修改单&#x2F;修改配置&#x2F;回滚&#x2F;紧急版本&#x2F;放到大版本 在故障确认处理完成后，关闭生产缺陷 组织故障复盘，产出故障分析报告，将问题记录到事件管理。故障分析报告需要同步给技术副总监、PMO，以及其他的产品、开发、测试和运维，以便后续吸取教训 根据事件管理的记录，进行故障数据分析 分析角度包括:每月故障数对比、每月故障处理时间对比近两月故障等级占比分布、近两月故障类别占比分布、近两月故障来源对比和近两月各业务组故障数对比 3.3 故障的信息通报对于每一级故障的知会人员的标准参考如下： P0&#x2F;P1：技术总监&#x2F;PMO P2：技术副总监&#x2F;PMO&#x2F;测试主管&#x2F;运维主管 P3&#x2F;P4：技术经理&#x2F;产品经理 4.故障的复盘首先要明确复盘的目的。复盘的目的是为了从故障中学习我到我们技术和管理上的不足，然后不断改进。切总将复盘过程和目的搞成追究责任或实施惩罚，这对于团队氛围和员工积极性的打击是非常大的。在复盘过程中，故障应急小组要起到关键作用，组织复盘会议。对于低级别的生产事故，在晨会夕会之后顺带进行；对于高级别的生产事故，需要专门安排时间进行。 复盘会议的环节包括： 故障的回顾 包括故障发生时间点，故障影响面，恢复时长，主要处理人或团队 故障处理时间线回顾 故障应急小组在整个过程记录时间点，以便真实再现整个故障处理过程 针对时间线合理讨论 比如为什么没有告警而是用户反馈的，响应时长是否符合规范，是否有预案和预案执行完成度，测试环节为什么没有发现等 故障应急小组负责人注意控制场面，务必注意对事不对人，及时干预和警告，避免演变成批斗会 确定故障根因 故障定级定责 对于高级别生产故障定责时可以仅少数相关人在场时进行，考虑责任人个人感受 产出故障分析报告 将问题记录到事件管理 对故障根因的讨论可以诸如： 是否是人员对业务不熟悉导致？ 是否有人为操作导致？如果是的话，是否能改为自动化？ 是否在代码静态扫描中包含但被忽略了？ 为什么容量不足没有更早发现？ 为什么没法快速定位？是监控不够，还是告警太多人员麻木？ 管理上，人员的on call机制是否及时？应对故障的协作方式上是否还能改进？ 此外，按季度、半年和全年的周期，需要进行周期内的故障案例总结会。总结会的目的包括： 分析故障趋势，观察是否需要进行人员安排的调整 发现共性的问题，贡献给整个研发团队 5.故障的定责定责的目的是为了责任到人，并且使责任人能够真真切切地认识到自己的不足之处，能够主导改进措施的落地。相比故障复盘的对事不对人，定责就是对人不对事了，所以不能一刀切，不能上纲上线，一定要慎重。对于故障的定责方式，也要根据故障的类型来划分。 5.1 高压线规则有一类是绝对不允许触碰的底线。对于这类高压线规则，需要让每个成员牢记在心，并经常重复提醒。例如： 未经发布系统，私自变更线上代码和配置 未经授权、严格的方案准备和评审，私自在业务高峰期进行硬件和网络设备变更 未经授权，私自在生产环境进行调测性质的操作 未经授权，私自变更生产环境数据 通过高压线去加强安全稳定意识，目的是要让每一个人对线上都心存敬畏。从经验来看，绝大多数的严重故障都是因为无意识或意识薄弱导致的，并不是因为单纯的技术能力不足等技术因素。很多人事后复盘时候最常讲的话就是：“我以为是没问题的，我以为是没影响的。”其实恰恰就是因为这种“想当然”，导致了严重故障。 对于高压线问题，碰一次就要疼一次。 5.2 鼓励做事，而不是处罚错误Google的专家有一句名言:理解一个系统应该如何工作并不能使人成为专家，只能靠调查系统为何不能正常工作才行。 每个人的技术能力提升，基本都是伴随着大大小小故障的发生、处理、复盘和改进。虽然我们不希望有故障发生，但是真的没有了故障，我们也就没有了真刀真枪实战成长的机会。我们对待故障一定要客观和辩证地理解，特别是对于管理者来说，对于故障，一定要有容忍度，一定要有耐心。我们的团队和人员，在这样一次次痛苦的经历后，各方面的能力都得到了锻炼，素养也一定会有大幅度提升。所以，对故障有容忍度，有耐心，我们的团队就会变得越来越强，对于故障的应对也会变得更加游刃有余。而一出故障就劈头盖脸地把团队和责任人骂一通，并且还要严厉处罚的方式，最后的效果就是严重打击士气，适得其反。特别是以下这些原因造成的故障： 员工积极主动地承担了一些极具挑战性的工作，需要尝试某个新技术或解决方案 业务高速发展时期，业务量成指数级增长时，团队人员能和经验水平整体上还没法很好地应对。这个时候可能任何一个小变动都是最后一根稻草 何况，如果不出问题，可能很多主管压根都没有关注过员工在做的事情，过程中是否有困难，是否需要支持等等，这本身就是管理者的失责。员工努力做事的积极性一旦被打击，变得畏首畏尾起来，也就谈不上什么技术进步和突破了而且想要再恢复起来也会非常困难，最终很大概率上会导致优秀人才流失。 5.3 定责和绩效非强挂钩在故障后直接谈及处罚，员工的情绪很可能会消极和抵触。例如“反正都是我的错，你说咋样就咋样”，或“凭什么罚我却不罚别人，又不是我一个人的问题”。员工害怕、甚至拒绝承担责任，宁可少做不做，也不愿多做多错，团队沟通成本上升，运作效率自然下降。特别是一个故障如果是涉及多方的，扯皮推诿就开始了，都想着把责任撇干净，甚至当众相互指责，这个负面效应杀伤力极大。所以可以考虑将定责放到季度、半年为维度，根据事件管理中的记录来整体判断。如果员工整体的表现都是不错的，甚至是突出的，说明员工已经改正或者那件事情确实是偶尔的失误导致，这种情况下员工仍然会有好的绩效。但如果是频繁出问题，这种情况就基于事实反馈，也会更加容易沟通。","categories":[],"tags":[{"name":"运维","slug":"运维","permalink":"https://galaxyyao.github.io/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"运维-运营体系标准化之配置管理CMDB","slug":"运维-运营体系标准化之配置管理CMDB","date":"2020-07-28T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2020/07/29/运维-运营体系标准化之配置管理CMDB/","permalink":"https://galaxyyao.github.io/2020/07/29/%E8%BF%90%E7%BB%B4-%E8%BF%90%E8%90%A5%E4%BD%93%E7%B3%BB%E6%A0%87%E5%87%86%E5%8C%96%E4%B9%8B%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86CMDB/","excerpt":"本文是极客时间《赵成的运维体系管理课》的读后体会之一。 运维配置管理实践中一些混乱现象在公司的运维配置管理实践中，存在一些混乱的现象： 信息安全收到fastjson的安全问题报告，但报告中的服务器对应的系统却有错位 信息安全收到了报告，发现tomcat的某个版本出现了漏洞需要升级，但不知道到底影响到哪些系统，只能逐个请每个系统的负责人判断 部分服务器（特别是开发坏境服务器）已经基本可以确认不再被使用，但服务器资源没有回收，每月持续占用硬件预算 某系统上线后发现中漏配置了一个域名，导致部分用户打开首页报错 redis缺少规划，多个系统公用一个redis集群的情况下，无法根据键（key）来区分每个系统占用了多少缓存。遇到内存不足的情况很难深入排查是哪个系统导致 生产消息队列中不少queue堆积着大量消息，但不知道是否还在用，不敢随意删除 这些情况很可能短期甚至长期内不会直接导致什么问题。可能就是让信息安全整理材料多费一点时间，开发排查多绕一点弯路，或多花一些硬件维持或扩容费用。但有些时候这些问题就是给未来出现的生产问题埋下隐患。硬件的归属，服务器的使用情况，应用域名管理，软件版本，应用与中间件的关联等等，都属于广义上的配置管理。所以归根到底，是配置管理的混乱。 ITIL和配置管理CMDB在ITIL（Information Technology Infrastrueture Library）中，有10个重要的IT管理关键模块。其中配置管理（CMDB）通常被认为是其他IT流程的基础。CMDB（Configuration Managoment DataBase），配置管理数据库，是与IT系统所有组件相关的信息库。它包含IT基础架构配置项的详细信息。在传统运维时代，CMDB的核心对象是资源，即网络和硬件设备。但在云计算和互联网运维时代，CMB的核心已经转变为了“应用”。随着微服务架构的推广，以应用为核心的注册中心、缓存、消息队列、数据库等都需要纳入配置管理的管理范畴。 以应用为核心的配置管理标准化可以包括：","text":"本文是极客时间《赵成的运维体系管理课》的读后体会之一。 运维配置管理实践中一些混乱现象在公司的运维配置管理实践中，存在一些混乱的现象： 信息安全收到fastjson的安全问题报告，但报告中的服务器对应的系统却有错位 信息安全收到了报告，发现tomcat的某个版本出现了漏洞需要升级，但不知道到底影响到哪些系统，只能逐个请每个系统的负责人判断 部分服务器（特别是开发坏境服务器）已经基本可以确认不再被使用，但服务器资源没有回收，每月持续占用硬件预算 某系统上线后发现中漏配置了一个域名，导致部分用户打开首页报错 redis缺少规划，多个系统公用一个redis集群的情况下，无法根据键（key）来区分每个系统占用了多少缓存。遇到内存不足的情况很难深入排查是哪个系统导致 生产消息队列中不少queue堆积着大量消息，但不知道是否还在用，不敢随意删除 这些情况很可能短期甚至长期内不会直接导致什么问题。可能就是让信息安全整理材料多费一点时间，开发排查多绕一点弯路，或多花一些硬件维持或扩容费用。但有些时候这些问题就是给未来出现的生产问题埋下隐患。硬件的归属，服务器的使用情况，应用域名管理，软件版本，应用与中间件的关联等等，都属于广义上的配置管理。所以归根到底，是配置管理的混乱。 ITIL和配置管理CMDB在ITIL（Information Technology Infrastrueture Library）中，有10个重要的IT管理关键模块。其中配置管理（CMDB）通常被认为是其他IT流程的基础。CMDB（Configuration Managoment DataBase），配置管理数据库，是与IT系统所有组件相关的信息库。它包含IT基础架构配置项的详细信息。在传统运维时代，CMDB的核心对象是资源，即网络和硬件设备。但在云计算和互联网运维时代，CMB的核心已经转变为了“应用”。随着微服务架构的推广，以应用为核心的注册中心、缓存、消息队列、数据库等都需要纳入配置管理的管理范畴。 以应用为核心的配置管理标准化可以包括： 元数据属性:系统名、 BA Owner&#x2F;开发Owner&#x2F;运维Owner 环境属性:拥有几套环境 逻辑实体属性:架构评审和变更设计 硬件和网络属性:服务器配置、SLB、域名、ip等 代码属性:编程语言、代码库地址、需求空间地址 应用日录属性:日志日录、应用目录、临时目录等 应用配置属性:端口号、jvm参数等 中间件属性:web容器、注册中心、缓存、消息队列、数据库等 最终目标是能达成传统CMDB和应用视角CMDB的统一。 也有开源的CMDB，比如腾讯的蓝鲸智云。可以参考它的架构 CMDB的维护和流转CMDB并不能只靠运维团队内部封闭来做，而要站在怎么能够打造和发挥出整个技术架构体系运维能力的视角。有部分信息也需要开发来提供，例如消息队列的queue名等。所以CMDB需要跨团队协作。一方面运维团队要主动出击，去沟通，去推进；另一方面，必须能得到上级主管甚至是更高层技术领导的支持。配置数据也需要保持持续维护。过时的配置数据等于没有，甚至更差。这个首先需要在所有运维人员的重要性认识上保持统一。此外，也对每个应用的负责运维和研发人员提出相应的问责机制。和货币类似，只是维护信息不会产生价值。只有把信息流转到开发，测试和信息安全等角色，对整体的研发效能和故障率做出贡献，才能使配置管理免于成为运维的纯负担。","categories":[],"tags":[{"name":"运维","slug":"运维","permalink":"https://galaxyyao.github.io/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"管理-远程办公项目管理经验总结","slug":"管理-远程办公项目管理经验总结","date":"2020-03-01T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2020/03/02/管理-远程办公项目管理经验总结/","permalink":"https://galaxyyao.github.io/2020/03/02/%E7%AE%A1%E7%90%86-%E8%BF%9C%E7%A8%8B%E5%8A%9E%E5%85%AC%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/","excerpt":"疫情期间积累了一些远程办公条件下的项目管理经验，稍微整理一下。我司从企业文化到网络硬件，不太具备远程办公的基因，要补课的地方就额外多。 1. 按小团队划分并设定第一责任人亚马逊CEO贝索斯提到过一个原则：如果两个披萨饼都喂不饱一个团队，那么这个团队可能就太大了。按照这个逻辑，我的团队可能只能容纳两个人。。。玩笑开完了。但事实就是，对于一般人来说，能较好管理5~6个就已经是不错了。当团队人数超过这个规模，需要将团队拆分为6-10人的小团队规模，增加汇报层级，才能管得过来。每个小团队可以包含前端、后端和测试，而数据库、UI等共享资源单独一个团队。 对每个小团队需要指定一个第一责任人（以下简称“责任人”）。这个责任人需要有以下的素质： 对小团队成员知根知底 快速响应的执行力和跟进能力 对任务目标有充分的理解 2. 通知走大群，信息收集走小群2.1 通知远程办公期间的通知事项会比较多。邮件通知方式不能确保所有人都会在第一时间查看邮件。通过即时通信的群通知，可以确保绝大部分人都能第一时间看到并响应。即时通信大群的注意点：","text":"疫情期间积累了一些远程办公条件下的项目管理经验，稍微整理一下。我司从企业文化到网络硬件，不太具备远程办公的基因，要补课的地方就额外多。 1. 按小团队划分并设定第一责任人亚马逊CEO贝索斯提到过一个原则：如果两个披萨饼都喂不饱一个团队，那么这个团队可能就太大了。按照这个逻辑，我的团队可能只能容纳两个人。。。玩笑开完了。但事实就是，对于一般人来说，能较好管理5~6个就已经是不错了。当团队人数超过这个规模，需要将团队拆分为6-10人的小团队规模，增加汇报层级，才能管得过来。每个小团队可以包含前端、后端和测试，而数据库、UI等共享资源单独一个团队。 对每个小团队需要指定一个第一责任人（以下简称“责任人”）。这个责任人需要有以下的素质： 对小团队成员知根知底 快速响应的执行力和跟进能力 对任务目标有充分的理解 2. 通知走大群，信息收集走小群2.1 通知远程办公期间的通知事项会比较多。邮件通知方式不能确保所有人都会在第一时间查看邮件。通过即时通信的群通知，可以确保绝大部分人都能第一时间看到并响应。即时通信大群的注意点： 大群要求对于一般消息不要回复“收到”、“1”、“了解”等确认信息 不得灌水，仅作通知用 有疑问可以提。这样回复了一个人后，其他有同样问题的人也可以得到回复了 2.2 信息收集当需要收集例如“疫情期间所在地”、“可以使用的远程办公方式”、“工作日报”等信息，还是通过小团队来分别收集效率较高。如果只将问卷发在大群里，到了截止时间总会有那么一两个人没填。这就要靠负责人call电话等方式来催。 3. 小团队工作形式分派工作也以小团队为粒度。当责任人收到任务后，需要进行以下的日常管理工作： 给出所承担工作的评估结果及具体计划 制定具体开发人员的工作计划 小团队内每日晨会，跟进进度，整理问题 晚上下班前收取工作日报 将小团队内部无法解决的问题向上反应到项目经理，并及时跟进进展 4. 每日晨会网络会议的特点就是很容易发生两个人同时讲话，然后两个人注意到后又同时沉默。这会影响会议效率。所以每日晨会建议由一个人单独主持。内容包括： 整体通知事项 小团队各人当前的问题和风险 前一天收集到的问题的反馈 自由发言，收集当天新的问题 项目经理&#x2F;责任人在会后私下和提出问题的人沟通，并在第二天晨会上公布问题进展。晨会尽量固定时间，保持精简，尽量不超过15分钟。 5. 进度检视会&#x2F;专题事项会 主持人或项目经理提前1天发会议邀约 大团队的管理者和项目经理需要制定项目整体跟踪事项表。事项表列出具体事项、责任团队、各阶段时间节点、进度百分比等 每周1次或2次开大团队的进度检视会，小团队责任人参加。内容包含团队内同步各事项进度。着重解决需要团队间协同的事项 会议中安排1个人记录会议纪要，并在会后发给所有与会人员 6. 协同远程办公带来的最大挑战是协同。对协同最重要的是文档。文档的形式可以是传统Word或Excel文档，也可以是API平台上的接口文档等。文档的注意点主要有： 工作内容尽量形成文档或表格，包括且不限于进度表、需求文档、详细设计、数据库设计、测试用例等 文档放在svn等可以版本管理和便于查看的平台上 尽量文档由专人负责编辑，以防提交时冲突 如果必须由多人编辑的文档，可以考虑找在线系统方案替代。例如文档协同、API接口平台等 7. 到场在限制到场人数的情况下，优先安排以下两类人员到现场： 有管理职责 自我驱动能力差","categories":[],"tags":[{"name":"项目管理","slug":"项目管理","permalink":"https://galaxyyao.github.io/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"}]},{"title":"中台-读《说透中台》和《企业IT架构转型之道》有感","slug":"中台-读说透中台和企业IT架构转型之道有感","date":"2020-02-04T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2020/02/05/中台-读说透中台和企业IT架构转型之道有感/","permalink":"https://galaxyyao.github.io/2020/02/05/%E4%B8%AD%E5%8F%B0-%E8%AF%BB%E8%AF%B4%E9%80%8F%E4%B8%AD%E5%8F%B0%E5%92%8C%E4%BC%81%E4%B8%9AIT%E6%9E%B6%E6%9E%84%E8%BD%AC%E5%9E%8B%E4%B9%8B%E9%81%93%E6%9C%89%E6%84%9F/","excerpt":"春节期间看完了极客时间的《说透中台》的课程，顺便也读了《企业IT架构转型之道-阿里巴巴中台战略思想与架构实战》一书。这篇从实际项目的角度来想象一下，如果让我来负责公司的中台，应该怎么做。首先说下评价：这个课程符合我对ThoughtWorks的刻板印象：有点滥用理论术语，干货不多；问题题了不少，解决方案不落地。可能是都是给其他公司做的项目，有保密协议的缘故。能理解，但还是不推荐。阿里的那本中台书更加实在。 1. 中台概念整理1.1 中台的目的中台的目的就是企业能力复用。 1.2 中台的分类中台主流分为两大类：业务中台&#x2F;数据中台。业务中台产生数据，数据中台做数据的二次加工，并将结果再服务于业务中台。也有“技术中台”的概念，可以理解为一些技术中间件的整合和封装，但我倾向于不将其认定为中台。中台强调一个复用。如果根本没有系统从零开始建设，一上来就搞中台很容易会过度设计。 2. 中台的抓手中台会面对所有业务线的需求。虽然中台有企业级的属性，但不代表建设中台的时候必须梳理企业的全业务线。中台的愿景是能力复用，那么最好有具体的新业务作为抓手。","text":"春节期间看完了极客时间的《说透中台》的课程，顺便也读了《企业IT架构转型之道-阿里巴巴中台战略思想与架构实战》一书。这篇从实际项目的角度来想象一下，如果让我来负责公司的中台，应该怎么做。首先说下评价：这个课程符合我对ThoughtWorks的刻板印象：有点滥用理论术语，干货不多；问题题了不少，解决方案不落地。可能是都是给其他公司做的项目，有保密协议的缘故。能理解，但还是不推荐。阿里的那本中台书更加实在。 1. 中台概念整理1.1 中台的目的中台的目的就是企业能力复用。 1.2 中台的分类中台主流分为两大类：业务中台&#x2F;数据中台。业务中台产生数据，数据中台做数据的二次加工，并将结果再服务于业务中台。也有“技术中台”的概念，可以理解为一些技术中间件的整合和封装，但我倾向于不将其认定为中台。中台强调一个复用。如果根本没有系统从零开始建设，一上来就搞中台很容易会过度设计。 2. 中台的抓手中台会面对所有业务线的需求。虽然中台有企业级的属性，但不代表建设中台的时候必须梳理企业的全业务线。中台的愿景是能力复用，那么最好有具体的新业务作为抓手。 阿里是以聚划算作为抓手 蘑菇街的新增抓手是直播电商 美菜网是遇到了生鲜和2B的一些新玩法需求 2.1 对当前所在公司构建业务中台的构想《说透中台》中虚拟出来了一个极客地产。极客地产做中台的抓手是有新增的长租公寓的需求。长租公寓需要复用已有的地产投资和物业能力，但工程设计&#x2F;招采&#x2F;建设&#x2F;装修&#x2F;租赁等是从零开始。那么就可以针对地产投资和物业这两块来建设中台。而另外独立发展的业务线可以先跳过。极客地产这个例子其实和当前所在公司挺像（为了避免再被信息安全“训诫”，这里就不提公司名字了）。已有的业务是投资，金融和运营三个板块，而现在新增了商业地产板块。是不是可以趁这个机会也建设中台？我觉得可以有针对性发展少数几个业务中台，主要做数据中台。楼宇项目信息可能是少数比较共通的部分，但投前和投后在剩余的部分的交集有限。没有复用，还不如将功能独立建设在自己各自板块的应用里。当然现在对其他三个板块了解也有限，可能在了解更多后会推翻这个判断。 3. 中心与服务如何划分在具体动手开始搭建中台后，我们首先面临的问题是服务中心怎么划分怎么建设。淘宝的经历是首先四个服务中心： 用户中心 商品中心 交易中心 店铺中心 用户中心因为调用频繁收效大，而且复杂性和重要性较小，不出意外成为了最开始的试点。商品中心后来又拆出了评价中心。交易中心拆出了营销中心。另外还增加了一个库存中心。中心的划分的原则是：“高内聚、低耦合”。如果只有增删改查这样的简单需求，不建议单独拆出一个中心。像是积分这种，等发展到足够丰富或对其他业务中心的影响已经不可忽视再拆。每个中心也可以由多个服务组成。例如交易中心可以分为订单服务和购物车服务。服务中心是业务领域的概念，是为了业务和数据的完整性而设立的。而包含的子服务模块是根据系统架构设计的层面来考虑的。但一开始不要拆得太细。 4. 中台的其他技术考量点4.1 服务插件《京东服务技术中台探索与实践》的视频中提到了他们建设中台的时候有一个“服务插件”的概念。简单来说就是对于非常个性化的需求（例如对用户体验的预警），提供插件协议，由相应前台团队自己来开发插件。 这种中台的场景可能更适合中台把前端页面也包揽的情况。下图是插件在实际页面上的规定展示区域。 4.2 租户还是《京东服务技术中台探索与实践》的视频。为了防止某些用户在大促的时候把中台资源（包括计算资源和存储资源）给吃完，引入了租户的概念。租户的设计对于有2C秒杀大促等场景的企业可能是必要的，毕竟不能在中台层面产生雪崩。 4.3 配置化包括蘑菇街和京东服务中台的例子中，都提到了中台可以成为业务逻辑的下沉，前台应用做配置。像蘑菇街就是将促销的逻辑做成模板： 5. 中台的实施路径阿里的中台实施路径分为三个阶段： API as Service Product as Service Solution as Service 第一个阶段比较好实现，第三个阶段是到一定层次后的追求，我觉得关键是第二个阶段，就是将中台打造成一个产品。打造成产品，意味着将API进行场景化的组装。如果只是一堆API列表，无法达成快速支持前台的目的。除此以外，产品化也意味着： 产品有自己的定位，可以选择性地实现前台需求 产品要想方设法为客户（前台）体现价值，保持用户满意度才能生存下来。如果一定期限内无法获取前台用户，或前台用户不满意，则及时中止建设止损 产品要讲究易用性，提供完善的文档和手册 中台的绩效考核可以参考阿里的做法： 服务稳定40% 业务创新25%（适当允许因此带来的上线事故） 服务接入量20% 客户满意度15%","categories":[],"tags":[{"name":"中台","slug":"中台","permalink":"https://galaxyyao.github.io/tags/%E4%B8%AD%E5%8F%B0/"}]},{"title":"容器-14-国内Windows10环境安装Minikube","slug":"容器-14-国内Windows10环境安装Minikube","date":"2020-01-21T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2020/01/22/容器-14-国内Windows10环境安装Minikube/","permalink":"https://galaxyyao.github.io/2020/01/22/%E5%AE%B9%E5%99%A8-14-%E5%9B%BD%E5%86%85Windows10%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85Minikube/","excerpt":"上家公司虽然有这样那样的问题，但在能让我掌控的服务器资源自由度上，也不是随便在哪家公司就能有的。能随便申请个半打一打的4核8G的虚机来搞事情什么的。。。跳槽后就只有自己的Windows工作机了。Docker Desktop搞了半天也没法启用Kubernetes，这也是为什么之前的“Kubernetes实战”系列到7月就戛然而止的原因。只靠Docker Desktop，平时开发的时候起个数据库或redis是足够用了，但像service mesh之类的就玩不了了。趁年前有空，搭了一套Minikube，把步骤顺便记录一下。原本想合并到之前kubeadm安装的那篇里，但可能会翻起来不方便，还是单独另开一篇吧。后续“Kubernetes实战”系列都会基于minikube环境来搭建。 1. 软硬件条件现在内存也不值钱了，插个16G足够玩了。操作系统上，虽然Windows 10家庭版+VirtualBox&#x2F;VMWare也可以，但从硬件利用率角度，还是用Windows 10企业版&#x2F;专业版&#x2F;教育版+Hyper-V比较好。在控制面板-&gt;程序-&gt;启动或关闭Windows 功能 里面打开所有Hyper-V选项然后重启。重启后运行systeminfo，看到如下内容，说明操作系统层面已经ok了： 1Hyper-V 要求: 已检测到虚拟机监控程序。将不显示 Hyper-V 所需的功能。 Docker Desktop是否安装不影响，但在安装Minikube的过程中最好不要启动。在安装过程中报过一个create: precreate: no External vswitch nor Default Switch found的报错，不确定是不是相关。顺带提一句，如果装了Docker Desktop，可以在Settings-&gt;Daemon-&gt;Registry mirrors里填写：https://dockerhub.azk8s.cn、http://hub-mirror.c.163.com和https://docker.mirrors.ustc.edu.cn另外感谢这篇docker&#x2F;kubernetes国内源&#x2F;镜像源解决方式 - xinkun的博客 | Xinkun Blog的整理，我也复制一下备忘： global proxy in China format example dockerhub (docker.io) dockerhub.azk8s.cn dockerhub.azk8s.cn/&lt;repo-name&gt;/&lt;image-name&gt;:&lt;version&gt; dockerhub.azk8s.cn&#x2F;microsoft&#x2F;azure-cli:2.0.61 dockerhub.azk8s.cn&#x2F;library&#x2F;nginx:1.15 gcr.io gcr.azk8s.cn gcr.azk8s.cn/&lt;repo-name&gt;/&lt;image-name&gt;:&lt;version&gt; gcr.azk8s.cn&#x2F;google_containers&#x2F;hyperkube-amd64:v1.13.5 quay.io quay.azk8s.cn quay.azk8s.cn/&lt;repo-name&gt;/&lt;image-name&gt;:&lt;version&gt; quay.azk8s.cn&#x2F;deis&#x2F;go-dev:v1.10.0 2. 网络条件以防万一请先关闭Windows防火墙。因为你懂的那个原因，需要本地搞个SS的梯子。如果哪个步骤因为网络原因卡住了，可以切成代理再试一次。","text":"上家公司虽然有这样那样的问题，但在能让我掌控的服务器资源自由度上，也不是随便在哪家公司就能有的。能随便申请个半打一打的4核8G的虚机来搞事情什么的。。。跳槽后就只有自己的Windows工作机了。Docker Desktop搞了半天也没法启用Kubernetes，这也是为什么之前的“Kubernetes实战”系列到7月就戛然而止的原因。只靠Docker Desktop，平时开发的时候起个数据库或redis是足够用了，但像service mesh之类的就玩不了了。趁年前有空，搭了一套Minikube，把步骤顺便记录一下。原本想合并到之前kubeadm安装的那篇里，但可能会翻起来不方便，还是单独另开一篇吧。后续“Kubernetes实战”系列都会基于minikube环境来搭建。 1. 软硬件条件现在内存也不值钱了，插个16G足够玩了。操作系统上，虽然Windows 10家庭版+VirtualBox&#x2F;VMWare也可以，但从硬件利用率角度，还是用Windows 10企业版&#x2F;专业版&#x2F;教育版+Hyper-V比较好。在控制面板-&gt;程序-&gt;启动或关闭Windows 功能 里面打开所有Hyper-V选项然后重启。重启后运行systeminfo，看到如下内容，说明操作系统层面已经ok了： 1Hyper-V 要求: 已检测到虚拟机监控程序。将不显示 Hyper-V 所需的功能。 Docker Desktop是否安装不影响，但在安装Minikube的过程中最好不要启动。在安装过程中报过一个create: precreate: no External vswitch nor Default Switch found的报错，不确定是不是相关。顺带提一句，如果装了Docker Desktop，可以在Settings-&gt;Daemon-&gt;Registry mirrors里填写：https://dockerhub.azk8s.cn、http://hub-mirror.c.163.com和https://docker.mirrors.ustc.edu.cn另外感谢这篇docker&#x2F;kubernetes国内源&#x2F;镜像源解决方式 - xinkun的博客 | Xinkun Blog的整理，我也复制一下备忘： global proxy in China format example dockerhub (docker.io) dockerhub.azk8s.cn dockerhub.azk8s.cn/&lt;repo-name&gt;/&lt;image-name&gt;:&lt;version&gt; dockerhub.azk8s.cn&#x2F;microsoft&#x2F;azure-cli:2.0.61 dockerhub.azk8s.cn&#x2F;library&#x2F;nginx:1.15 gcr.io gcr.azk8s.cn gcr.azk8s.cn/&lt;repo-name&gt;/&lt;image-name&gt;:&lt;version&gt; gcr.azk8s.cn&#x2F;google_containers&#x2F;hyperkube-amd64:v1.13.5 quay.io quay.azk8s.cn quay.azk8s.cn/&lt;repo-name&gt;/&lt;image-name&gt;:&lt;version&gt; quay.azk8s.cn&#x2F;deis&#x2F;go-dev:v1.10.0 2. 网络条件以防万一请先关闭Windows防火墙。因为你懂的那个原因，需要本地搞个SS的梯子。如果哪个步骤因为网络原因卡住了，可以切成代理再试一次。 3. 安装步骤3.1 安装Minikube首先注意一下，后续的步骤都需要用管理员权限的命令行来执行。Powershell应该也行。安装步骤在官方文档里有详细描述，但似乎遗漏了需要先安装Kubernetes Cli的提示。所以最彻底的方法是通过Chocolatey来安装，它会帮忙把该装的都装好： 1choco install minikube 3.2 Minikube初始启动接下来就可以通过start命令来启动了。第一次启动可能会因为kubeadm和kubelet镜像下载失败而失败。不过不用担心，可以反复执行的，大不了用minikube delete重置重来。第一次启动主要是看个版本号。 1minikube start --vm-driver=hyperv --cpus=2 --memory=6g --image-repository=&quot;registry.cn-hangzhou.aliyuncs.com/google_containers&quot; PS1. memory参数建议在start的时候就加上。通过minikube config set memory 4096来调整，需要delete了重新start。PS2. 有些步骤里会加上一个--registry-mirror=https://registry.docker-cn.com的参数。但实际Docker的国内站已经挂了（公司都快撑不下去了，有点惨），加个image-repository的参数就足够了。这里image-repository用的是阿里的镜像。PS3. 如果怀疑是网络问题想通过代理下载，可以添加HTTP_PROXY等参数，例如： 1minikube start --vm-driver=hyperv --cpus=2 --memory=6g --image-repository=&quot;registry.cn-hangzhou.aliyuncs.com/google_containers&quot; --docker-env HTTP_PROXY=http://127.0.0.1:1080 --docker-env HTTPS_PROXY=http://127.0.0.1:1080 --docker-env NO_PROXY=localhost,127.0.0.1,10.96.0.0/12,192.168.99.0/24,192.168.39.0/24 3.3 手动下载kubeadm和kubelet第一次启动可能会非常花时间（视你的网络而定）。如果失败的话，可以根据kubeadm和kubelet下载失败报错信息，来确定k8s的版本。我安装的minikube使用的版本是v1.17.0。然后可以手动从googleapis网站下载（Windows机器默认没有curl，也可以通过Chocolatey安装，参考这篇： 12curl -Lo kubeadm http://storage.googleapis.com/kubernetes-release/release/v1.17.0/bin/linux/amd64/kubeadmcurl -Lo kubelet http://storage.googleapis.com/kubernetes-release/release/v1.17.0/bin/linux/amd64/kubelet 下载后扔到C:\\Users\\用户名\\.minikube\\cache\\版本号的目录里。如果其他K8S的组件下载失败，也可以使用类似的方式下载后扔到C:\\Users\\用户名\\.minikube\\cache\\images\\registry.cn-hangzhou.aliyuncs.com\\google_containers的目录里。例如kube-proxy： 1curl -Lo kube-proxy_v1.17.0 http://storage.googleapis.com/kubernetes-release/release/v1.17.0/bin/linux/amd64/kube-proxy 但这招对版本号不一样的例如etcd没用。这个我是靠重试多次硬抗的。对于国内网络来说，这个步骤很花时间，不要急慢慢等。 3.4 更新minikube kubectl如果同时装了Docker Desktop和minkube，会提示Docker下的kubectl.exe的版本不够高。 12* 完成！kubectl 已经配置至 &quot;minikube&quot;! C:\\Program Files\\Docker\\Docker\\Resources\\bin\\kubectl.exe is version 1.14.0, and is incompatible with Kubernetes 1.17.0. You will need to update C:\\Program Files\\Docker\\Docker\\Resources\\bin\\kubectl.exe or use &#x27;minikube kubectl&#x27; to connect with this cluster 接下来先用minikube stop停止，然后用minikube kubectl options的命令，会提示下载最新版本的kubectl： 12$ minikube kubectl options* 正在下载 kubectl.exe v1.17.0 最新版的kubectl会下载到C:\\Users\\用户\\.minikube\\cache\\v版本的目录里，然后复制到Docker的bin目录（例如C:\\Program Files\\Docker\\Docker\\Resources\\bin\\）里覆盖即可。 4. 验证安装好后就可以验证了。先看看是不是所有系统容器都启动了： 12345678910111213$ kubectl get pod -ANAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-7f9c544f75-j6cfj 1/1 Running 1 6h10mkube-system coredns-7f9c544f75-wpcn7 1/1 Running 1 6h10mkube-system etcd-minikube 1/1 Running 1 6h10mkube-system kube-addon-manager-minikube 1/1 Running 1 6h10mkube-system kube-apiserver-minikube 1/1 Running 1 6h10mkube-system kube-controller-manager-minikube 1/1 Running 2 6h10mkube-system kube-proxy-kbnsr 1/1 Running 1 6h10mkube-system kube-scheduler-minikube 1/1 Running 1 6h10mkube-system storage-provisioner 1/1 Running 1 6h10mkubernetes-dashboard dashboard-metrics-scraper-7b64584c5c-6k8f8 1/1 Running 1 6h4mkubernetes-dashboard kubernetes-dashboard-79d9cd965-565j6 1/1 Running 2 6h4m 然后可以看看dashboard。本机就是好不用token： 1minikube dashboard 然后也可以照官方的Example里的，创建个echo server来验证。需要注意image里的k8s.gcr.io需要替换为registry.cn-hangzhou.aliyuncs.com/google_containers： 123kubectl create deployment hello-minikube --image=registry.cn-hangzhou.aliyuncs.com/google_containers/echoserver:1.4kubectl expose deployment hello-minikube --type=NodePort --port=8080minikube service hello-minikube 最后一句minikube service &lt;service名&gt;的作用是获取本地集群中指定服务的kubernetes URL。对于以NodePort对外暴露的服务，会自动打开浏览器并跳转到对应的ip+端口。注意对应的ip是Hyper-V管理器中的“vEthernet (Default Switch)”。 5. 启用ingressminikube启用ingress还是挺简单的，可以参考Installation Guide - NGINX Ingress Controller，一句命令启动： 1minikube addons enable ingress 但有可能会遇到Back-off pulling image的报错信息。看到image的名字里包含&lt;quay.io&gt;就可以知道又是被墙拦住了。对于类似的问题，可以采用以下的步骤，从Azure拉取后打tag。对于ingress，命令如下： 123minikube sshdocker pull quay.azk8s.cn/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1docker tag quay.azk8s.cn/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1 部署完成后可以通过kubectl get pod -n kube-system的命令来确认ingress是否有安装成功。如果失败的话，最坏结果有可能需要minikube delete后重新start。 6. 常见问题6.1 minikube delete失败遇到过一次minikube delete失败，报的是C:\\Users\\用户名.minikube\\machines下某个conf文件找不到。尝试手动删除该目录会报文件被锁定，无法删除。重启后文件夹依然被锁定。这个时候打开services.msc，停止“Hyper-V 虚拟机管理”服务后，就可以删除了。删除完把这个服务重新启用即可。 6.2 Unable to connect to the server在kubectl apply命令的时候有些时候会出现Unable to connect to the server的报错。这种情况下kubectl get node的命令也会失败。电脑重启后问题解决。原因暂不明。。。可能是minikube的bug。 7. 参考资料常用操作可以参考这个官方文档：使用 Minikube 安装 Kubernetes - Kubernetes","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://galaxyyao.github.io/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://galaxyyao.github.io/tags/k8s/"}]},{"title":"Java-从FeignClient的Ambiguous mapping报错,重温RequestMapping原理","slug":"Java-从FeignClient的Ambiguous-mapping报错-重温RequestMapping原理","date":"2019-12-22T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/12/23/Java-从FeignClient的Ambiguous-mapping报错-重温RequestMapping原理/","permalink":"https://galaxyyao.github.io/2019/12/23/Java-%E4%BB%8EFeignClient%E7%9A%84Ambiguous-mapping%E6%8A%A5%E9%94%99-%E9%87%8D%E6%B8%A9RequestMapping%E5%8E%9F%E7%90%86/","excerpt":"1. 微服务的公共API模块微服务之间调用进程会出现DTO实体类的重复定义。比如服务A的接口返回User实体，服务B接收的时候，也需要定义一个同样的User实体。在引入了Feign后，就有了一个避免项目间重复定义实体类的简单方案：我们可以在服务A开发的时候专门抽出来一个API模块。 这个API模块可以包含接口方法定义，URI以及和对外实体类定义（DTO），可以认为是A和B之间互通的约定。一个最简单的API模块代码如下： 123456789101112@Data@AllArgsConstructor@NoArgsConstructorpublic class DemoDto implements Serializable &#123; private String text;&#125;@RequestMapping(&quot;/demo&quot;)public interface DemoApiService &#123; @GetMapping(&quot;/hello&quot;) DemoDto hello();&#125; 服务A的Controller负责对接口定义进行实现： 1234567@RestControllerpublic class DemoProducerController implements DemoApiService &#123; @Override public DemoDto hello() &#123; return new DemoDto(&quot;hello&quot;); &#125;&#125; 服务A项目将API模块发布到Maven私服上。服务B项目只需要对API模块添加依赖：","text":"1. 微服务的公共API模块微服务之间调用进程会出现DTO实体类的重复定义。比如服务A的接口返回User实体，服务B接收的时候，也需要定义一个同样的User实体。在引入了Feign后，就有了一个避免项目间重复定义实体类的简单方案：我们可以在服务A开发的时候专门抽出来一个API模块。 这个API模块可以包含接口方法定义，URI以及和对外实体类定义（DTO），可以认为是A和B之间互通的约定。一个最简单的API模块代码如下： 123456789101112@Data@AllArgsConstructor@NoArgsConstructorpublic class DemoDto implements Serializable &#123; private String text;&#125;@RequestMapping(&quot;/demo&quot;)public interface DemoApiService &#123; @GetMapping(&quot;/hello&quot;) DemoDto hello();&#125; 服务A的Controller负责对接口定义进行实现： 1234567@RestControllerpublic class DemoProducerController implements DemoApiService &#123; @Override public DemoDto hello() &#123; return new DemoDto(&quot;hello&quot;); &#125;&#125; 服务A项目将API模块发布到Maven私服上。服务B项目只需要对API模块添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.galaxy.demo&lt;/groupId&gt; &lt;artifactId&gt;feign-demo-api&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt; 并且扩展一下该接口并添加@FeignClient注解： 123@FeignClient(name = &quot;demo&quot;, contextId = &quot;demoSpiService&quot;, url = &quot;http://localhost:8080/&quot;)public interface DemoSpiService extends DemoApiService &#123;&#125; 就可以很轻松地像调用本地方法一样调用A应用的接口了。 1234567@Resourceprivate DemoSpiService demoSpiService;@GetMapping(&quot;/hello&quot;)public String hello() &#123; return demoSpiService.hello().toString();&#125; 2. Ambiguous mapping报错如果你像我上面描述的那样实现，就会在消费者服务B启动的时候遇到如下的报错信息： 123Caused by: java.lang.IllegalStateException: Ambiguous mapping. Cannot map &#x27;com.galaxy.demo.feign.consumer.spi.DemoSpiService&#x27; method com.galaxy.demo.feign.consumer.spi.DemoSpiService#hello()to &#123;GET /demo/hello&#125;: There is already &#x27;demoConsumerController&#x27; bean method 报错信息很直白：同一个URI被重复映射了两次。一次是在DemoConsumerController，一次是在DemoSpiService。But Why? DemoSpiService里只是一个FeignClient，不是RestController啊？ 3. @RestController，@Controller，@RequestMapping原理重温我们通过这个问题，正好来重温一下@RestController，@Controller和@RequestMapping几个Spring中的经典概念。 3.1 @RestController我们先来看一下@RestController的原代码： 1234567891011@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Controller@ResponseBodypublic @interface RestController &#123; @AliasFor( annotation = Controller.class ) String value() default &quot;&quot;;&#125; 可以看到@RestController&#x3D;@Controller+@ResponseBody 上图是一个Spring MVC从接收请求到返回响应的完整流程。我理解对于SpringBoot的RestController来说，在第四步没有返回ModelAndView，而是直接返回了Json，并通过@ResponseBody将Json直接写到了响应Body，略过了第5步和第6步。 3.2 @Controller和@RequestMapping如果只从@Controller的源代码来看，@Controller只是@Component的一个别名。 12345678910@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface Controller &#123; @AliasFor( annotation = Component.class ) String value() default &quot;&quot;;&#125; 但注解怎么用不是看定义的。从Spring的AbstractHandlerMethodMapping.java的源代码，我们可以看到Spring会根据一个名为isHandler方法的判断结果，对Handler处理器里的方法进行扫描，获得URL映射。 1234567891011121314 if (beanType != null &amp;&amp; isHandler(beanType)) &#123; detectHandlerMethods(beanName); &#125;// 省略部分 protected void detectHandlerMethods(Object handler) &#123; Class&lt;?&gt; handlerType = (handler instanceof String ? obtainApplicationContext().getType((String) handler) : handler.getClass()); if (handlerType != null) &#123; Class&lt;?&gt; userType = ClassUtils.getUserClass(handlerType); Map&lt;Method, T&gt; methods = MethodIntrospector.selectMethods(userType, (MethodIntrospector.MetadataLookup&lt;T&gt;) method -&gt; &#123; try &#123; return getMappingForMethod(method, userType); 而isHandler的逻辑很简单，就是看Bean上是否有@Controller注解或@RequestMapping注解。参见源代码： 12345@Overrideprotected boolean isHandler(Class&lt;?&gt; beanType) &#123; return (AnnotatedElementUtils.hasAnnotation(beanType, Controller.class) || AnnotatedElementUtils.hasAnnotation(beanType, RequestMapping.class));&#125; 4. Ambiguous mapping报错原因总结和解决方案归根到底，Ambiguous mapping报错原因在于上面的那个逻辑中使用的是“或”（||），而不是“和”（&amp;&amp;）。由于我们的DemoSpiService扩展了DemoApiService，而DemoApiService的接口定义上有@RequestMapping注解，于是DemoSpiService也被Spring MVC扫描Handler了。而偏生对于DemoSpiService和DemoConsumerController的URL路径都是“&#x2F;demo”，于是就产生了冲突。知道了原因后，解决方案也就很简单了：修改一下DemoConsumerController的@RequestMapping的URL，例如改为@RequestMapping(&quot;/consumer/demo&quot;)，就可以成功启动了。 你可能会担心@FeignClient+API模块是否会暴露不该暴露的接口？直接访问的话会返回404： 1234567&#123; &quot;timestamp&quot;: &quot;2019-12-23T12:51:05.376+0000&quot;, &quot;status&quot;: 404, &quot;error&quot;: &quot;Not Found&quot;, &quot;message&quot;: &quot;No message available&quot;, &quot;path&quot;: &quot;/demo/hello&quot;&#125; 也很容易理解：请求在找RequestMapping对应的View：”&#x2F;demo&#x2F;hello”。但View不存在，就只能返回404了。只有在DemoSpiService上主动添加@ResponseBody注解，才能对外暴露。 5. 参考资料这篇是主要参考资料。作者认为这是Spring MVC的锅。我理解指的是“或”的那个逻辑。但我觉得当初Spring这么写肯定是有原因的。。。虽然我没找到相关文章。FeignClient 出现 Ambiguous mapping 重复映射 | Japari Park 另外是两篇Spring原理解析参考SpringMVC在@RequestMapping配置两个相同路径 - Text_Dexter - 博客园 Spring MVC — @RequestMapping原理讲解-1 - 小小默：进无止境","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://galaxyyao.github.io/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://galaxyyao.github.io/tags/Spring/"}]},{"title":"Java-Feign+服务注册的多环境方案","slug":"Java-Feign-服务注册的多环境方案","date":"2019-12-08T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/12/09/Java-Feign-服务注册的多环境方案/","permalink":"https://galaxyyao.github.io/2019/12/09/Java-Feign-%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E7%9A%84%E5%A4%9A%E7%8E%AF%E5%A2%83%E6%96%B9%E6%A1%88/","excerpt":"微服务的开发模式下，联调和服务注册一旦涉及多个环境（开发&#x2F;SIT&#x2F;UAT），就会变得有些复杂。本文总结一下我们在此问题上尝试过的几个workaround，以及最终推荐的方案。 1. 背景以下描述的案例中，将我们所拥有的服务精简为三个： um：用户微服务 ent：企业微服务 bi：BI微服务ent会调用um；bi会调用ent和um。网络环境分成办公网段和开发环境网段。办公网段可以访问开发环境网段，但开发环境网段无法访问办公网段。三个微服务都被打包成镜像，以单副本Pod的形式部署在K8S云的开发环境节点上。服务注册使用Nacos，网关路由使用的是Zuul。 2. 单环境内部请求流程如果只考虑SIT环境，整个服务注册+请求的处理流程可以简单描述如下： um-sit服务（um的sit环境，下同）启动，将自己的service ip注册到Nacos服务端 ent-sit服务启动，将自己的service ip注册到Nacos服务端 前端web对http:&#x2F;&#x2F;域名&#x2F;api&#x2F;ent-sit 的某个接口发起请求 通过K8S Ingress的域名映射，找到了Zuul应用 Zuul向Nacos查询ent-sit的地址，得到ip：172.0.0.2。这个是ent-sit的service内部ip Zuul将请求转给ent-sit的service，Pod里的ent-sit容器中的应用接收到请求，开始处理 ent-sit容器在处理过程中需要解析token，于是向Zuul请求um-sit Zuul向Nacos查询um-sit的地址，得到ip：172.0.0.1。这个是um-sit的service内部ip Zuul将请求转给um-sit。um处理完token，返回用户信息 ent-sit处理结束，将结果返回给Zuul Zuul将结果转给前端web，流程结束","text":"微服务的开发模式下，联调和服务注册一旦涉及多个环境（开发&#x2F;SIT&#x2F;UAT），就会变得有些复杂。本文总结一下我们在此问题上尝试过的几个workaround，以及最终推荐的方案。 1. 背景以下描述的案例中，将我们所拥有的服务精简为三个： um：用户微服务 ent：企业微服务 bi：BI微服务ent会调用um；bi会调用ent和um。网络环境分成办公网段和开发环境网段。办公网段可以访问开发环境网段，但开发环境网段无法访问办公网段。三个微服务都被打包成镜像，以单副本Pod的形式部署在K8S云的开发环境节点上。服务注册使用Nacos，网关路由使用的是Zuul。 2. 单环境内部请求流程如果只考虑SIT环境，整个服务注册+请求的处理流程可以简单描述如下： um-sit服务（um的sit环境，下同）启动，将自己的service ip注册到Nacos服务端 ent-sit服务启动，将自己的service ip注册到Nacos服务端 前端web对http:&#x2F;&#x2F;域名&#x2F;api&#x2F;ent-sit 的某个接口发起请求 通过K8S Ingress的域名映射，找到了Zuul应用 Zuul向Nacos查询ent-sit的地址，得到ip：172.0.0.2。这个是ent-sit的service内部ip Zuul将请求转给ent-sit的service，Pod里的ent-sit容器中的应用接收到请求，开始处理 ent-sit容器在处理过程中需要解析token，于是向Zuul请求um-sit Zuul向Nacos查询um-sit的地址，得到ip：172.0.0.1。这个是um-sit的service内部ip Zuul将请求转给um-sit。um处理完token，返回用户信息 ent-sit处理结束，将结果返回给Zuul Zuul将结果转给前端web，流程结束 3. 遇到的问题联调和测试过程中我们遇到了两个主要问题： 联调会串服务 无法和测试环境的服务联通 3.1 串服务假设Alice和Bob都在开发ent，服务名都是ent-dev。于是Nacos记录了两个服务注册信息。Cathy想和Alice联调。但如果Cathy配置调用的服务id也是ent-dev，请求就有一定几率会飘到Bob那里。那么很可能会发生Cathy的请求返回的结果不稳定，时对时错。 一种解决方案就是每个人在本地将自己的spring.application.name改为“服务名-姓名”，例如：ent-alice。 但这个方案也存在问题：很容易在提交代码的时候误提交了自己的个人配置。Git里这个配置文件修改频繁，一看log就是服务名从Alice改为Bob，然后又被改会Alice。如果只是这个问题，还有办法可以搞定，例如将服务名放到环境变量等。下一个问题是真正具有阻碍性的。 无法和测试环境的服务联通在办公网段开发过程中，会发现无法调通SIT的微服务。究其原因，这个是由于办公网段无法访问到service内部ip导致的。 办公网段ent-dev尝试调用um-sit服务的流程如下： um-sit服务启动，将自己的service ip（172.0.0.1）注册到Nacos服务端 本地的ent-dev启动，将自己的ip（10.0.0.1）注册到Nacos服务端 ent-dev通过name（um-sit）发起请求，向Nacos服务端查询um-sit的地址 Nacos服务端返回172.0.0.1 ent-dev尝试请求172.0.0.1，但由于这个地址是K8S的内部地址，外部无法访问，所以请求失败 一个解决方案就是每个人自己本地也起一个um的服务，假设服务名为um-alice。然后将请求的服务名也改为同样的服务名。这样当发起请求时，Nacos会返回本机的地址，自然请求就可以成功了。但这个解决方案除了和上个解决方案有同样的问题（容易误提交自己的个人配置）之外，也会导致每个人开发过程中都需要启动一堆依赖的微服务。姑且不说开发机的性能压力，也容易因为没有及时更新依赖服务的代码，导致联调出错。 4. 解决方案4.1 解决方案一：在容器之外再部署一套微服务既然service的内部ip地址无法被办公网段访问，那么另外以非容器方式在ECS上另外部署一套dev环境，就可以解决网络访问的问题。但这个解决方案不完美： 没有解决个人配置的问题 在基于容器的持续集成方案之外，多维护了一套持续集成方案 需要多部署一套环境，消耗硬件资源 4.2 【推荐】解决方案二：不使用name方式访问，使用域名&#x2F;ip方式需要同时避免串服务和个人配置这两个看起来互相冲突的问题，看起来只有放弃通过服务name方式调用，改为url调用。通过Feign可以简化调用的代码。只要在@FeignClient的参数里配置了url，就会优先使用url。范例如下： 123456789@FeignClient(name = &quot;jsonPlaceHolderClient&quot;, url = &quot;$&#123;feign-client.json-place-holder.url&#125;&quot; , contextId = &quot;JsonPlaceHolderClient&quot;)public interface JsonPlaceHolderClient &#123; @GetMapping(value = &quot;/posts&quot;) List&lt;Post&gt; getPosts(); @GetMapping(value = &quot;/posts/&#123;postId&#125;&quot;) Post getPostById(@PathVariable(&quot;postId&quot;) Long postId);&#125; 在application-dev.yml配置文件中，feign-client.json-place-holder.url可以默认填写为sit测试环境的地址。这样如果只是作为基础服务来调用（例如用户服务），就不需要在本地启动了。同样以办公网段ent-dev调用um-sit服务的流程作为范例： um-sit服务启动，将自己的service ip（172.0.0.1）注册到Nacos服务端 本地开发机的ent-dev启动 ent-dev通过域名http:&#x2F;&#x2F;域名&#x2F;api&#x2F;um-sit，对um的某个接口发起请求 Zuul收到请求，向Nacos查询um-sit的地址，得到ip：172.0.0.1。这个是um-sit的service内部ip Zuul将请求转给um-sit的service，Pod里的um-sit容器中的应用接收到请求，开始处理 um-sit容器中的应用处理请求完毕，返回结果给Zuul Zuul将结果转给本地开发机和K8S的Service不同，Ingress是可以被容器外访问到的，所以网络连通性上也没有任何问题。 如果是需要本机服务联调或与其他开发进行联调，只需要将url改为localhost或其他开发的ip即可。这样就等同于不涉及服务注册的直连。提交的时候把这个临时改动revert回来，就不会将个人配置提交到代码仓库了。 5. Feign配置中的name和url在最新版的Spring Cloud OpenFeign中，@FeignClient的name属性是必需的。参见Spring Cloud OpenFeign： 1Previously, using the url attribute, did not require the name attribute. Using name is now required. 上文提到了如果同时配置了name和url，会优先使用url，而不是通过name访问服务。原理我们可以通过源代码来说明。这段是FeignClientFactoryBean的源代码片段： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * @param &lt;T&gt; the target type of the Feign client * @return a &#123;@link Feign&#125; client created with the specified data and the context * information */ &lt;T&gt; T getTarget() &#123; //FeignContext在FeignAutoConfiguration中自动注册，FeignContext用于客户端配置类独立注册 FeignContext context = this.applicationContext.getBean(FeignContext.class); //创建Feign.Builder Feign.Builder builder = feign(context); //如果@FeignClient注解没有设置url参数 if (!StringUtils.hasText(this.url)) &#123; //url为@FeignClient注解的name参数 if (!this.name.startsWith(&quot;http&quot;)) &#123; this.url = &quot;http://&quot; + this.name; &#125; else &#123; this.url = this.name; &#125; //加上path this.url += cleanPath(); //返回loadBalance客户端，也就是ribbon+eureka/Nacos的客户端 return (T) loadBalance(builder, context, new HardCodedTarget&lt;&gt;(this.type, this.name, this.url)); &#125; //@FeignClient设置了url参数，不走服务注册的负载均衡 if (StringUtils.hasText(this.url) &amp;&amp; !this.url.startsWith(&quot;http&quot;)) &#123; this.url = &quot;http://&quot; + this.url; &#125; //加上path String url = this.url + cleanPath(); //从FeignContext中获取client Client client = getOptional(context, Client.class); if (client != null) &#123; if (client instanceof LoadBalancerFeignClient) &#123; // not load balancing because we have a url, // but ribbon is on the classpath, so unwrap client = ((LoadBalancerFeignClient) client).getDelegate(); &#125; builder.client(client); &#125; //从FeignContext中获取Targeter Targeter targeter = get(context, Targeter.class); //生成客户端代理 return (T) targeter.target(this, builder, context, new HardCodedTarget&lt;&gt;(this.type, this.name, url)); &#125; 从代码可以看到，只有没有设置url的情况下，才会通过loadBalance方法生成Ribbon的动态代理。更多关于Spring Cloud OpenFeign的源代码分析，可以参见本文最后的参考资料。 6. 一些想法实际调用过程中会发现第一次通过域名调用会较慢（2-3秒），但第二次就很快了。这是由于Zuul会通过SpringMVC对请求进行缓存。但其实Zuul的路由功能Ingress本身已经实现得很好了。多引入一个Zuul会增加运维架构的复杂度，也会带来潜在的性能瓶颈。不过这个目前不在我们的控制范围。。。Zuul除了路由之外也可以做一些通用的token校验等，也并不是完全冗余，只是我们目前没有这么使用。 7. 参考资料spring-cloud-openfeign原理分析 | 拍拍贷基础框架团队博客","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://galaxyyao.github.io/tags/Java/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://galaxyyao.github.io/tags/Spring-Cloud/"},{"name":"Feign","slug":"Feign","permalink":"https://galaxyyao.github.io/tags/Feign/"}]},{"title":"数据库-转到PostgreSQL的新手Tips","slug":"数据库-转到PostgreSQL的新手","date":"2019-09-21T16:00:00.000Z","updated":"2025-06-11T15:54:07.157Z","comments":true,"path":"2019/09/22/数据库-转到PostgreSQL的新手/","permalink":"https://galaxyyao.github.io/2019/09/22/%E6%95%B0%E6%8D%AE%E5%BA%93-%E8%BD%AC%E5%88%B0PostgreSQL%E7%9A%84%E6%96%B0%E6%89%8B/","excerpt":"从Oracle或MySQL切换到PostgreSQL（以下简称pgsql）后，多少有些不一样的地方需要适应。这里就将和开发相关的一些区别挂一漏万地列举一下。 1. Schema模式和Oracle与MySQL一样，pgsql中也有TableSpace（表空间），用于定义用来存放表示数据库对象的文件的位置。但在Schema（模式）的定义上，三者有很大的差别。对于MySQL，模式与数据库同义。甚至可以用CREATE SCHEMA来创建数据库，效果和CREATE DATABASE一样。对于Oracle，schema与数据库用户密切相关： A schema is a collection of logical structures of data, or schema objects. A schema is owned by a database user and has the same name as that user. Each user owns a single schema. 而pgsql中，层次结果如下： 从图中可以看到，schema是database与table中间的一层。可以理解为命名空间类似的概念。当新创建一个数据库时，pgsql会默认创建一个public schema。如果没有指定的话，就是以public schema来操作各种数据对象。 例如：CREATE TABLE products ( ... ) 等同于 CREATE TABLE public.products ( ... ) schema不能互相嵌套。同一个schema下不能有重复的对象名字，但在不同schema下可以重复。schema与database的差别在于schema不是严格分离的：一个用户可以访问他所连接的数据库中的任意模式中的对象。 对于数据库管理人员来说，还需要了解一下授权相关的差别，但在本文中就略过了。更多可以参考这篇：PostgreSQL · 特性分析 · 逻辑结构和权限体系 1.1 Schema与开发相关","text":"从Oracle或MySQL切换到PostgreSQL（以下简称pgsql）后，多少有些不一样的地方需要适应。这里就将和开发相关的一些区别挂一漏万地列举一下。 1. Schema模式和Oracle与MySQL一样，pgsql中也有TableSpace（表空间），用于定义用来存放表示数据库对象的文件的位置。但在Schema（模式）的定义上，三者有很大的差别。对于MySQL，模式与数据库同义。甚至可以用CREATE SCHEMA来创建数据库，效果和CREATE DATABASE一样。对于Oracle，schema与数据库用户密切相关： A schema is a collection of logical structures of data, or schema objects. A schema is owned by a database user and has the same name as that user. Each user owns a single schema. 而pgsql中，层次结果如下： 从图中可以看到，schema是database与table中间的一层。可以理解为命名空间类似的概念。当新创建一个数据库时，pgsql会默认创建一个public schema。如果没有指定的话，就是以public schema来操作各种数据对象。 例如：CREATE TABLE products ( ... ) 等同于 CREATE TABLE public.products ( ... ) schema不能互相嵌套。同一个schema下不能有重复的对象名字，但在不同schema下可以重复。schema与database的差别在于schema不是严格分离的：一个用户可以访问他所连接的数据库中的任意模式中的对象。 对于数据库管理人员来说，还需要了解一下授权相关的差别，但在本文中就略过了。更多可以参考这篇：PostgreSQL · 特性分析 · 逻辑结构和权限体系 1.1 Schema与开发相关连接url字符串中除了需要指定数据库之外，还需要加一个currentSchema。例如下面的范例中，database是pabem，schema是pabem_um_dev： 1jdbc:postgresql://localhost:5432/pabem?currentSchema=pabem_um_dev schema可能对需要跨数据源的应用开发带来一些简便。如果需要跨的两个数据源只是同一个数据库的两个schema，就可以去掉连接url中的currentSchema，就可以当单数据源应用来开发了。 参考资料表空间 默认schema是public，切换执行sql所在schema的语法是： 1set search_path to &lt;schema_name&gt; 2. 自增字段和MySQL中使用的auto increment不同，PostgreSQL和Oracle类似，都是用sequence（序列）。sequence的好处在于可以让多张表共享同一个自增序列，但创建起来的确也挺麻烦。所以pgsql还新增了一个语法糖serial。和数值类型一样，也分为smallserial, serial和bigserial： Name Storage Size Range smallserial 2 bytes 1 to 32767 serial 4 bytes 1 to 2147483647 bigserial 8 bytes 1 to 9223372036854775807 2.1 Serial与开发相关对于自增的id字段，需要在Entity的属性上加上注解： 1@GeneratedValue(strategy = GenerationType.IDENTITY) 3. 数据类型映射 PostgreSQL数据类型 Oracle数据类型 MySQL数据类型 备注 char char char Oracle中char(n)的n表示byte数，而pgsql和mysql中表示字符数。对于中文字无需除以2或除以3 varchar varchar2 varchar 同char text clob text bytea blob blob smallint number(4) smallint,tinyint pgsql中没有tinyint，所以我们的布尔型字段用smallint类型 int number(9) int int是integer的缩写 bigint number(18) bigint decimal decimal decimal decimal与numeric等价，都是SQL标准。我们就统一用decimal date （无） date Oracle没有纯日期类型，date会返回日期和时间 timestamp timestamp timestamp pgsql还有timestampz表示带时区的时间戳 参考资料PostgreSQL: Documentation: 11: 8.1. Numeric Types 4. 常用函数与语法差异4.1 DUALpgsql中的select可以省略from，所以不再需要强制加一个from dual。 4.2 日期和时间 当前时间：now() 日期转字符串：select to_char(current_date,&#39;YYYY-MM-dd&#39;); 时间转字符串：select to_char(now(),&#39;YYYY-MM-dd HH24:MI:SS&#39;); 4.3 字符串 拼接：select &#39;a&#39;||&#39;b&#39; as col1; 获取指定字符串的下标：select position(&#39;om&#39; in &#39;Thomas&#39;); 4.4 序列获取序列下一个值的语法为：nextval(&#39;sequence_name&#39;) 4.5 行数1234ROW_NUMBER() OVER( [PARTITION BY column_1, column_2,…] [ORDER BY column_3,column_4,…]) 例如： 1234567891011SELECT product_id, product_name, group_id, ROW_NUMBER () OVER ( PARTITION BY group_id ORDER BY product_name )FROM products; 4.6 NVL（判断为空赋值）1SELECT coalesce(null,0) as col1; 4.7 分页PostgreSQL中的分页语法和MySQL类似： 12345SELECT *FROM tableLIMIT n OFFSET m; 4.8 CRUD语法差异网上也看到有人整理了一下CRUD语法的差异：简单总结一下，就是支持插入&#x2F;更新&#x2F;删除并返回，以及插入冲突则更新或什么不做。前者从通用性考虑不推荐，后者MyBatis Plus也封装了一个，不一定需要使用数据库的实现。表关联多字段更新倒可能比较常用，在Oracle中也有同样的语法： 123update table1 set (col1, col2) = (select col1, col2 from table2 where table2.col3 = table1.col3) 参考资料MySQL和PostgreSQL的常用语法差异-云栖社区-阿里云 5. Rule规则系统这个是pgsql中的一个特性。或者更准确地说，是查询重写规则系统，即把根据既定规则修改后的查询再提交给查询规划器。实际上PostgreSQL中的视图就是通过规则系统来实现的。例如如下的查询： 1CREATE VIEW myview AS SELECT * FROM mytab; 内部的规则： 123CREATE TABLE myview (same column list as mytab);CREATE RULE &quot;_RETURN&quot; AS ON SELECT TO myview DO INSTEAD SELECT * FROM mytab; pgsql中同样也有触发器。你可能会发现规则系统和触发器的作用有点相像。其实他们的作用域有重叠的部分，也有另一方无法替换的场景。只有触发器能做的场景：约束触发器。触发器能抛出异常，而规则系统只能静默地选择处理或不处理。而只有规则系统能做更新视图。另外触发器会对被影响的每一行触发一次，而规则系统是一次性的重写。所以在某些场景下规则系统的性能会高于触发器。 参考资料Chapter 41. 规则系统PostgreSQL: Documentation: 11: 41.7. Rules Versus TriggersPostgreSQL的规则系统 | P.Linux Laboratory 6. Java开发配置使用JPA作为数据源的时候，启动的时候会告警： 123Caused by: java.sql.SQLFeatureNotSupportedException: 这个 org.postgresql.jdbc.PgConnection.createClob() 方法尚未被实作。 at org.postgresql.Driver.notImplemented(Driver.java:692) ~[postgresql-42.2.8.jar:42.2.8] at org.postgresql.jdbc.PgConnection.createClob(PgConnection.java:1268) ~[postgresql-42.2.8.jar:42.2.8] 这是由于Hibernate尝试验证PostgreSQL的CLOB特性，但是PostgreSQL的JDBC驱动并没有实现这个特性，所以抛出了异常。可以增加配置，关闭这个特性的检测： 123456spring: jpa: properties: hibernate: temp: use_jdbc_metadata_defaults: false 参考资料：SpringBoot连接PostgreSQL - ldp.im - 博客园 如果只使用MyBatis就不需要加这个配置了。 7. 排序规则与大小写敏感大小写敏感分为两个不同的方面：数据库对象名的大小写敏感，以及字段内容的大小写敏感。 7.1 数据库对象名的大小写敏感PostgreSQL在创建数据库对象（表&#x2F;字段等）时，会默认将对象名改为小写。例如会将如下的SQL 1SELECT FullName FROM Person 转换为 1SELECT fullname FROM person 如果一定要使用大小写敏感的对象名，则需要在创建和查询的时候都带上双引号。例如： 1CREATE TABLE &quot;Person&quot; (&quot;FullName&quot; VARCHAR(100), &quot;Address&quot; VARCHAR(100)) 但非常不推荐这种方式。 7.2 字段内容的大小写敏感PostgreSQL查询的时候是大小写敏感的。而且在建库时不能像MySQL那样，通过collation参数来指定数据库是否大小写敏感。如果需要进行大小写不敏感的查询和模糊查询，可以使用如下两种方法之一： 等号=或LIKE的两边的表达式加上LOWER()或UPPER() 使用ILIKE（应该是Insensitive Like的缩写吧） 例如： 1select * from person where lower(user_name) like lower(&#x27;%alice%&#x27;) 或 1select * from person where user_name ilike &#x27;%alice%&#x27; LIKE和ILIKE也可以换成~~和~~*。但为了SQL的可读性和统一，还是避免使用这样的语法吧。对于text类型的字段，可以在PostgreSQL安装citext模块后，改为citext类型。这样就可以大小写不敏感了。 7.3 字段内容的大小写敏感带来的问题字段内容大小写敏感可能会带来三个问题： 排序 性能 索引 先来看排序。 因为大小写敏感，所以英文是按照ASCII排序。’a’开头的内容会被排在’B’之后。所以如果需要忽略大小写来排序，则排序字段也需要加lower： 1select * from person order by lower(user_name) 性能方面，有人做过测试，使用lower+like会比ilike快17%左右。再考虑到数据库迁移过程中的兼容性，还是推荐使用lower+like。通过UNIQUE或者PRIMARY KEY隐式产生的索引是大小写敏感的。如果使用lower的话，就不会走索引。如果对这方面有性能要求的话，可以给PostgreSQL安装上pg_trgm模块。 8. 其他MySQL与PostgreSQL比较 PostgreSQL中天然支持emoji，不需要像MySQL中一样专门设置utf8mb4编码 PostgreSQL和Oracle一样有物化视图 支持CTE语法 支持intersect语法 PostgreSQL中没有单独的存储过程，是通过Function实现的 9. 其他Oracle与PostgreSQL比较 NULL与空字符串在Oracle里是同一含义，但在pgsql中是不同的 同义词synonym在pg中使用search_path来实现，例如：SET search_path TO myschema; 10. PostgreSQL独有特性json&#x2F;jsonb这两个是PostgreSQL专有的数据类型。从用户操作的角度来说没有区别，区别主要是存储和读取的系统处理（预处理）和耗时方面有区别。json写入快，读取慢，jsonb写入慢，读取快。有文章说jsonb的性能已经优于MongoDB的BSON。但至少有一个好处是如果需要处理json数据，在有PostgreSQL的情况下可以少引入一个数据库。 GISPostGIS基本成为了空间地理信息数据的存储标准。 11. 其他如果想在本机Docker Desktop上启动pgsql，用官方的postgres:latest好像会有些问题，需要改为用alpine镜像。命令可参考： 1docker run --name posttest -d -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:alpine","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://galaxyyao.github.io/tags/Java/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://galaxyyao.github.io/tags/PostgreSQL/"},{"name":"pgsql","slug":"pgsql","permalink":"https://galaxyyao.github.io/tags/pgsql/"}]},{"title":"Maven-组织内部项目统一配置DistributionManagement","slug":"Maven-组织内部项目统一配置DistributionManagement","date":"2019-09-17T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/09/18/Maven-组织内部项目统一配置DistributionManagement/","permalink":"https://galaxyyao.github.io/2019/09/18/Maven-%E7%BB%84%E7%BB%87%E5%86%85%E9%83%A8%E9%A1%B9%E7%9B%AE%E7%BB%9F%E4%B8%80%E9%85%8D%E7%BD%AEDistributionManagement/","excerpt":"搜了一下中文技术博客上似乎没有相关的文章，就简要翻译一下。 需求假设公司内部有非常多Maven项目，需要deploy到一个内部maven私有仓库中。如果希望maven deploy命令可以成功执行，一般需要在pom.xml中添加： 123456&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-site&lt;/id&gt; &lt;url&gt;http://central_nexus/server&lt;/url&gt; &lt;/repository&gt;&lt;/distributionManagement&gt; 但需要deploy的项目很多的情况下，我们肯定不希望在每个项目的pom文件中都重复添加这个配置。 方案一为所有项目增加一个公共的parent pom项目。那么只需要在这个项目的pom文件中添加： 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;your.company&lt;/groupId&gt; &lt;artifactId&gt;company-parent&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-site&lt;/id&gt; &lt;url&gt;http://central_nexus/server&lt;/url&gt; &lt;/repository&gt; &lt;/distributionManagement&gt;&lt;/project&gt;","text":"搜了一下中文技术博客上似乎没有相关的文章，就简要翻译一下。 需求假设公司内部有非常多Maven项目，需要deploy到一个内部maven私有仓库中。如果希望maven deploy命令可以成功执行，一般需要在pom.xml中添加： 123456&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-site&lt;/id&gt; &lt;url&gt;http://central_nexus/server&lt;/url&gt; &lt;/repository&gt;&lt;/distributionManagement&gt; 但需要deploy的项目很多的情况下，我们肯定不希望在每个项目的pom文件中都重复添加这个配置。 方案一为所有项目增加一个公共的parent pom项目。那么只需要在这个项目的pom文件中添加： 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;your.company&lt;/groupId&gt; &lt;artifactId&gt;company-parent&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-site&lt;/id&gt; &lt;url&gt;http://central_nexus/server&lt;/url&gt; &lt;/repository&gt; &lt;/distributionManagement&gt;&lt;/project&gt; 然后使其他项目的parent项目变成这个项目： 12345&lt;parent&gt; &lt;groupId&gt;your.company&lt;/groupId&gt; &lt;artifactId&gt;company-parent&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt;&lt;/parent&gt; 方案二方案一存在两个问题： 如果代码泄露或将代码开源，会使该内部私有仓库的地址被暴露 私有仓库这种环境配置信息最好和代码分离。类似通过配置中心，将数据库地址等配置和代码分离。 我们完全可以将这个配置放到maven中。可以通过mvn命令的启动参数来实现： 12-DaltSnapshotDeploymentRepository=snapshots::default::https://YOUR_NEXUS_URL/snapshots-DaltReleaseDeploymentRepository=releases::default::https://YOUR_NEXUS_URL/releases 更好的方法是将其配在settings.xml中： 1234567891011121314151617&lt;settings&gt;[...] &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;properties&gt; &lt;altSnapshotDeploymentRepository&gt;snapshots::default::https://YOUR_NEXUS_URL/snapshots&lt;/altSnapshotDeploymentRepository&gt; &lt;altReleaseDeploymentRepository&gt;releases::default::https://YOUR_NEXUS_URL/releases&lt;/altReleaseDeploymentRepository&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;nexus&lt;/activeProfile&gt; &lt;/activeProfiles&gt;&lt;/settings&gt; 不要忘记也在&lt;server&gt;和&lt;/server&gt;之间加上snapshots和releases的账号。 参考资料java - How to specify maven’s distributionManagement organisation wide? - Stack Overflow","categories":[],"tags":[{"name":"Maven","slug":"Maven","permalink":"https://galaxyyao.github.io/tags/Maven/"}]},{"title":"持续集成-Jenkins离线安装与配置","slug":"持续集成-Jenkins离线安装与配置","date":"2019-08-13T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/08/14/持续集成-Jenkins离线安装与配置/","permalink":"https://galaxyyao.github.io/2019/08/14/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90-Jenkins%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/","excerpt":"本篇主要针对的是Jenkins服务器处在局域网中，无法连上互联网的情况下如何做自动化部署。本文对网络的架设是：有内部的git服务器，yum私有仓库，nexus Repository OSS私有仓库和npm私有仓库。如果这些条件都没有，可能你们还是本地开发机上打包稍微快一些。 1. 安装1.1 安装必要依赖Jenkins的必要依赖是JDK。后续自动化部署的必要依赖是git，Maven和Node.js。具体的安装步骤就不详述了。rpm安装（针对JDK）或私有YUM仓库安装都可以。例如私有YUM仓库中openjdk，直接运行yum install java-1.8.0-openjdk.x86_64 即可。 Maven私有仓库配置Maven打包的时候默认会从公网的仓库拉取依赖的第三方库。我们需要将其改为指向私有仓库。首先可以通过如下两条命令之一获得配置文件地址： 12mvn --versionmvn -e -X 假设settings.xml文件的位置在/etc/maven/路径下。编辑该文件内容： 1vi /etc/maven/settings.xml","text":"本篇主要针对的是Jenkins服务器处在局域网中，无法连上互联网的情况下如何做自动化部署。本文对网络的架设是：有内部的git服务器，yum私有仓库，nexus Repository OSS私有仓库和npm私有仓库。如果这些条件都没有，可能你们还是本地开发机上打包稍微快一些。 1. 安装1.1 安装必要依赖Jenkins的必要依赖是JDK。后续自动化部署的必要依赖是git，Maven和Node.js。具体的安装步骤就不详述了。rpm安装（针对JDK）或私有YUM仓库安装都可以。例如私有YUM仓库中openjdk，直接运行yum install java-1.8.0-openjdk.x86_64 即可。 Maven私有仓库配置Maven打包的时候默认会从公网的仓库拉取依赖的第三方库。我们需要将其改为指向私有仓库。首先可以通过如下两条命令之一获得配置文件地址： 12mvn --versionmvn -e -X 假设settings.xml文件的位置在/etc/maven/路径下。编辑该文件内容： 1vi /etc/maven/settings.xml 我们首先需要在&lt;profiles&gt;和&lt;/profiles&gt;之间添加私有仓库地址（包括仓库和插件仓库）： 12345678910111213141516171819202122&lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;local-nexus&lt;/id&gt; &lt;url&gt;http://10.16.34.197:8081/repository/maven-central/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Internal Mirror of Central Plugins Repository&lt;/name&gt; &lt;url&gt;http://10.16.34.197:8081/repository/maven-central/&lt;/url&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;&lt;/profile&gt; 然后在最后的&lt;/settings&gt;上添加当前活跃的profile： 123&lt;activeProfiles&gt; &lt;activeProfile&gt;dev&lt;/activeProfile&gt;&lt;/activeProfiles&gt; npm私有仓库配置npm配置私有仓库就简单多了，一条命令搞定： 1npm set registry http://10.16.34.197:8081/repository/npm-central 1.2 安装JenkinsJenkins离线rpm安装包下载地址：https://jenkins.io/zh/download/然后rpm安装jenkins（根据你下载的具体rpm包名更新命令）： 1rpm -ivh jenkins-2.176.2-1.1.noarch.rpm 然后启动jenkins： 1systemctl start jenkins 确保防火墙已关闭或开放端口8080，然后就可以访问http://ip:8080/来访问jenkins首页了。万一没有成功启动，可以通过systemctl status jenkins来确认失败的原因。 2. 配置2.1 启动配置在启动Jenkins后，首先需要解锁。在服务器上执行： 1cat /var/lib/jenkins/secrets/initialAdminPassword 然后输入到Unlock Jenkins的Administrator password框。由于我们是离线模式安装，Jenkins会提醒“This Jenkins instance appears to be offline”。我们先点击“Skip Plugin Installations”跳过插件安装。然后创建管理员用户，点击下一步。如果有域名的话，在当前步骤的“Jenkins URL”中填写域名。我们这里直接点击“Save and Finish”。Welcome to Jenkins:) 2.2 插件配置在左侧菜单点击Manage Jenkins，然后点击Manage Plugins-&gt;Advanced。我们可以在这边上传所需插件的hpi文件。hpi文件可以从https://plugins.jenkins.io下载。需要注意有一些包具有依赖。我们需要的几个插件： Publish Over SSH build-pipeline-plugin cron_column git nodejs 插件安装先后顺序如下： structs credentials ssh-credentials publish-over publish-over-ssh jquery scm-api workflow-step-api workflow-api junit javadoc display-url-api mailer apache-httpcomponents-client-4-api maven-plugin matrix-project token-macro run-condition conditional-buildstep parameterized-trigger build-pipeline-plugin cron_column git-client workflow-scm-step git config-file-provider nodejs 当中根据提示可能需要重启数次Jenkins，也可以等所有插件都安装完再重启Jenkins。 2.3 配置工具在左侧菜单点击Manage Jenkins，然后点击Global Tool Configuration，配置JDK、Maven和NodeJS（Git默认已配置）。注意配置的时候取消“Install Automatically”。 配置JDK执行 1alternatives --config java 如果是OpenJDK，Command那列括号里/jre/bin/java之前的内容就是JAVA_HOME 配置gitPath to Git executable可以填git或usr/bin/git都可以。 配置Maven执行如下命令 1mvn --version 返回的结果里取Maven home即可。 配置node可以通过npm config list命令查看当前的npm配置。node bin location后的就是node的位置。 至此Jenkins的安装和基本配置就完成了。 3. 参考资料这篇整理完才发现Nexus Repository Manager真强大，除了maven之外，docker&#x2F;npm&#x2F;pypi&#x2F;yum都可以proxy。虽然也有单独npm代理的方案，比如：使用verdaccio搭建npm私有仓库 - Better’s study fairyland","categories":[],"tags":[{"name":"持续集成","slug":"持续集成","permalink":"https://galaxyyao.github.io/tags/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"name":"CI","slug":"CI","permalink":"https://galaxyyao.github.io/tags/CI/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://galaxyyao.github.io/tags/Jenkins/"}]},{"title":"MySQL-没有必要的varchar(255)长度及存储汉字问题汇总","slug":"MySQL-没有必要的varchar-255-长度及存储汉字问题汇总","date":"2019-07-29T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/07/30/MySQL-没有必要的varchar-255-长度及存储汉字问题汇总/","permalink":"https://galaxyyao.github.io/2019/07/30/MySQL-%E6%B2%A1%E6%9C%89%E5%BF%85%E8%A6%81%E7%9A%84varchar-255-%E9%95%BF%E5%BA%A6%E5%8F%8A%E5%AD%98%E5%82%A8%E6%B1%89%E5%AD%97%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/","excerpt":"起因最近在整理代码规范，按照之前oracle的习惯，定了以下的字段长度设定规范： 名称字段：varchar(200) 较长的名称字段&#x2F;简介字段：varchar(500) 特别长的描述字段： varchar(2000) 超过2000中文字的字段：text为什么是200长度，而不是100或300，也是拍脑袋想的，类似DND里的房规。但在被问起为什么不设置为经常见到的varchar(255)时，一时回答不上来。趁这个机会，把字段长度这块的知识汇总梳理一下。 为什么会经常被设置为varchar(255)MySQL 4.1版本之前，varchar的最大长度是255 byte字节（也有一说是5.0.3版本之前）。查了下这个版本发布都是2004年的事情了。惯性真恐怖，我可不相信还有多少系统是从2004年升级过来的。 varchar(50)和varchar(255)有性能上的差别么？对于INNODB，varchar(50)和varchar(255)这两者在存放方式上完全一样：1-2 byte保存长度，实际的字符串存放在另外的位置，每个字符1 byte到4 byte不定（视编码和实际存储的字符而定）。所以将一个字段从varchar(50)长度改成varchar(100)长度不会导致表的重建。但如果把长度从varchar(50)改成varchar(256)就不一样了，表示长度会需要用到2 byte或更多。 既然255长度以下对INNODB都一样，而且我们平时基本上也不太会使用到MYISAM，那么是不是为了省心，我们就可以把255长度以下的字段的类型都设置成varchar(255)了呢？非也。因为内存表介意。虽然我们不会明文创建内存表，但所有的中间结果都会被数据库引擎存放在内存表。我们可以通过EXPLAIN或者SHOW STATUS可以查看MYSQL是否使用了内存表用来帮助完成某个操作。而内存表会按照固定长度来保存。以utf-8编码为例，对于varchar(255)，每一行所占用的内存就是长度的2 byte + 3 * 255 byte。对于100条数据，光一个varchar字段就占约1GB内存。如果我们该用varchar(50)，就可以剩下来约80%的内存空间。除此之外，255长度也可能会对索引造成坑。MySQL在5.6版本及之前的最大长度是767 byte。但MySQL 5.5版本后开始支持4个byte的字符集utf8mb4（沙雕表情用到的字符太多，长度不够用）。255 * 4 &gt; 767，所以索引就放不下varchar(255)长度的字段了。虽然MySQL在5.7版本后将限制改成了3072 byte，但如果是多字段的联合索引还是有可能会超过这个限制。","text":"起因最近在整理代码规范，按照之前oracle的习惯，定了以下的字段长度设定规范： 名称字段：varchar(200) 较长的名称字段&#x2F;简介字段：varchar(500) 特别长的描述字段： varchar(2000) 超过2000中文字的字段：text为什么是200长度，而不是100或300，也是拍脑袋想的，类似DND里的房规。但在被问起为什么不设置为经常见到的varchar(255)时，一时回答不上来。趁这个机会，把字段长度这块的知识汇总梳理一下。 为什么会经常被设置为varchar(255)MySQL 4.1版本之前，varchar的最大长度是255 byte字节（也有一说是5.0.3版本之前）。查了下这个版本发布都是2004年的事情了。惯性真恐怖，我可不相信还有多少系统是从2004年升级过来的。 varchar(50)和varchar(255)有性能上的差别么？对于INNODB，varchar(50)和varchar(255)这两者在存放方式上完全一样：1-2 byte保存长度，实际的字符串存放在另外的位置，每个字符1 byte到4 byte不定（视编码和实际存储的字符而定）。所以将一个字段从varchar(50)长度改成varchar(100)长度不会导致表的重建。但如果把长度从varchar(50)改成varchar(256)就不一样了，表示长度会需要用到2 byte或更多。 既然255长度以下对INNODB都一样，而且我们平时基本上也不太会使用到MYISAM，那么是不是为了省心，我们就可以把255长度以下的字段的类型都设置成varchar(255)了呢？非也。因为内存表介意。虽然我们不会明文创建内存表，但所有的中间结果都会被数据库引擎存放在内存表。我们可以通过EXPLAIN或者SHOW STATUS可以查看MYSQL是否使用了内存表用来帮助完成某个操作。而内存表会按照固定长度来保存。以utf-8编码为例，对于varchar(255)，每一行所占用的内存就是长度的2 byte + 3 * 255 byte。对于100条数据，光一个varchar字段就占约1GB内存。如果我们该用varchar(50)，就可以剩下来约80%的内存空间。除此之外，255长度也可能会对索引造成坑。MySQL在5.6版本及之前的最大长度是767 byte。但MySQL 5.5版本后开始支持4个byte的字符集utf8mb4（沙雕表情用到的字符太多，长度不够用）。255 * 4 &gt; 767，所以索引就放不下varchar(255)长度的字段了。虽然MySQL在5.7版本后将限制改成了3072 byte，但如果是多字段的联合索引还是有可能会超过这个限制。 所以我们的结论就是：在长度够用的情况下，越短越好。 varchar的最大长度是多少varchar的最大长度是65535 byte。所以 字符类型若为gbk，每个字符最多占2个字节，最大长度不能超过32766字符 字符类型若为utf8，每个字符最多占3个字节，最大长度不能超过21845字符 字符类型若为utf8mb，每个字符最多占4个字节，最大长度不能超过16383字符但通常导致varchar长度限制的通常是一行定义的长度,就是表里所有字段定义的长度总和。这个限制也是65535 byte。如果超出长度，会报错： 1ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. You have to change some columns to TEXT or BLOBs。 这也是为什么阿里开发规范中这么要求： 1【强制】varchar是可变长字符串，不预先分配存储空间，长度不要超过5000，如果存储长度大于此值，定义字段类型为text，独立出来一张表，用主键来对应，避免影响其它字段索引效率。 varchar(50)是能保存16个汉字，还是25个，抑或50个？以前SQL Server的nvarchar转Oracle的varchar2时造成的固有印象，让我一直觉得varchar保存中文字时长度需要打对折或除以3。但这个也是MySQL 5.0版本之前的事。现在varchar(n)是几，就能存几个中文字。不过也需要注意统计字数使用CHARACTER_LENGTH而非LENGTH 1234-- 返回为12SELECT LENGTH(&quot;轻松工作&quot;);-- 返回为4SELECT CHARACTER_LENGTH(&quot;轻松工作&quot;); 为什么还是用MySQL为什么MySQL坑那么多，不改用PostgreSQL？相比MySQL，我个人更偏好PostgreSQL，能从各种设计细节就感觉得到很规范。但无奈国内分布式数据库方案基本都是基于MySQL的。。。虽然我们的场景在今年年内暂时也看不到用分布式的必要性，但万一有了呢。。。先发优势真是可怕。 参考资料感谢这篇stackexchange的详细回答database design - MySQL - varchar length and performance - Database Administrators Stack Exchange 关于内存表的详细介绍MySQL · 特性分析 · 内部临时表 为什么索引长度会有767 byte或3072 byte的限制的详细解释关于InnoDB索引长度限制的tips - 追风刀·丁奇 - ITeye博客","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://galaxyyao.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://galaxyyao.github.io/tags/MySQL/"}]},{"title":"前端-通过自定义协议URI Scheme,点击Chrome中的链接打开IE","slug":"前端-通过自定义协议URI-Scheme-点击Chrome中的链接打开IE","date":"2019-07-10T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/07/11/前端-通过自定义协议URI-Scheme-点击Chrome中的链接打开IE/","permalink":"https://galaxyyao.github.io/2019/07/11/%E5%89%8D%E7%AB%AF-%E9%80%9A%E8%BF%87%E8%87%AA%E5%AE%9A%E4%B9%89%E5%8D%8F%E8%AE%AEURI-Scheme-%E7%82%B9%E5%87%BBChrome%E4%B8%AD%E7%9A%84%E9%93%BE%E6%8E%A5%E6%89%93%E5%BC%80IE/","excerpt":"有部分老Web系统只有在IE下才能正常打开。其中有一部分是即使polyfill也没法搞定的兼容性原因，另一部分就是因为使用到了ActiveX。后者中我接触到的就有金格控件和泛微OA的。对于新开发的Portal系统，没有余力为了迁就IE，对每个功能还额外做兼容性测试。于是剩下的方案就是在单点登录跳转到相应的页面的时候，指定使用IE打开。其实这个功能并不罕见。比如腾讯的网站上经常有点击图标打开QQ，而淘宝网页上也有很多点击打开阿里旺旺。从原理上，这是利用到了Windows自定义协议URI Scheme。 URI Scheme自定义协议从本质上就是修改注册表。官方资料可以参考这篇Registering an Application to a URI Scheme (Windows) | Microsoft Docs官方给了一个范例，注册一个alert:&#x2F;&#x2F;的协议，点击后打开自定义的alert.exe。 12345678910HKEY_CLASSES_ROOT alert (Default) = &quot;URL:Alert Protocol&quot; URL Protocol = &quot;&quot; DefaultIcon (Default) = &quot;alert.exe,1&quot; shell open command (Default) = &quot;C:\\Program Files\\Alert\\alert.exe&quot; &quot;%1&quot; 方案1能看出这是一种比较通用的方案。能打开自定义的alert.exe，自然也能打开IE。所以只要将以下内容保存为test.reg，点击运行后就能将注册表项导入： 123456789101112131415Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\\openIE] @=&quot;URL:OpenIE Protocol&quot; &quot;URL Protocol&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\\openIE\\DefaultIcon] @=&quot;iexplore.exe,1&quot; [HKEY_CLASSES_ROOT\\openIE\\shell] [HKEY_CLASSES_ROOT\\openIE\\shell\\open] [HKEY_CLASSES_ROOT\\openIE\\shell\\open\\command] @=&quot;cmd /c set m=%1 &amp; call set m=%%m:openIE:=%% &amp; call \\&quot;C:\\\\Program Files\\\\Internet Explorer\\\\iexplore.exe\\&quot; %%m%% &amp; exit&quot; 与微软官方范例的差别在于将协议改为了openIE:&#x2F;&#x2F;（这个不重要），以及最后的命令改为了一串很长的：","text":"有部分老Web系统只有在IE下才能正常打开。其中有一部分是即使polyfill也没法搞定的兼容性原因，另一部分就是因为使用到了ActiveX。后者中我接触到的就有金格控件和泛微OA的。对于新开发的Portal系统，没有余力为了迁就IE，对每个功能还额外做兼容性测试。于是剩下的方案就是在单点登录跳转到相应的页面的时候，指定使用IE打开。其实这个功能并不罕见。比如腾讯的网站上经常有点击图标打开QQ，而淘宝网页上也有很多点击打开阿里旺旺。从原理上，这是利用到了Windows自定义协议URI Scheme。 URI Scheme自定义协议从本质上就是修改注册表。官方资料可以参考这篇Registering an Application to a URI Scheme (Windows) | Microsoft Docs官方给了一个范例，注册一个alert:&#x2F;&#x2F;的协议，点击后打开自定义的alert.exe。 12345678910HKEY_CLASSES_ROOT alert (Default) = &quot;URL:Alert Protocol&quot; URL Protocol = &quot;&quot; DefaultIcon (Default) = &quot;alert.exe,1&quot; shell open command (Default) = &quot;C:\\Program Files\\Alert\\alert.exe&quot; &quot;%1&quot; 方案1能看出这是一种比较通用的方案。能打开自定义的alert.exe，自然也能打开IE。所以只要将以下内容保存为test.reg，点击运行后就能将注册表项导入： 123456789101112131415Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\\openIE] @=&quot;URL:OpenIE Protocol&quot; &quot;URL Protocol&quot;=&quot;&quot; [HKEY_CLASSES_ROOT\\openIE\\DefaultIcon] @=&quot;iexplore.exe,1&quot; [HKEY_CLASSES_ROOT\\openIE\\shell] [HKEY_CLASSES_ROOT\\openIE\\shell\\open] [HKEY_CLASSES_ROOT\\openIE\\shell\\open\\command] @=&quot;cmd /c set m=%1 &amp; call set m=%%m:openIE:=%% &amp; call \\&quot;C:\\\\Program Files\\\\Internet Explorer\\\\iexplore.exe\\&quot; %%m%% &amp; exit&quot; 与微软官方范例的差别在于将协议改为了openIE:&#x2F;&#x2F;（这个不重要），以及最后的命令改为了一串很长的： 1cmd /c set m=%1 &amp; call set m=%%m:openIE:=%% &amp; call \\&quot;C:\\\\Program Files\\\\Internet Explorer\\\\iexplore.exe\\&quot; %%m%% &amp; exit /c参数表示执行完命令后关闭窗口。所以运行的时候你会看到先弹出一个命令行窗口。该命令接收了openIE:&#x2F;&#x2F;后的参数，然后传递给IE浏览器。 对应的超链接HTML如下： 1&lt;a href=&quot;openIE:www.baidu.com&quot;&gt;网络联通性测试&lt;/a&gt; 方案1的问题除了会弹出一个命令行窗口的问题之外，还有一个比较严重的问题：无法正确处理特殊字符&amp;。比如url是https://www.baidu.com?key1=value&amp;key2=value，那么第二个参数就会丢失。有些文章提出用^&amp;替代&amp;可以转义，实测无效。而大部分的单点登录URL后都是跟着一串参数。所以该方案无效。 方案2方案2做了两个改动。一个是将改动从HKEY_CLASSES_ROOT改到了HKEY_CURRENT_USER\\Software\\Classes下。我理解影响范围会小一些。另一个改动是在cmd参数里将也用引号包起来，并用反斜杠转义。修改后的注册表导入内容如下： 123456789101112Windows Registry Editor Version 5.00[HKEY_CURRENT_USER\\Software\\Classes\\ie]&quot;URL Protocol&quot;=&quot;&quot;@=&quot;URL:IE Protocol&quot;[HKEY_CURRENT_USER\\Software\\Classes\\ie\\shell][HKEY_CURRENT_USER\\Software\\Classes\\ie\\shell\\open][HKEY_CURRENT_USER\\Software\\Classes\\ie\\shell\\open\\command]@=&quot;cmd /c set url=\\&quot;%1\\&quot; &amp; call set url=%%url:ie:=%% &amp; call start iexplore -nosessionmerging -noframemerging %%url%%&quot; 对应的HTML超链接如下： 1&lt;a href=&quot;ie:https://www.baidu.com?key1=value&amp;key2=value2&quot;&gt;网络联通性测试&lt;/a&gt; 事实证明方案2可以正确处理URL参数。 最后提一下，标题中的Chrome只是指代我们常用的浏览器。并不表示Firefox或那一坨国产浏览器（恕不列举）就不能打开了。 参考资料方案2的来源windows - Registry - How to register Internet Explorer as a URI scheme and call from chrome? - Super User 方案1的来源使用自定义协议实现Chrome打开IE_木子网","categories":[],"tags":[{"name":"IE","slug":"IE","permalink":"https://galaxyyao.github.io/tags/IE/"},{"name":"Windows","slug":"Windows","permalink":"https://galaxyyao.github.io/tags/Windows/"}]},{"title":"容器-13-Kubernetes实战-静态网站部署优化2-InitContainer","slug":"容器-13-Kubernetes实战-静态网站部署优化2-InitContainer","date":"2019-07-03T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/07/04/容器-13-Kubernetes实战-静态网站部署优化2-InitContainer/","permalink":"https://galaxyyao.github.io/2019/07/04/%E5%AE%B9%E5%99%A8-13-Kubernetes%E5%AE%9E%E6%88%98-%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%962-InitContainer/","excerpt":"我们在上一篇已经将Dockerfile精简为了： 12FROM nginx:alpineCOPY ./dist /usr/share/nginx/html 但相信你也发现了，内容中还是对web服务器有着强依赖。当我们想换成其他版本的Nginx镜像，或换成其他Web服务器，就必须修改源代码中的Dockerfile，重新制作镜像。这种情况并非不可能。典型的场景之一：我们镜像所依赖的Nginx或tomcat版本出现了某个安全事故，而该问题可以通过将web服务器或web容器版本升级到最新版本解决。所以我们希望能在Dockerfile中将Nginx的痕迹彻底抹除，只在Kubernetes的YAML中指定web服务器。 一个很自然的想法就是：我们使用一个默认的web服务器镜像。在使用该镜像的容器启动之前，将静态网站的文件拷贝到相应目录，就像在前一篇从ConfigMap获取配置文件一样。这就是initContainer的作用。 1. initContainer我们首先将镜像改为alpine，并修改COPY的路径： 12FROM alpine:latestCOPY ./dist /html 重新编译为latest版本（这是为了强制每次重新拉取镜像）：","text":"我们在上一篇已经将Dockerfile精简为了： 12FROM nginx:alpineCOPY ./dist /usr/share/nginx/html 但相信你也发现了，内容中还是对web服务器有着强依赖。当我们想换成其他版本的Nginx镜像，或换成其他Web服务器，就必须修改源代码中的Dockerfile，重新制作镜像。这种情况并非不可能。典型的场景之一：我们镜像所依赖的Nginx或tomcat版本出现了某个安全事故，而该问题可以通过将web服务器或web容器版本升级到最新版本解决。所以我们希望能在Dockerfile中将Nginx的痕迹彻底抹除，只在Kubernetes的YAML中指定web服务器。 一个很自然的想法就是：我们使用一个默认的web服务器镜像。在使用该镜像的容器启动之前，将静态网站的文件拷贝到相应目录，就像在前一篇从ConfigMap获取配置文件一样。这就是initContainer的作用。 1. initContainer我们首先将镜像改为alpine，并修改COPY的路径： 12FROM alpine:latestCOPY ./dist /html 重新编译为latest版本（这是为了强制每次重新拉取镜像）： 1docker build -t 10.16.34.197:5000/staticsite . 然后我们将deployment的YAML修改为如下： 1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: apps/v1kind: Deploymentmetadata: name: poc-web labels: app: poc-webspec: replicas: 2 selector: matchLabels: app: poc-web template: metadata: labels: app: poc-web spec: initContainers: - image: 10.16.34.197:5000/staticsite:latest name: poc-web-dist command: [&quot;cp&quot;, &quot;/html&quot;, &quot;/website&quot;] args: [&quot;-r&quot;] volumeMounts: - mountPath: &quot;/website&quot; name: poc-web-volume containers: - name: nginx image: nginx:latest ports: - containerPort: 80 name: web volumeMounts: - name: poc-web-config mountPath: /etc/nginx/conf.d - name: poc-web-volume mountPath: &quot;/usr/share/nginx&quot; volumes: - name: poc-web-config configMap: name: poc-web-config - name: poc-web-volume emptyDir: &#123;&#125; 解释一下几个改动点。 1.1 挂载emptyDir首先我们除了ConfigMap之外，增加挂载了一个类型为emptyDir的卷。emptyDir是一个生命周期和Pod相同的空目录，作用是为多容器Pod内的容器提供一个公共盘来共享文件。当Pod从Node上被移除后，emptyDir也会随之被永久删除。缺省情况下，emptyDir使用主机磁盘进行存储的。也可以设置emptyDir.medium字段的值为Memory，来提高IO速度： 1234volumes:...- name: poc-web-volume emptyDir: &#123;&#125; 1.2 增加initContainer和复制命令然后我们在containers的平级增加一个initContainers。initContainer内定义的容器会比spec.containers内定义的容器先启动。启动的流程图可以参见下图： 在poc-web-dist这个容器启动后，会将emptyDir挂载到&#x2F;website路径，并执行以下命令： 1cp -r /html /website 即将包含所有的静态网站文件的html目录复制到emptyDir中。PS. 我尝试过将command和args参数改为： 12command: [&quot;cp&quot;, &quot;/html/*&quot;, &quot;/website&quot;]args: [&quot;-r&quot;] 这会导致Pod启动报错。明明cp -r /html/* /website这个命令是可以正常执行的。。。目前还没找到原因。 1.3 将镜像改为nginx并挂载emptyDir在执行完命令后initContainer完成使命退出。然后spec.container内的容器开始启动。我们将自定义镜像改为普通的nginx镜像，并在镜像的/usr/share/nginx路径上挂载emptyDir。emptyDir中的html目录会替换nginx镜像的/usr/share/nginx/html目录，达成和之前相同的效果。 在添加了initContainer后，启动速度略微变慢，会经历一个为时十几秒的PodInitializing状态，然后正常启动： 1poc-web-657d957f68-6m7xw 0/1 PodInitializing 0 12s 万事开头难。虽然我们目前只完成了一个静态网站的部署，但应该已经对Kubernetes有了基本的认识。 2. 参考资料官方的Demo，通过wget下载网页后也是加载到&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html目录Configure Pod Initialization - Kubernetes","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://galaxyyao.github.io/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://galaxyyao.github.io/tags/k8s/"}]},{"title":"容器-12-Kubernetes实战-静态网站部署优化1:ConfigMap,Secret与TLS","slug":"容器-12-Kubernetes实战-静态网站部署优化1-ConfigMap-Secret与TLS","date":"2019-07-02T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/07/03/容器-12-Kubernetes实战-静态网站部署优化1-ConfigMap-Secret与TLS/","permalink":"https://galaxyyao.github.io/2019/07/03/%E5%AE%B9%E5%99%A8-12-Kubernetes%E5%AE%9E%E6%88%98-%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%961-ConfigMap-Secret%E4%B8%8ETLS/","excerpt":"虽然我们已经成功地将一个静态网站成功地在Kubernetes里部署起来了，但还有很多细节可以完善。我们就在这一节里逐步优化。 1. ConfigMap问题最明显的是。重温一下我们静态网站之前使用的Dockerfile： 123FROM nginx:alpineCOPY default.conf /etc/nginx/conf.d/default.confCOPY ./dist /usr/share/nginx/html 首先是Nginx配置default.conf。网站的源代码不应该干涉网站怎么部署。到底部署在Apache，Nginx还是Node.js，是否要在部署的时候添加自定义Header，都不该是开发者关注的事情。我们也不希望修改网站的timeout配置还需要动到源代码。从耦合性的角度来看，这个Nginx网站的配置文件不应该放到源代码中。对于这类配置文件，Kubernetes里有专门的对象ConfigMap来保存。 从ConfigMap这个名字就可以猜得到，它存储的是配置信息，存储的格式是Map类型，即键值对。配置信息可以是像本篇中的Nginx config配置，可以设置环境变量，可以是Java的properties和application.yml配置文件，可以是Redis和MySQL的配置文件。它很适合需要在一套Kubernetes集群上部署多个环境（例如特性分支&#x2F;sit&#x2F;uat）的情况。（当然我们的Java应用将使用Spring Cloud Config配置中心，所以目前不会用ConfigMap管理配置）本篇POC的ConfigMap如下： 1234567891011121314151617181920212223242526272829apiVersion: v1kind: ConfigMapmetadata: name: poc-web-config labels: app: poc-webdata: default.conf: | server &#123; listen 80; server_name localhost; charset utf-8; #access_log /var/log/nginx/log/host.access.log main; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; &#125; default.conf为key（键），下面的内容为value（值）。结构非常简单。在Kubernetes中，ConfigMap是一种特殊的Volume（卷）：Projected Volume。可以认为ConfigMap是Kubernetes中的数据被投射（Project）到容器中的。关于Volume我们会在后续展开讨论，这里只是先提一下：要在容器中使用volume，需要先在spec中定义，然后mount到容器中。所以添加了ConfigMap后的Deployment定义YAML如下：","text":"虽然我们已经成功地将一个静态网站成功地在Kubernetes里部署起来了，但还有很多细节可以完善。我们就在这一节里逐步优化。 1. ConfigMap问题最明显的是。重温一下我们静态网站之前使用的Dockerfile： 123FROM nginx:alpineCOPY default.conf /etc/nginx/conf.d/default.confCOPY ./dist /usr/share/nginx/html 首先是Nginx配置default.conf。网站的源代码不应该干涉网站怎么部署。到底部署在Apache，Nginx还是Node.js，是否要在部署的时候添加自定义Header，都不该是开发者关注的事情。我们也不希望修改网站的timeout配置还需要动到源代码。从耦合性的角度来看，这个Nginx网站的配置文件不应该放到源代码中。对于这类配置文件，Kubernetes里有专门的对象ConfigMap来保存。 从ConfigMap这个名字就可以猜得到，它存储的是配置信息，存储的格式是Map类型，即键值对。配置信息可以是像本篇中的Nginx config配置，可以设置环境变量，可以是Java的properties和application.yml配置文件，可以是Redis和MySQL的配置文件。它很适合需要在一套Kubernetes集群上部署多个环境（例如特性分支&#x2F;sit&#x2F;uat）的情况。（当然我们的Java应用将使用Spring Cloud Config配置中心，所以目前不会用ConfigMap管理配置）本篇POC的ConfigMap如下： 1234567891011121314151617181920212223242526272829apiVersion: v1kind: ConfigMapmetadata: name: poc-web-config labels: app: poc-webdata: default.conf: | server &#123; listen 80; server_name localhost; charset utf-8; #access_log /var/log/nginx/log/host.access.log main; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; &#125; default.conf为key（键），下面的内容为value（值）。结构非常简单。在Kubernetes中，ConfigMap是一种特殊的Volume（卷）：Projected Volume。可以认为ConfigMap是Kubernetes中的数据被投射（Project）到容器中的。关于Volume我们会在后续展开讨论，这里只是先提一下：要在容器中使用volume，需要先在spec中定义，然后mount到容器中。所以添加了ConfigMap后的Deployment定义YAML如下： 1234567891011121314151617181920212223242526272829apiVersion: apps/v1kind: Deploymentmetadata: name: poc-web labels: app: poc-webspec: replicas: 2 selector: matchLabels: app: poc-web template: metadata: labels: app: poc-web spec: containers: - name: poc-web image: 10.16.34.197:5000/staticsite:latest ports: - containerPort: 80 name: web volumeMounts: - name: poc-web-config mountPath: /etc/nginx/conf.d volumes: - name: poc-web-config configMap: name: poc-web-config poc-web-config中的default.conf被挂到了Nginx镜像的&#x2F;etc&#x2F;nginx&#x2F;conf.d目录下。Nginx的nginx.conf中定义了会加载conf.d下所有conf后缀的Nginx配置： 1include /etc/nginx/conf.d/*.conf; 而Projected Volume的挂载是在容器启动步骤最开始就进行的。所以当容器启动之前，会从ConfigMap中获取default.conf配置文件，放到&#x2F;etc&#x2F;nginx&#x2F;conf.d目录中。当Nginx进程启动的时候，就会读到该站点的配置。既然Nginx的配置已经由ConfigMap提供，我们就可以不需要在源代码中包含。于是Dockerfile就被精简为： 12FROM nginx:alpineCOPY ./dist /usr/share/nginx/html 不管Pod在哪台宿主机上，都可以访问到ConfigMap，所以我们很容易就能猜到ConfigMap的数据保存在etcd上。除了以YAML方式定义，还可以通过–from-file参数将文件创建为ConfigMap。在一般情况下ConfigMap会先覆盖掉挂载目录然后再将ConfigMap中的内容作为文件挂载进行。如果想要不覆盖原本文件夹下的文件可以使用subPath参数。 1.1 热更新更新ConfigMap不会触发Pod的滚动更新，所以每次需要修改Pod Annotation的方式来强制触发滚动更新。具体命令如： 1kubectl patch deployment &lt;Deployment名&gt; --patch &#x27;&#123;&quot;spec&quot;: &#123;&quot;template&quot;: &#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;: &#123;&quot;version/config&quot;: &quot;20180411&quot; &#125;&#125;&#125;&#125;&#125;&#x27; 更多可以参考：ConfigMap的热更新 从这个角度来说，ConfigMap不太适合保存频繁更新的配置。 2. Secret除了ConfigMap之外，还有一种Projected Volume：Secret。从名字就可以猜得到，保存的是敏感信息，包括密码，认证token，密钥key等。像这些比较敏感的信息，直接写在Kubernetes的Deployment YAML定义里肯定不合适。放在Secret中会比较安全和灵活。除了保存信息是加密的之外，Secret和ConfigMap并没有太大差别。Kubernetes官网有一个生成用户名和密码的简单范例：使用 Secret 安全地分发凭证 - Kubernetes，这里就不多复述了。这里介绍Secret，主要是因为我们接下来要给网站添加强制HTTPS访问。要开启HTTPS访问，就先需要一个SSL证书。如果我们没有SSL证书的话，可以自己签发一个： 1openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/CN=traefik-ui.demosite.net&quot; 然后我们就可以通过如下的命令，根据SSL证书生成Secret： 1kubectl create secret tls traefik-cert --key=tls.key --cert=tls.crt -n kube-system 由于是供Traefik用的，所以创建在kube-system的namespace里。接下来我们修改Traefik配置。 3. 配置TLS3.1 配置traefik.toml我们接下来为Traefik Ingress Controller配置证书。Traefik的配置文件是traefik.toml。我们按照第1节的方式，将其配置为ConfigMap： 12345678910111213141516171819202122232425apiVersion: v1kind: ConfigMapmetadata: name: traefik-conf namespace: kube-systemdata: traefik.toml: | # 设置insecureSkipVerify = true，可以配置backend为443(比如dashboard)的ingress规则 insecureSkipVerify = true defaultEntryPoints = [&quot;http&quot;, &quot;https&quot;] [entryPoints] [entryPoints.http] address = &quot;:80&quot; ### 配置http 强制跳转 https [entryPoints.http.redirect] entryPoint = &quot;https&quot; ### 配置只信任trustedIPs传递过来X-Forwarded-*，默认全部信任；为了防止客户端地址伪造，需开启这个 #[entryPoints.http.forwardedHeaders] # trustedIPs = [&quot;10.1.0.0/16&quot;, &quot;172.20.0.0/16&quot;, &quot;192.168.1.3&quot;] [entryPoints.https] address = &quot;:443&quot; [entryPoints.https.tls] [[entryPoints.https.tls.certificates]] CertFile = &quot;/ssl/tls.crt&quot; KeyFile = &quot;/ssl/tls.key&quot; 其中的[entryPoints.http.redirect]就是强制重定向的配置。更多可选配置可以参考官方文档。 3.2 修改Traefik Ingress Controller配置在Ingress Controller引入ConfigMap中的配置和Secret中的证书，增加443端口。先上个配置全文： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980---apiVersion: v1kind: ServiceAccountmetadata: name: traefik-ingress-controller namespace: kube-system---kind: DaemonSetapiVersion: extensions/v1beta1metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lbspec: template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 volumes: - name: ssl secret: secretName: traefik-cert - name: config configMap: name: traefik-conf containers: - image: traefik name: traefik-ingress-lb volumeMounts: - mountPath: &quot;/ssl&quot; name: &quot;ssl&quot; - mountPath: &quot;/config&quot; name: &quot;config&quot; ports: - name: http containerPort: 80 hostPort: 80 - name: admin containerPort: 8080 hostPort: 8080 - name: https containerPort: 443 hostPort: 443 securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE args: - --api - --kubernetes - --logLevel=INFO - --configfile=/config/traefik.toml nodeSelector: node: edge---kind: ServiceapiVersion: v1metadata: name: traefik-ingress-service namespace: kube-systemspec: selector: k8s-app: traefik-ingress-lb ports: - protocol: TCP port: 80 name: http - protocol: TCP port: 443 name: https - protocol: TCP port: 8080 name: admin 首先把ConfigMap和Secret以Projected Volume的形式挂上： 1234567volumes:- name: ssl secret: secretName: traefik-cert- name: config configMap: name: traefik-conf 然后mount到container上： 12345volumeMounts:- mountPath: &quot;/ssl&quot; name: &quot;ssl&quot;- mountPath: &quot;/config&quot; name: &quot;config&quot; 在container和service里增加443端口： 12345volumeMounts:- mountPath: &quot;/ssl&quot; name: &quot;ssl&quot;- mountPath: &quot;/config&quot; name: &quot;config&quot; 启动参数里增加config文件： 12345args:- --api- --kubernetes- --logLevel=INFO- --configfile=/config/traefik.toml 因为不太确定怎么热加载Ingress Controller，所以我采用了先delete然后重新apply。然后我们就能以https形式访问了。顺带着Traefik Admin UI也变成https了： 4. 参考资料其他加载ConfigMap的方式，以及加载为环境变量的DemoK8S学习笔记之Kubernetes 配置管理 ConfigMap - 时光飞逝，逝者如斯 - 博客园 ConfigMap加载为命令行参数和非覆盖加载的DemoKubernetes对象之ConfigMap - 简书 官方的ConfigMap Demo使用ConfigMap来配置Redis - Kubernetes 网上有些配置较为过时，配置后虽然没有报错，但访问https地址就是遇到Connection Refused。下面这篇比较新一些，是本篇的主要参考：kubeasz&#x2F;ingress-tls.md at master · easzlab&#x2F;kubeasz","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://galaxyyao.github.io/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://galaxyyao.github.io/tags/k8s/"}]},{"title":"容器-11-Kubernetes实战-Ingress与Traefik","slug":"容器-11-Kubernetes实战-Ingress与Traefik","date":"2019-06-26T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/06/27/容器-11-Kubernetes实战-Ingress与Traefik/","permalink":"https://galaxyyao.github.io/2019/06/27/%E5%AE%B9%E5%99%A8-11-Kubernetes%E5%AE%9E%E6%88%98-Ingress%E4%B8%8ETraefik/","excerpt":"在上一篇中，通过NodePort模式，其实我们已经可以将一系列Pod暴露给集群外。但Service最多只能做好OSI 4层的负载均衡。而OSI 7层的负载均衡需要交给Ingress。简单解释一下，2层的负载均衡就是虚拟MAC地址接收请求；3层的负载均衡就是虚拟IP地址；4层就是基于IP + 端口；7层就涉及URI等应用层。以下文会提到的Traefik官方的一张图来说明，Ingress的作用就是根据不同的域名，正确找到对应的后台的Service：在Service接到请求后，再负责转交给Pod： 要使用Ingress需要先安装一个Ingress Controller。一般比较常用的有两个：Nginx Ingress Controller和Traefik Ingress Controller。在我做POC的过程中一开始选择Nginx Ingress Controller，但总是curl调不通，于是最终选择了Traefik。Traefik还多带了一个UI不错的后台管理admin dashboard。而Nginx的不少功能需要用Nginx Plus版的才有。虽然也很理解，毕竟Nginx也是要恰饭的。。。需要说明一点：虽然Nginx本身可以同时担任静态网站web server和反向代理两种职责，但Nginx Ingress Controller只负责反向代理。Ingress Controller除了这两个之外，还是有F5的、Kong的和Voyager等等。从这里就可以看到Kubernetes的一大特点：指定了方向，让各厂家和开源开发者发挥自己的特长来做实现。 1. Traefik Ingress安装安装按照官方文档一步步做就行了。先配置RBAC（关于RBAC我们之后详细介绍）： 1kubectl apply -f https://raw.githubusercontent.com/containous/traefik/v1.7/examples/k8s/traefik-rbac.yaml 然后配置Ingress Controller和对应的Service，对外暴露http的80端口和admin的8080端口。Ingress Controller有两种部署方式：Deployment和DaemonSet。DaemonSet是一种特殊的Pod： 在每个节点上有且仅有一个Pod实例 当有新Worker节点加入时，自动在新节点上创建；旧节点被删除后，上面的DaemonSet Pod会被自动回收 两种方式各有各的好处。官方文档上都有详细比较，并在最后很贴心地对选择困难症给了建议：遇事不决先用DaemonSet试试。那么作为POC我就却之不恭选择DaemonSet了。反正要吃后悔药也就是几个命令的事情。","text":"在上一篇中，通过NodePort模式，其实我们已经可以将一系列Pod暴露给集群外。但Service最多只能做好OSI 4层的负载均衡。而OSI 7层的负载均衡需要交给Ingress。简单解释一下，2层的负载均衡就是虚拟MAC地址接收请求；3层的负载均衡就是虚拟IP地址；4层就是基于IP + 端口；7层就涉及URI等应用层。以下文会提到的Traefik官方的一张图来说明，Ingress的作用就是根据不同的域名，正确找到对应的后台的Service：在Service接到请求后，再负责转交给Pod： 要使用Ingress需要先安装一个Ingress Controller。一般比较常用的有两个：Nginx Ingress Controller和Traefik Ingress Controller。在我做POC的过程中一开始选择Nginx Ingress Controller，但总是curl调不通，于是最终选择了Traefik。Traefik还多带了一个UI不错的后台管理admin dashboard。而Nginx的不少功能需要用Nginx Plus版的才有。虽然也很理解，毕竟Nginx也是要恰饭的。。。需要说明一点：虽然Nginx本身可以同时担任静态网站web server和反向代理两种职责，但Nginx Ingress Controller只负责反向代理。Ingress Controller除了这两个之外，还是有F5的、Kong的和Voyager等等。从这里就可以看到Kubernetes的一大特点：指定了方向，让各厂家和开源开发者发挥自己的特长来做实现。 1. Traefik Ingress安装安装按照官方文档一步步做就行了。先配置RBAC（关于RBAC我们之后详细介绍）： 1kubectl apply -f https://raw.githubusercontent.com/containous/traefik/v1.7/examples/k8s/traefik-rbac.yaml 然后配置Ingress Controller和对应的Service，对外暴露http的80端口和admin的8080端口。Ingress Controller有两种部署方式：Deployment和DaemonSet。DaemonSet是一种特殊的Pod： 在每个节点上有且仅有一个Pod实例 当有新Worker节点加入时，自动在新节点上创建；旧节点被删除后，上面的DaemonSet Pod会被自动回收 两种方式各有各的好处。官方文档上都有详细比较，并在最后很贴心地对选择困难症给了建议：遇事不决先用DaemonSet试试。那么作为POC我就却之不恭选择DaemonSet了。反正要吃后悔药也就是几个命令的事情。 最后部署一个Traefik Dashboard的UI。配置的时候需要注意根据自己的情况调整host域名，然后我们就能通过域名访问admin Dashboard了。 2. Ingress YAML定义对于Ingress来说，最关键的就是定义找服务的规则：IngressRule。下面是最简单的Ingress模板： 12345678910111213piVersion: extensions/v1beta1kind: Ingressmetadata: name: &lt;ingress-name&gt;spec: rules: - host: &lt;host-name&gt; http: paths: - path: / backend: serviceName: &lt;service-name&gt; servicePort: http 如果配置过Nginx的话就很容易理解这个配置文件了。需要注意的是：Nginx的server_name可以是ip，但Ingress的spec.rules.host必须是一个域名格式（FQDN），不能是ip。我们的POC项目按照这个模板配置一下： 1234567891011121314apiVersion: extensions/v1beta1kind: Ingressmetadata: name: poc-ingressspec: rules: - host: poc.demosite.net http: paths: - path: / backend: serviceName: poc-web-service servicePort: http 然后配置一个DNS指向任何一个Worker节点的ip，或修改本地的hosts，就可以访问我们的静态网站了。如果是本机测试没有域名，可以将YAML简化为： 12345678apiVersion: extensions/v1beta1kind: Ingressmetadata: name: poc-ingressspec: backend: serviceName: poc-web-service servicePort: 80 3. Ingress Controller高可用我们在上文使用的DNS指定节点的方式会有不少弊端。虽然我们可以给每个域名都配置所有Worker节点的ip，即Round-robin DNS方式。但Round-robin本身就不是一个故障转移方案。在Kubernetes官方的Service章节就有提到其他缺陷。 一般有两种方式： 3.1 Deployment方式部署Ingress Controller，Service类型指定为LoadBalancer如果使用公有云，或私有服务器有自己的LoadBalancer，一般就使用该方案了。云会给每个LoadBalancer类型的Service分配公网ip地址。但公有云的LoadBalancer服务是要收费的，而自己很难部署。 3.2 DaemonSet方式在边缘节点部署Ingress Controller，外部通过虚拟ip和keepalived访问边缘节点首先我们可以通过label命令，选定几台服务器作为边缘节点： 12kubectl label node docker-5 node=edgekubectl label node docker-6 node=edge 然后修改traefik-ds.yaml，将通过nodeSelector，限定Ingress Controller部署在边缘节点上： 1234567891011spec: template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller # 中间省略 nodeSelector: node: edge 在重新apply后，可以看到traefik-ingress-controller的Pod数量降低到了2个。然后参考这篇部署keepalived即可：边缘节点配置 4. 参考资料Kubernetes Ingress Controller的使用介绍及高可用落地 · Service Mesh|服务网格中文社区http://www.servicemesher.com/blog/kubernetes-ingress-controller-deployment-and-ha/ Traefik Ingress Controller的安装和使用官方文档Kubernetes - Traefik Nginx Ingress Controller的一个范例，虽然最后没跑起来Kubernetes Ingress with Nginx Example - Kubernetes BookNginx Ingress Controller的排查手册Troubleshooting - NGINX Ingress Controller","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://galaxyyao.github.io/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://galaxyyao.github.io/tags/k8s/"}]},{"title":"容器-10-Kubernetes实战-Service","slug":"容器-10-Kubernetes实战-Service","date":"2019-06-25T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/06/26/容器-10-Kubernetes实战-Service/","permalink":"https://galaxyyao.github.io/2019/06/26/%E5%AE%B9%E5%99%A8-10-Kubernetes%E5%AE%9E%E6%88%98-Service/","excerpt":"Deployment只是保证了支撑服务的Pod的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod。因此不能以确定的IP和端口号提供服务。要稳定地提供服务，需要服务发现和负载均衡能力。服务发现是一个微服务中很基础的概念，即当服务提供者网络发生变化时，服务消费者能及时获得最新的位置信息。对于k8s来说，服务提供者就是Pod，提供服务发现能力的是Service。Deployment和Service分别负责Pod的部署和访问策略，互不相关。所以从下面这张图也可以看出来，Service和Deployment并不是上下层级的关系。 1. Service YAML一个简单的Service YAML模板如下： 1234567891011apiVersion: v1kind: Servicemetadata: name: &lt;endpoint-name&gt;spec: ports: - port: &lt;port&gt; name: &lt;port-name&gt; targetPort: &lt;target-port-number&gt; selector: app: &lt;app-name&gt; 可以看到它是由selector.app来选择需要暴露的Pod。spec.ports.port是service对外暴露的端口，而targetPort是Pod的端口。默认使用TCP协议。对于我们的POC的网站，service YAML定义如下： 1234567891011apiVersion: v1kind: Servicemetadata: name: poc-web-servicespec: ports: - port: 80 name: web targetPort: 80 selector: app: poc-web 当我们查看已部署的service的时候，可以看到该service对应的ip：","text":"Deployment只是保证了支撑服务的Pod的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod。因此不能以确定的IP和端口号提供服务。要稳定地提供服务，需要服务发现和负载均衡能力。服务发现是一个微服务中很基础的概念，即当服务提供者网络发生变化时，服务消费者能及时获得最新的位置信息。对于k8s来说，服务提供者就是Pod，提供服务发现能力的是Service。Deployment和Service分别负责Pod的部署和访问策略，互不相关。所以从下面这张图也可以看出来，Service和Deployment并不是上下层级的关系。 1. Service YAML一个简单的Service YAML模板如下： 1234567891011apiVersion: v1kind: Servicemetadata: name: &lt;endpoint-name&gt;spec: ports: - port: &lt;port&gt; name: &lt;port-name&gt; targetPort: &lt;target-port-number&gt; selector: app: &lt;app-name&gt; 可以看到它是由selector.app来选择需要暴露的Pod。spec.ports.port是service对外暴露的端口，而targetPort是Pod的端口。默认使用TCP协议。对于我们的POC的网站，service YAML定义如下： 1234567891011apiVersion: v1kind: Servicemetadata: name: poc-web-servicespec: ports: - port: 80 name: web targetPort: 80 selector: app: poc-web 当我们查看已部署的service的时候，可以看到该service对应的ip： 12345[root@docker-4 poc]# kubectl get service -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORhostnames ClusterIP 10.99.149.202 &lt;none&gt; 80/TCP 14d app=hostnameskubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 28d &lt;none&gt;poc-web-service ClusterIP 10.110.52.238 &lt;none&gt; 80/TCP 4m43s app=poc-web 甚至YAML中selector也不是必须的。可以额外定义一个映射到外部ip&#x2F;域名+端口的EndPoint，然后将Service指向这个EndPoint。这个特性我理解是类似于对集群内的反向代理。如果有一个服务在测试环境调用集群内服务，生产环境调用集群外服务，只需要在service里定义两套namespace或env的label就可以解决了。 2. kube-proxy每个Pod有自己独一无二的ip。但当一组Pod组成了一个Service对外提供服务时，只能保持一个ip对外。当我们请求这个ip时，Kubernetes需要将我们的请求相对平均地分发给每个Pod。这个听上去像什么？没错，就是虚ip（virtual ip） + 反向代理（reverse proxy）。Kubernetes中为service提供虚ip + 反向代理的就是kube-proxy。kube-proxy有三种实现方式： userspace iptables ipvs iptables是当前版本的默认。从名称上就可以猜到是通过在宿主机上设置iptables规则来实现的。由于是内核态，所以性能比用户态的userspace方式高。但节点和Pod多了之后刷新iptables规则就变成了瓶颈。所以待ipvs成熟后应该会改为ipvs。以下这张就是的示意图： 3. Service对外发布服务的方式Service可以通过type属性，以不同的方式对外发布服务，包括： ClusterIP NodePort LoadBalancer ExternalName 3.1 ClusterIP这是不指定type时的默认方式。暴露为一个集群内部的ip，只能在集群内部访问。这个也是我们POC采用的方式。 3.2 NodePort通过NAT的方式，在选定的数个节点上以IP形式暴露。可以通过那几个宿主机节点中的任意一个的ip+端口访问。该模式经常用于外部还有一个独立的负载均衡服务的时候使用。 如果我们现在就急不可待想从集群外访问看一下，就可以稍微改一下配置，在ports的同一级加一个type: NodePort。 1234[root@docker-4 poc]# kubectl get serviceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 28dpoc-web-service NodePort 10.105.188.12 &lt;none&gt; 80:30611/TCP 24s 如果没有指定端口的话，会随机分配一个30000-32767之间的端口。查好被分配的端口，然后就可以通过http://任意一个Worker节点的ip:30611/ 访问我们的网站了。可以自己通过nodePort参数指定固定端口。但如果不在30000-32767的范围，就会报错： 1The Service &quot;poc-web-service&quot; is invalid: spec.ports[0].nodePort: Invalid value: 80: provided port is not in the valid range. The range of valid ports is 30000-32767 为什么是任何一个Worker节点的ip都可以有效，是因为分为两种情况： Pod在该宿主机节点上 Pod不在该宿主机节点上 如果Pod在该节点上，那么没问题IP包直接给Pod。如果不再该节点上，节点之间会做SNAT，即原地址转换。IP包会由接收到的节点转给带有Pod的宿主机节点。 12345678910 client \\ ^ \\ \\ v \\ node 1 &lt;--- node 2 | ^ SNAT | | ---&gt; v |endpoint 3.3 LoadBalancer借用外部云服务的负载均衡能力，暴露一个固定的ip。使用公有云服务基本使用该方式。 3.4 ExternalName通过一个固定的CNAME记录暴露，kube-dns 1.7版本之后的特性，方便实现上文提到的无selector Service。 4. 参考资料Kubernetes Service的官方文档Service - KubernetesUsing a Service to Expose Your App - Kubernetes ExternalName的一些范例Kubernetes Tips - Part 1 各种service type的更详细介绍Kubernetes service types","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://galaxyyao.github.io/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://galaxyyao.github.io/tags/k8s/"}]},{"title":"容器-9-Kubernetes实战-当你拍下kubectl命令背后的行为","slug":"容器-9-Kubernetes实战-当你拍下kubectl命令背后的行为","date":"2019-06-24T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/06/25/容器-9-Kubernetes实战-当你拍下kubectl命令背后的行为/","permalink":"https://galaxyyao.github.io/2019/06/25/%E5%AE%B9%E5%99%A8-9-Kubernetes%E5%AE%9E%E6%88%98-%E5%BD%93%E4%BD%A0%E6%8B%8D%E4%B8%8Bkubectl%E5%91%BD%E4%BB%A4%E8%83%8C%E5%90%8E%E7%9A%84%E8%A1%8C%E4%B8%BA/","excerpt":"我们上一章部署都是通过神奇的kubectl命令。我们这章就探寻一下，当我们拍下kubectl命令到Pod成功启动之间，Kubernetes究竟做了一些什么事情。先上一张总的架构图，下面提到每个组件的时候可以在这张架构图上找位置，以及和其他组件间的关联关系： 1. 全流程1.1 Kubectlkubectl是用于针对Kubernetes集群运行命令的命令行接口。虽然我们是在Master节点上执行运行的kubectl，但其实kubectl也可以在本地安装，与k8s的api server远程通信交互。kubectl在接到apply命令后，会先做一个基本的验证。如果要创建的资源不合法，或YAML格式错误，就会快速失败。除了通过kubectl之外，也可以直接调用api，或通过dashboard UI等多种方式与api server通信。 在通信之前，kubectl需要先进行身份认证。认证信息保存在$HOME&#x2F;.kube&#x2F;config文件里，大致内容如下： 123456789101112131415161718192021[root@docker-4 .kube]# pwd/root/.kube[root@docker-4 .kube]# cat configapiVersion: v1clusters:- cluster: certificate-authority-data: &lt;证书授权信息&gt; server: https://10.16.34.54:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetescurrent-context: kubernetes-admin@kuberneteskind: Configpreferences: &#123;&#125;users:- name: kubernetes-admin user: client-certificate-data: &lt;客户端证书数据&gt; config文件中的clusters.cluster.server就是要访问的api server的地址 1.2 kube-apiserver","text":"我们上一章部署都是通过神奇的kubectl命令。我们这章就探寻一下，当我们拍下kubectl命令到Pod成功启动之间，Kubernetes究竟做了一些什么事情。先上一张总的架构图，下面提到每个组件的时候可以在这张架构图上找位置，以及和其他组件间的关联关系： 1. 全流程1.1 Kubectlkubectl是用于针对Kubernetes集群运行命令的命令行接口。虽然我们是在Master节点上执行运行的kubectl，但其实kubectl也可以在本地安装，与k8s的api server远程通信交互。kubectl在接到apply命令后，会先做一个基本的验证。如果要创建的资源不合法，或YAML格式错误，就会快速失败。除了通过kubectl之外，也可以直接调用api，或通过dashboard UI等多种方式与api server通信。 在通信之前，kubectl需要先进行身份认证。认证信息保存在$HOME&#x2F;.kube&#x2F;config文件里，大致内容如下： 123456789101112131415161718192021[root@docker-4 .kube]# pwd/root/.kube[root@docker-4 .kube]# cat configapiVersion: v1clusters:- cluster: certificate-authority-data: &lt;证书授权信息&gt; server: https://10.16.34.54:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetescurrent-context: kubernetes-admin@kuberneteskind: Configpreferences: &#123;&#125;users:- name: kubernetes-admin user: client-certificate-data: &lt;客户端证书数据&gt; config文件中的clusters.cluster.server就是要访问的api server的地址 1.2 kube-apiserverAPI Server对外暴露了Kubernetes API，用于提供查询&#x2F;操作&#x2F;监控服务。当接收到来自kubectl的请求后，API Server会先做三件事： 验证认证信息 确认授权，即发送请求的用户有权限进行这个操作 准入控制，封装了一系列额外的检查以确保操作不会产生意外或负面结果。还可以自定义插件实现自己的准入控制 1.3 etcdetcd是一种高可用分布式存储，用于共享配置和服务发现。之前在研究服务注册的时候还比较过它与Consul。etcd和Consul一样都是在CAP中保证CP，都是用Go语言开发的，一致性协议也都是用raft。Consul相比etcd多了多数据中心的支持。当然在k8s出现的时候还没有Consul，只有在zookeeper和etcd之间选。etcd相比zookeeper能稳定提供更大的吞吐量和延迟，而且和k8s使用的开发语言都是Go，这大概是最终选择了etcd的主要原因吧。 k8s集群将etcd当做数据库来使用，把所有的数据都存储在etcd上。当执行kubectl get命令时，结果就是从etcd中获取的。假设kubectl执行的是创建上一篇中nginx-deployment的行为，那么最终etcd中保存的是4个对象： 1个Deployment对象 1个ReplicaSet对象 2个Pod对象关于为什么还多了一个ReplicaSet对象，我们在下面说明。 1.4 Initializer初始化在Pod还处于Pending状态，可以对Pod进行一些修改。例如给容器插入一个Sidecar容器，添加一些环境变量，挂载volume等等。Initializer初始器就是负责这个工作的。最热门的Service Mesh–Istio项目就是通过Initializer，将Envoy容器作为Sidecar插入到每个启动的Pod中的。 1.5 控制循环Kubernetes内部始终在运行着一个“控制循环”来实现资源的调整。控制循环，就是控制平面的死循环。每次循环过程中，都会通过将k8s的“当前状态”和“期望状态”进行比对，来决定下一步进行什么操作。用伪代码来描述就是： 123456789for &#123; 实际状态 := 获取集群中对象 X 的实际状态（Actual State） 期望状态 := 获取集群中对象 X 的期望状态（Desired State） if 实际状态 == 期望状态&#123; 什么都不做 &#125; else &#123; 执行编排动作，将实际状态调整为期望状态 &#125;&#125; 例如当刚接收到nginx-deployment的命令时，期望是要部署2个pod，实际状态是0个pod已Ready，差额是2个： 123[root@docker-4 deployment]# kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 0/2 2 0 3s 当部署完成后，期望状态&#x3D;&#x3D;实际状态，部署结束： 123[root@docker-4 deployment]# kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 2/2 2 2 19s 1.6 DeploymentController与ReplicaSet对于每个对象类型，由kube-controller-manager对应的controller来创建。例如Deployment就对应DeploymentController。在一些比较早的文章里，你还能看到ReplicationController，但现在它已经不再被使用。DeploymentController是其升级版，在包含了ReplicationController所有功能的基础上还增加了回滚暂停等功能。 在说明DeploymentController之前，先提一下上一章里没有提到的一个细节：Deployment和Pod之间还隔了一层ReplicaSet。 保持副本数量其实主要是靠ReplicaSet。从一个ReplicaSet的YAML可以看到，几乎和Deployment一模一样： 1234567891011121314151617181920apiVersion: apps/v1kind: ReplicaSetmetadata: name: nginx-set labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 但我们之所以不直接使用ReplicaSet部署，是由于ReplicaSet的功能比较简陋。当我们想实现滚动更新的时候，就需要更上一层的Deployment支援了。 DeploymentController通过一个叫Informer的模块对Deployment、ReplicaSet和Pod的变更进行监听。假设上述范例中poc-deployment里的应用升级了一个版本，从v1升级到了v2。控制循环会获得一个新的期望：部署两个v2的Pod。现状是有两个v1的Pod。但此时不能立即把v1的Pod停止了，不然服务就会有一段时间不可用了。在整个滚动更新的过程中，需要保证至少有两个Pod可用，无论是v1还是v2。所以这时候Deployment会创建一个v2的ReplicaSet，包含v2的Pod。当v2的ReplicaSet中Pod的状态Ready后，v1的ReplicaSet就可以进行缩容为0个Pod了。 PS. 我们实际场景中可能会有会话黏连（session sticky）的情况存在。会话还处于活跃状态的Pod不应该被直接下线。怎么处理就是后话了。 1.7 kube-schedulerPod创建好之后，还没有被分派节点。kube-scheduler就是用来将待分派的Pod调度到指定Worker节点，并将节点与Pod的绑定信息也记录到etcd。Master上的工作到此为止。 1.8 kubelet每个Worker节点上会有一个Kubelet服务进程。kube-scheduler下发的任务就是由kubelet接收的。除此之外，它还负责： 挂载Pod所需要的volume 下载Pod的secret 运行容器 对容器生命周期进行检测 回报节点和Pod的状态可以把Kubelet当成一种特殊的Controller。 至此容器正常启动，整个流程结束。 2. 声明式APIkubectl apply和docker run看上去是两句很类似的命令，但表现出来的理念截然不同。docker run是命令式的。你发出命令，服务器接收，并按照命令创建出容器。但Kuberentes的API是所谓的“声明式”，即你向Kubernetes提交一个定义好的API对象，声明自己想要达到的目标状态。当Kubernetes接收到这个目标状态后，自己内部协调各种组件，达成并保持这个状态。 声明式对于分布式系统有着重大的意义。 首先是能实现自动化调整。分布式系统的每个组件都可能会随时发生故障。假设一个节点在部署某个Pod的过程中突然挂掉了，如果采用的是命令式API，就需要人工干预：“我换个节点再重新拍命令。等恢复那个节点后再进行之前操作的回滚”。但对于使用了声明式API的Kubernetes，会在每个控制循环的开始检查：“之前部署Pod的任务还没完成，和kubelet联系一下，问问看Node进展如何了？怎么联系不上Node？换个Node部署吧。”在挂掉的Node恢复后，它会自动调用API Server获取当前状态并进行分析：“之前要我部署的Pod已经在其他Node上部署好了？如果我继续部署的话，Pod数量就比目标多了。那么我把自己进行到一半的操作回滚吧。”整个过程完全无需外界干预。 其次，对于命令式API，每个命令都是独占且阻塞的。只有先等前一个命令执行完之后才能执行下一个命令，不然就有出现冲突的可能。而声明式API使得多个写操作都能并行执行，使得处理效率大大提升。 此外，声明式API还支持操作的合并。你可以设置一个YAML为基础YAML，在用户提交YAML后会和基础YAML合并，然后再提交给API Server。我感觉这有点像Java里的自定义拦截器。知名的Istio项目是主要实现原理也就是靠这种方式注入Envoy。 3. 参考资料本篇主要参考了jamiehannaford&#x2F;what-happens-when-k8s: What happens when I type kubectl run?翻译版 对k8s如何使用etcd的简要介绍How Does Kubernetes Use etcd? Deployment原理主要是参考这篇详解 Kubernetes Deployment 的实现原理 关于Informer机制的更详细介绍Kubernetes Informer 详解_Kubernetes中文社区 这篇是Kubernetes的开发者介绍的Kubernetes设计原则，值得完整读一下Kubernetes 设计与开发原则 - 杨传胜的博客|Cloud Native|yangcs.net原文是这篇：Kubernetes Design and Development Explained - The New Stack","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://galaxyyao.github.io/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://galaxyyao.github.io/tags/k8s/"}]},{"title":"容器-8-Kubernetes实战-k8s核心概念之Node,Pod与Deployment","slug":"容器-8-Kubernetes实战-k8s核心概念之Node-Pod与Deployment","date":"2019-06-23T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/06/24/容器-8-Kubernetes实战-k8s核心概念之Node-Pod与Deployment/","permalink":"https://galaxyyao.github.io/2019/06/24/%E5%AE%B9%E5%99%A8-8-Kubernetes%E5%AE%9E%E6%88%98-k8s%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BNode-Pod%E4%B8%8EDeployment/","excerpt":"本篇我们开始用k8s的方式部署静态网站镜像，并通过这个过程了解k8s为什么会抽象出那么多概念。 1. Nodek8s首先需要选定在哪一台或哪几台服务器上部署。如我们在kubeadm部署k8s集群的时候就已知的，k8s集群是由1-N台Master节点和N个Worker节点组成的。 k8s的设计原则之一就是不挑Worker节点的硬件配置。毕竟当初Google搭Borg集群的时候淘到的服务器硬件各式各样都有。（想起来当初搭Hadoop的时候也有人问过我是不是什么硬件都可以。。。以Hadoop MapReduce的吃硬盘和网络程度，实体机+专用万兆宽带跑出来的性能比虚拟机快出好几倍。当然配置低也的确能跑起来不死。）你手头的机器可能是什么歪瓜裂枣都有：不管是实体机还是虚拟机，不管什么硬件配置，不管高的矮的胖的瘦的，k8s都将其一视同仁地抽象为一个Node（曾经也有一个专有名词minion）。 一个典型的Node信息如下（部分删减）： 12345678910111213141516171819202122232425262728293031Name: docker-7Roles: workerConditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Fri, 21 Jun 2019 16:58:54 +0800 Fri, 21 Jun 2019 16:57:54 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 21 Jun 2019 16:58:54 +0800 Fri, 21 Jun 2019 16:57:54 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Fri, 21 Jun 2019 16:58:54 +0800 Fri, 21 Jun 2019 16:57:54 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Fri, 21 Jun 2019 16:58:54 +0800 Fri, 21 Jun 2019 16:57:54 +0800 KubeletReady kubelet is posting ready statusAddresses: InternalIP: 10.16.34.59 Hostname: docker-7Capacity: cpu: 4 ephemeral-storage: 101729776Ki hugepages-2Mi: 0 memory: 8010576Ki pods: 110Allocatable: cpu: 4 ephemeral-storage: 93754161407 hugepages-2Mi: 0 memory: 7908176Ki pods: 110Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 320m (8%) 300m (7%) memory 150Mi (1%) 150Mi (1%) ephemeral-storage 0 (0%) 0 (0%) Master节点不断轮询更新每个Node的状态信息：“你还活着么？压力大不大？CPU&#x2F;内存&#x2F;存储已经分配了多少？还剩下多少？”这样当Master节点为了新的容器需求征兵时，就能很容易地知道哪几个节点还够压榨。 2. Pod","text":"本篇我们开始用k8s的方式部署静态网站镜像，并通过这个过程了解k8s为什么会抽象出那么多概念。 1. Nodek8s首先需要选定在哪一台或哪几台服务器上部署。如我们在kubeadm部署k8s集群的时候就已知的，k8s集群是由1-N台Master节点和N个Worker节点组成的。 k8s的设计原则之一就是不挑Worker节点的硬件配置。毕竟当初Google搭Borg集群的时候淘到的服务器硬件各式各样都有。（想起来当初搭Hadoop的时候也有人问过我是不是什么硬件都可以。。。以Hadoop MapReduce的吃硬盘和网络程度，实体机+专用万兆宽带跑出来的性能比虚拟机快出好几倍。当然配置低也的确能跑起来不死。）你手头的机器可能是什么歪瓜裂枣都有：不管是实体机还是虚拟机，不管什么硬件配置，不管高的矮的胖的瘦的，k8s都将其一视同仁地抽象为一个Node（曾经也有一个专有名词minion）。 一个典型的Node信息如下（部分删减）： 12345678910111213141516171819202122232425262728293031Name: docker-7Roles: workerConditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Fri, 21 Jun 2019 16:58:54 +0800 Fri, 21 Jun 2019 16:57:54 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 21 Jun 2019 16:58:54 +0800 Fri, 21 Jun 2019 16:57:54 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Fri, 21 Jun 2019 16:58:54 +0800 Fri, 21 Jun 2019 16:57:54 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Fri, 21 Jun 2019 16:58:54 +0800 Fri, 21 Jun 2019 16:57:54 +0800 KubeletReady kubelet is posting ready statusAddresses: InternalIP: 10.16.34.59 Hostname: docker-7Capacity: cpu: 4 ephemeral-storage: 101729776Ki hugepages-2Mi: 0 memory: 8010576Ki pods: 110Allocatable: cpu: 4 ephemeral-storage: 93754161407 hugepages-2Mi: 0 memory: 7908176Ki pods: 110Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 320m (8%) 300m (7%) memory 150Mi (1%) 150Mi (1%) ephemeral-storage 0 (0%) 0 (0%) Master节点不断轮询更新每个Node的状态信息：“你还活着么？压力大不大？CPU&#x2F;内存&#x2F;存储已经分配了多少？还剩下多少？”这样当Master节点为了新的容器需求征兵时，就能很容易地知道哪几个节点还够压榨。 2. PodPod是Kubernetes中的原子调度单位。最简单的Pod就等于一个容器。这么表述也就意味着Pod里也可以放多个容器。看为什么Kubernetes会发明出来Pod这个概念？因为有些容器之间有共享网络和存储的需求。 举个最典型的例子：日志收集器。例如我们的静态网站容器会在被访问的时候生成access.log文件。如果是在虚机里部署，那么就会在服务器上另外起一个logstash，收集这些日志文件后汇总到Elasticsearch。如果logstash和Nginx不在同一台宿主机上部署，虽然也不是不可行，但就会很折腾：后文会提到挂载PV，可以在logstash和Nginx的Pod上都挂载同一个PV。但这个折腾毫无必要。日志文件并不那么重要，不值得永久存储，生命周期跟着容器即可。当容器被销毁时，日志也可以跟着被销毁。 那么我们是不是可以把Nginx和logstash打包到同一个容器中呢？这会产生一个问题：当logstash进程挂掉时，k8s的监控怎么表示容器的状态？如果显示Failure然后重启容器，则无辜的Nginx进程也受到了牵连；如果显示正常，那么要单独重启容器里的某一个进程就会变得很麻烦。所以我们需要将Nginx和logstash单独打包为容器，部署成一个Pod。k8s会将一个Pod里的容器都部署在同一台宿主机上。 k8s的官方博客将多容器Pod的类型列为三种： Sidecar（边车）容器 Ambassador（大使）容器 Adapter（适配器）容器k8s Pod的实现方式特别适合我们将一些控制平面的功能放到Sidecar中。近几年很知名的Service Mesh项目就是完全通过Sidecar模式支撑起来的。我们以后再详细讨论这个话题。 其实对于Pod不用想得太复杂，可以认为它就是逻辑上的一台虚机的概念。只有必须部署在同一台虚机上的，才会被并到一个Pod里。例如虽然MySQL和Java应用虽然也可以部署在同一台虚机上，但从best practice考量，一般不会这么部署，所以就不是一个Pod。MySQL的Master和Slave一般是部署在两台服务器上，所以也是两个独立的Pod。 2.1 Pod YAML要部署一个Pod，我们需要先写一个YAML描述文件，然后用k8s的命令部署。我从Spring Boot的配置开始就已经接触了挺久的YAML，所以这边就不详细介绍YAML的语法了。如果有对语法部署的可以参考Wiki。一个最简单的Nginx Pod部署YAML如下： 12345678910apiVersion: v1kind: Podmetadata: name: nginx-pod labels: app: nginxspec: containers: - name: nginx image: nginx:latest 这个YAML第一层的元素有四个：apiVersion, kind, metadata和spec。apiVersion的v1表示是稳定版。（像之前kubeadm的apiVersion就是kubeadm.k8s.io&#x2F;v1beta1，还在快速迭代中）。kind就是想要创建的对象的类型。metadata是用来识别对象的唯一性。spec字段是对象规约，内容随着每个Kubernetes对象而不同。对于该Nginx Pod来说，需要指定使用的唯一镜像以及镜像的版本。关于镜像和镜像版本可以在Docker Hub上查到。 2.2 部署命令我们可以使用如下的命令部署： 1kubectl apply -f nginx-pod.yaml 然后就可以通过如下命令查看Pod创建进展： 1kubectl get pod 以及通过如下命令查看Pod详细信息： 1kubectl describe pod &lt;Pod名&gt; 想进入Pod，就是用exec -it命令： 1kubectl exec -it nginx-pod /bin/bash 如果不需要这个Pod了，可以用以下命令取消部署： 1kubectl delete -f nginx-pod.yaml k8s的命令虽然很多，但都非常有规律。基本格式都是： 1kubectl 动作 对象类型 或 1kubectl 动作 [-参数] 对象类型 对象名 对于非default namespace的对象，再加一个namespace参数： 1kubectl 动作 对象类型 对象名 -n namespace名 基本不怎么需要特别记忆。 另外提一下，在使用了k8s后，最好不要再打Docker命令了。当然自己本地开发机上跑容器的时候还是需要拍Docker命令。在本次POC中，我们不将Nginx部署为Pod。原因在下一章中说明。 2.3 验证此时Pod已部署成功，但我们还没法从容器外部访问。想验证的话可以稍微修改一下YAML，增加一个验证的shell容器，然后共享PID Namespace： 123456789101112131415apiVersion: v1kind: Podmetadata: name: nginx-pod labels: app: nginxspec: shareProcessNamespace: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true 上述配置在spec下增加了shareProcessNamespace: true，表示PID Namespace共享。最底下还增加一个镜像为busybox的容器。（busybox和alpine由于体积比较小，在k8s部署的时候有广泛的用途）要部署之前，我们需要先kubectl delete -f nginx-pod.yaml把pod删除，不然会得到告警： 1spec.containers: Forbidden: pod updates may not add or remove containers 在重新apply后使用如下的命令进入Pod 1kubectl attach -it nginx-pod -c shell 进入busybox容器后执行ps aux，就可以看到Nginx的进程了： 1234567/ # ps auxPID USER TIME COMMAND 1 root 0:00 /pause 6 root 0:00 nginx: master process nginx -g daemon off; 11 101 0:00 nginx: worker process 12 root 0:00 sh 17 root 0:00 ps aux 2.4 infra容器：pause上一节进程的查询结果中有一个pause进程需要说明一下。当我们使用docker ps查看容器时，会发现有相当多的pause容器在启动。该容器和我们前几节提到的多容器Pod有关。以我们上面修改后的nginx-pod范例为例。当我们需要让两个容器共享Namespace（不仅仅是IPC，还包括Network等其他Namespace），方法之一是让一个容器先启动，然后将另外一个容器的Namespace设置为前一个容器的Namespace。从技术上可行，但这个会导致这两个容器之间的关系不再是对等拓扑关系。所以需要有个中间容器，也就是pause容器存在。pause容器非常小，也基本干不了什么事。它唯一的作用就是在Pod启动的最开始就启动，然后在其他容器启动后和它们共享自己的Namespace。在所有的容器启动完成后就暂停自己，不再消耗资源。 3. DeploymentPod的副本实例数是在Deployment中定义的。假设我们从负载和高可用的角度考虑，想要在k8s中部署2个Nginx的实例，那么只需要在Deployment的配置中定义replicas: 2即可。 我们实际写k8s yaml配置的时候，不推荐直接部署为Pod。即使是只有一个副本，也推荐部署为replicas为1的Deployment。这是由于k8s的调度机制是通过Deployment来确保副本数量。万一Pod所在的服务器挂了，k8s会检测到副本数不足1，于是将Pod调度到健康的Node上。 3.1 Deployment YAML一个Nginx Deployment的精简版的YAML如下： 12345678910111213141516171819202122apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 name: web 可以看到spec下有一个replicas，表述副本数量。 看到这个YAML，我第一反应是有些晕：为什么spec套了一层spec，template下还有一层metadata。这篇matchLabels, labels, and selectors explained in detail, for beginners解释了为什么这个YAML有点绕的原因。关键点是这个Deployment YAML中的template其实是podTemplate。这样你就会发现template中的部分只要加上apiVersion和kind，基本就是Pod的YAML。 与template平级的还有一个selector选择器属性。这个selector选择器表示deployment部署的是label里带app的podTemplate。label作为一个非唯一的标签属性，可以使Kubernetes的运维更加灵活。这个我们后续再详细研究。 我们现在可以总结出一个最简单版的deployment yaml模板： 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: labels: app: &lt;deployment-name&gt;spec: replicas: &lt;replicas-number&gt; selector: matchLabels: app: &lt;app-name&gt; template: metadata: labels: app: &lt;app-name&gt; spec: containers: - name: &lt;app-name&gt; image: &lt;image-name&gt;:&lt;tag-name&gt; ports: - containerPort: &lt;container-port&gt; name: &lt;container-port-name&gt; 3.2 部署命令部署命令和Pod基本一样： 1kubectl apply -f nginx-deployment.yaml 然后查看到ready的deployment了： 123[root@docker-4 pod]# kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 2/2 2 2 16h 3.3 部署POC Nginx镜像要部署POC的Nginx镜像，只需要将image修改为私有仓库的镜像即可： 12345678910111213141516171819202122apiVersion: apps/v1kind: Deploymentmetadata: name: poc-web labels: app: poc-webspec: replicas: 2 selector: matchLabels: app: poc-web template: metadata: labels: app: poc-web spec: containers: - name: poc-web image: 10.16.34.197:5000/staticsite:1.0 ports: - containerPort: 80 name: web 如果使用Docker Hub作为Docker Registry，则首先创建secret（什么是secret会在容器-12-Kubernetes实战-静态网站部署优化1:ConfigMap,Secret与TLS | Galaxy 中详细说明）： 1kubectl create secret docker-registry regcred --docker-username=账号 --docker-password=密码 --docker-email=Docker邮箱 -n 命名空间默认default 然后在pod定义的同一级加上imagePullSecrets，例如： 123456789101112131415161718192021222324apiVersion: apps/v1kind: Deploymentmetadata: name: poc-web labels: app: poc-webspec: replicas: 2 selector: matchLabels: app: poc-web template: metadata: labels: app: poc-web spec: containers: - name: poc-web image: galaxyyao/demosite:1.0 ports: - containerPort: 80 name: web imagePullSecrets: - name: regcred 4. 参考资料Kubernetes Node的官方文档概念介绍Nodes - Kubernetes Kubernetes Pod的官方文档概念介绍Pod Overview - Kubernetes Kubernetes Deployment的官方文档概念介绍Deployments - Kubernetes 这是一篇关于Pause容器的详细介绍The Almighty Pause Container - Ian Lewis 本篇部分有趣的图来自这篇。很简要易懂的科普文。Kubernetes &amp; Traefik 101— When Simplicity Matters – Gérald Croës – Medium 如果想补习YAML的话可以参考这篇：YAML basics in Kubernetes – IBM Developer","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://galaxyyao.github.io/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://galaxyyao.github.io/tags/k8s/"}]},{"title":"容器-7-Kubernetes实战-私有仓库和打包镜像","slug":"容器-7-Kubernetes实战-私有仓库和打包镜像","date":"2019-06-17T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/06/18/容器-7-Kubernetes实战-私有仓库和打包镜像/","permalink":"https://galaxyyao.github.io/2019/06/18/%E5%AE%B9%E5%99%A8-7-Kubernetes%E5%AE%9E%E6%88%98-%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E5%92%8C%E6%89%93%E5%8C%85%E9%95%9C%E5%83%8F/","excerpt":"我们先从比较简单的部分开始做。静态网站是一个比较合适的开端。独立，有界面方便验证，而且无状态。 1. 搭建Docker私有仓库按照容器的思路，我们需要先做一个静态网站文件+Nginx的镜像，然后在服务器上把镜像拖下来后实例化为容器运行。 从安全和网络速度上考虑，我们做的这个镜像不太适合放到公网的Docker Hub上。所以要先搭个私有仓库。 我们先在一台虚机上按照kubeadm部署指南中的“2.1 安装Docker”和“2.2 启动Docker服务”两节安装docker。然后关闭防火墙。最后执行一句docker run就解决了： 1docker run -d -p 5000:5000 --restart=always --name docker-registry registry 一开始没有关闭防火墙，在启动的时候报错了。再次启动的时候提示容器已存在。这时候只需要将docker run命令改为docker start命令就可以了。PS. 如果是要在Windows上运行，要把端口从5000映射到5001，不然会因为和Docker Desktop端口冲突而容器启动失败。我把Docker Registry私有仓库部署在10.16.34.197服务器上，于是现在访问http://10.16.34.197:5000/v2/_catalog，就可以看到当前还没有镜像： 1&#123;&quot;repositories&quot;:[]&#125;","text":"我们先从比较简单的部分开始做。静态网站是一个比较合适的开端。独立，有界面方便验证，而且无状态。 1. 搭建Docker私有仓库按照容器的思路，我们需要先做一个静态网站文件+Nginx的镜像，然后在服务器上把镜像拖下来后实例化为容器运行。 从安全和网络速度上考虑，我们做的这个镜像不太适合放到公网的Docker Hub上。所以要先搭个私有仓库。 我们先在一台虚机上按照kubeadm部署指南中的“2.1 安装Docker”和“2.2 启动Docker服务”两节安装docker。然后关闭防火墙。最后执行一句docker run就解决了： 1docker run -d -p 5000:5000 --restart=always --name docker-registry registry 一开始没有关闭防火墙，在启动的时候报错了。再次启动的时候提示容器已存在。这时候只需要将docker run命令改为docker start命令就可以了。PS. 如果是要在Windows上运行，要把端口从5000映射到5001，不然会因为和Docker Desktop端口冲突而容器启动失败。我把Docker Registry私有仓库部署在10.16.34.197服务器上，于是现在访问http://10.16.34.197:5000/v2/_catalog，就可以看到当前还没有镜像： 1&#123;&quot;repositories&quot;:[]&#125; 2. 镜像打包服务器配置在要打包和push镜像的服务器上（一般是Gitlab或Jenkins服务器吧），需要执行以下命令（要替换ip）： 12345678cat &lt;&lt;EOF &gt; /etc/docker/daemon.json&#123; &quot;insecure-registries&quot;: [ &quot;10.16.34.197:5000&quot; ]&#125;EOFsystemctl restart docker 要不然之后push的时候就会遇到报错信息： 123[root@docker-4 vue-hello-world]# docker push 10.16.34.197:5000/staticsiteThe push refers to repository [10.16.34.197:5000/staticsite]Get https://10.16.34.197:5000/v2/: http: server gave HTTP response to HTTPS client 另外也需要在k8s的每个master和worker节点上都执行。不然在部署的时候也会遇到报错信息。 3. 打包网站镜像略过不相关的web项目创建和提交代码过程，我们在Gitlab服务器上得到了需要部署的静态网站文件： 12345678910[root@docker-4 dist]# pwd/git/vue-hello-world/dist[root@docker-4 dist]# lltotal 12drwxr-xr-x. 2 root root 30 Jun 13 01:42 css-rw-r--r--. 1 root root 4286 Jun 13 01:42 favicon.icodrwxr-xr-x. 2 root root 31 Jun 13 01:42 img-rw-r--r--. 1 root root 730 Jun 13 01:42 index.htmldrwxr-xr-x. 2 root root 126 Jun 13 01:42 js[root@docker-4 dist]# 3.1 添加Nginx配置和Dockerfile要将该网站host在Nginx上，只需要做两个步骤： 将dist里的文件复制到Nginx的&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html目录下 做一个conf配置，根目录&#x2F;指向&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html，并设置默认首页网页文件名index.html 于是在网站下增加了一个default.conf文件： 123456789101112131415161718192021server &#123; listen 80; server_name localhost; charset utf-8; #access_log /var/log/nginx/log/host.access.log main; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125;&#125; 另外增加了一个Dockerfile： 123FROM nginx:alpineCOPY default.conf /etc/nginx/conf.d/default.confCOPY ./dist /usr/share/nginx/html nginx:alpine是Nginx官方的轻量级镜像。 3.2 执行打包命令然后到Dockerfile所在目录构建镜像并推送到私有仓库： 12docker build -t 10.16.34.197:5000/staticsite:1.0 .docker push 10.16.34.197:5000/staticsite 现在访问私有仓库的catalog目录，就能看到repositories中已增加了一个静态网站的镜像： 1&#123;&quot;repositories&quot;:[&quot;staticsite&quot;]&#125; 也可以不指定镜像版本，那么打出来的版本就是latest： 1docker build -t 10.16.34.197:5000/staticsite . 可以通过如下的URL查看该镜像的所有可用版本：http://10.16.34.197:5000/v2/staticsite/tags/list（要访问其他镜像替换staticsite这个镜像名就可以了） 4. 接下来的步骤如果只用docker，那么执行如下的命令就结束了： 1docker run -itd --name staticsite --publish 8080:80 10.16.34.197:5000/staticsite:1.0 但到此为止的话，我们还是需要明确地指定在哪台机器上部署该容器。如果需要在多台机器上部署为高可用，就需要重复N遍同样的操作。为了保持高可用，还需要在部署容器的服务器前面额外部署负载均衡。作为懒人，我很希望只要定义好需要的副本数量以及每个容器的硬件需求，就有人能替我把这些事情都包办了。万一哪个容器挂了还能迅速自动重启。k8s就是这样的集群容器管家。我们下一章继续。 PS. Private Registry也可以部署在k8s上（所以上一章的架构图中它也在虚线框内），但为了简化POC的流程，我们先把这个放在后面做。 5. 使用Docker Hub作为Docker Registry自己Demo的时候也可以省掉这个步骤，直接将镜像上传到Docker Hub，然后K8S从Docker Hub上拉取镜像。在注册Docker Hub账号之后，如果是Windows机器，需要打开Docker Desktop，然后输入docker login就可以直接登录了，不需要再输入密码。如果是Linux上，则需要在docker login命令后输入用户名和密码。构建镜像和推送命令需要改为： 12docker build -t 账号/staticsite:1.0 .docker push 账号/staticsite 在K8S集群里拉取镜像的步骤在下一章再介绍。 更详细步骤可以参考Pull an Image from a Private Registry - Kubernetes 6. 流程图画了一下按角色划分的发布流程图。不管用的是k8s还是docker swarm，CI&#x2F;CD的流程还是基本类似的。 7. 参考资料这是本篇参考的一个最精简的静态网站的Docker配置nishanttotla&#x2F;DockerStaticSite: A simple static website using Docker and Nginx","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://galaxyyao.github.io/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://galaxyyao.github.io/tags/k8s/"}]},{"title":"Nginx-替换response header中的Content-Disposition值","slug":"Nginx-替换response header中的Content-Disposition值","date":"2019-06-16T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/06/17/Nginx-替换response header中的Content-Disposition值/","permalink":"https://galaxyyao.github.io/2019/06/17/Nginx-%E6%9B%BF%E6%8D%A2response%20header%E4%B8%AD%E7%9A%84Content-Disposition%E5%80%BC/","excerpt":"我们有个需求要在打开合同PDF的时候，要将response的header里的Content-Disposition从 1attachment;filename*=&quot;utf-8\\&#x27; \\&#x27;文件名&quot; 改为 1inline;filename*=&quot;utf-8\\&#x27; \\&#x27;文件名&quot; 这样文件就可以直接在浏览器里预览打开，而不是直接下载。理论上最好的方式自然是从应用端解决。但我们提供文件的内容管理服务器不提供这个配置选项。虽然是开源软件，但我也不想为了这个修改源代码。除此之外，为了避免影响其他和文件相关的功能，减少回归测试量，我们也不想把全局修改这个header值。那么剩下的办法就只有从Nginx反向代理层找解决方案了。理想的解决方案是对xxx.domain.com域名（内容管理服务器的域名），所有URL中带PDF关键字和“?inline&#x3D;1”参数的请求，修改header中Content-Disposition的值。（我们可以在前端请求的时候加?inline&#x3D;1这个path variable）我模糊记得Nginx可以带if条件，所以原本估计就是个小case。事实证明我估计错得离谱【捂脸】。。。如果要直接看结论的请跳转到最后一节。 教训1：Nginx“基本”不支持if里多个条件我先找到了一段匹配文件后缀的正则表达式： 1.*\\.(后缀1|后缀2)$","text":"我们有个需求要在打开合同PDF的时候，要将response的header里的Content-Disposition从 1attachment;filename*=&quot;utf-8\\&#x27; \\&#x27;文件名&quot; 改为 1inline;filename*=&quot;utf-8\\&#x27; \\&#x27;文件名&quot; 这样文件就可以直接在浏览器里预览打开，而不是直接下载。理论上最好的方式自然是从应用端解决。但我们提供文件的内容管理服务器不提供这个配置选项。虽然是开源软件，但我也不想为了这个修改源代码。除此之外，为了避免影响其他和文件相关的功能，减少回归测试量，我们也不想把全局修改这个header值。那么剩下的办法就只有从Nginx反向代理层找解决方案了。理想的解决方案是对xxx.domain.com域名（内容管理服务器的域名），所有URL中带PDF关键字和“?inline&#x3D;1”参数的请求，修改header中Content-Disposition的值。（我们可以在前端请求的时候加?inline&#x3D;1这个path variable）我模糊记得Nginx可以带if条件，所以原本估计就是个小case。事实证明我估计错得离谱【捂脸】。。。如果要直接看结论的请跳转到最后一节。 教训1：Nginx“基本”不支持if里多个条件我先找到了一段匹配文件后缀的正则表达式： 1.*\\.(后缀1|后缀2)$ 后缀替换成pdf后，就尝试写了如下的代码： 123if ($request_filename ~* &quot;.*\\.(pdf)&quot; &amp;&amp; $request_uri ~ &quot;(.*)inline=1&quot;) &#123; # 修改header值&#125; 然而很快我就发现，Nginx不支持if(condition1 &amp;&amp; condition2)的语法【捂脸】。。。其实也有一些奇淫技巧可以实现AND和OR，比如这一篇，通过拼字符串的方式： 123456789101112131415location = /test_and/ &#123; default_type text/html; set $a 0; set $b 0; if ( $remote_addr != &#x27;&#x27; )&#123; set $a 1; &#125; if ( $http_x_forwarded_for != &#x27;&#x27; )&#123; set $a 1$a; &#125; if ( $a = 11 )&#123; set $b 1; &#125; echo $b;&#125; 根据Nginx企业官网的一篇文章：If Is Evil，平时应该尽量谨慎用if。除此以外，Nginx中要实现if…else…的语法也需要费一番周折。这里就不详细展开了。 教训2：location不包含参数接下来尝试用正则表达式表现url中同时包含.pdf（不区分大小写）和“inline&#x3D;1”参数。考虑到问号可能需要转义，就用.来替代。于是写了类似如下的正则表达式： 1location ~* &quot;.*\\.(pdf).(inline=1)&quot; 但结果发现死活匹配不到inline&#x3D;1的那段。反复尝试了多种正则表达式后，才想起来location不包含URI参数。。。最终决定通过location匹配后缀，在location内用if匹配URI参数（inline&#x3D;1）： 1234567location ~* &quot;.*\\.(pdf)$&quot; &#123; # 省略其他 if ($args ~ inline=) &#123; # 替换header值逻辑 &#125; # proxy_pass逻辑&#125; 教训3：当location为正则表达式时，proxy_pass不能包含URI部分在写proxy_pass的时候，参考了“location &#x2F;”的那段逻辑，写成了： 1proxy_pass http://docsvr/; nginx -s reload的时候报错： 12[root@nginx-internal proxy]# nginx -s reloadnginx: [emerg] &quot;proxy_pass&quot; cannot have URI part in location given by regular expression, or inside named location, or inside &quot;if&quot; statement, or inside &quot;limit_except&quot; block in /etc/nginx/conf.d/proxy/doc.conf:56 查了之后才得知当location为正则表达式时，proxy_pass不能包含URI部分。在此处“&#x2F;”也是URI部分。所以去除了http://docsvr/ 最后的斜杠，调整为： 1234567location ~* &quot;.*\\.(pdf)$&quot; &#123; # 省略其他 if ($args ~ inline=) &#123; # 替换header值逻辑 &#125; proxy_pass http://docsvr;&#125; 在location后使用~*是为了让后缀忽略大小写。 教训4：proxy_set_header不能包含在if语句中接下来就是要替换Content-Disposition值了。我们先尝试将该值替换成其他任意值： 123if ($args ~ inline=) &#123; proxy_set_header &#x27;Content-Disposition&#x27; &#x27;bbb&#x27;;&#125; 然后就在nginx -s reload的时候收到了报错： 1nginx: [emerg] &quot;proxy_set_header&quot; directive is not allowed here in /etc/nginx/conf.d/proxy/doc.conf:32 从这篇How nginx “location if” works，我们可以知道Nginx实现if是通过一个嵌入的location。而不允许proxy_set_header很可能是因为嵌套的location不支持。顺带提一句，除了proxy_set_header外，proxy_hide_header也不能包含在if语句中。 看上去我们只能靠变量了。逻辑大概如下： 1234567set $is_inline_pdf 0set $content_disposition &#x27;attachment;filename*=&quot;utf-8\\&#x27; \\&#x27;attachement.pdf&quot;&#x27;;if ($args ~ inline=) &#123; set $is_inline_pdf 1; set $content_disposition &#x27;inline;filename*=&quot;utf-8\\&#x27; \\&#x27;inline.pdf&quot;&#x27;;&#125;proxy_set_header &#x27;Content-Disposition&#x27; $content_disposition; 教训5：proxy_set_header只能用来设置自定义header上面那段配置测试后发现无效。事实上，不管proxy_set_header给Content-Disposition设置什么值都无效。查询之后发现proxy_set_header可能只对自定义的header有效，但不能改非自定义的header。 改用add_header替换proxy_set_header，会因为出现两个Content-Disposition而无法正常展现。在Chrome下会显示ERR_RESPONSE_HEADERS_MULTIPLE_CONTENT_DISPOSITION的报错。 所以需要用proxy_hide_header + add_header，先隐藏后添加了。即： 12proxy_hide_header &#x27;Content-Disposition&#x27;;add_header &#x27;Content-Disposition&#x27; $content_disposition; 教训6：if语句内外的add_header不会同时生效附带发现了一个很神奇的现象：当在命中if条件时，只有if条件内的add_header语句会执行。例如在下面的这个例子中： 12345add_header &#x27;testa&#x27; &#x27;aaa&#x27;;if ($args ~ inline=) &#123; add_header &#x27;testb&#x27; &#x27;bbb&#x27;;&#125;add_header &#x27;testc&#x27; &#x27;ccc&#x27;; 按照我们其他语言中对if的理解，当符合条件($args ~ inline&#x3D;)这个条件时，应该是testa&#x2F;testb&#x2F;testc三个header都会显示。但实际上，当符合($args ~ inline&#x3D;)这个条件时，只有testb这个header会显示；而如果不符合if条件时，testa和testc这两个header会显示。原因应该也和How nginx “location if” works这篇中介绍的原理有关。 最终成果最终语法如下： 12345678910111213set $is_inline_pdf 0;if ($args ~ inline=) &#123; set $is_inline_pdf 1;&#125;proxy_hide_header &#x27;Content-Disposition&#x27;;if ($is_inline_pdf = 1) &#123; add_header &#x27;Content-Disposition&#x27; &#x27;inline;filename*=&quot;utf-8\\&#x27; \\&#x27;inline.pdf&quot;&#x27;; proxy_pass http://docsvr;&#125;add_header &#x27;Content-Disposition&#x27; &#x27;attachment;filename*=&quot;utf-8\\&#x27; \\&#x27;attachement.pdf&quot;&#x27;;proxy_pass http://docsvr; 理论上要做的更好的话，可以用$request_filename或$request_uri中的文件名来替换Content-Disposition中的文件名。但实际发现Content-Disposition中的文件名不影响浏览器中显示，也不影响下载的文件名。而且要截取$request_filename中的filename所需要写的正则表达式有点变态，于是这个问题就先搁置不做优化了。 最终的感想：Nginx对if的支持太有限了。。。应该是Nginx为了解析速度和性能所必要的代价吧。 扩展阅读在查资料的时候顺带查到一篇挺有意思的文章和一个挺有用的网站： 通过正则表达式来DDOS还挺有创意。。。一个由正则表达式引发的血案（解决版） 看到知乎上尤雨溪推荐的JS正则可视化的工具，对理解复杂正则挺有帮助。Regexper","categories":[],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://galaxyyao.github.io/tags/Nginx/"}]},{"title":"容器-6-Kubernetes实战-POC目标","slug":"容器-6-Kubernetes实战-POC目标","date":"2019-06-12T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/06/13/容器-6-Kubernetes实战-POC目标/","permalink":"https://galaxyyao.github.io/2019/06/13/%E5%AE%B9%E5%99%A8-6-Kubernetes%E5%AE%9E%E6%88%98-POC%E7%9B%AE%E6%A0%87/","excerpt":"在较为彻底地理解了Docker原理后，我曾经天真地以为再花差不多的时间就可以同样掌握k8s。直到我看到一张k8s的核心概念关系图：Docker只是其中粉红色的那一小块（Container）的一部分。。。 我也曾考虑过按照我的理解总结k8s优于Docker Swarm之处，以及为什么k8s能赢下容器编排大战。但很快发现这意味着我还需要先去熟悉Docker Swarm，才能较为准确地进行分析。这又何苦呢？我们已经知道了结论：Kubernetes是容器编排之战的最终战胜者。就算Docker Swarm有再多的优点，我们也不会采用。 我还考虑过类似Docker系列那样，先从原理开始整理。但k8s涉及的原理范围更广，从各种存储介质到OSI七层原理，先研究透再写的话估计还得花一个月。 所以我们跳过枯燥的原理介绍，先进行最有趣的实战环节，做一个包含接近完整功能的POC（Proof of Concept，概念验证）。在POC的过程中，我们再来逐渐熟悉k8s的架构设计和原理。 POC目标我们的目标是把现在基于虚拟机的部署方式和基于脚本的打包方式，尝试用Docker容器+Kubernetes实现。部署的时候还需要考虑： 服务高可用 节点扩展与收缩 安全性 与代码版本管理平台（Gitlab）和持续集成系统（Gitlab CI&#x2F;Jenkins）的整合 多版本应用维护 日志收集 监控 数据备份 …其他等等 下面是一个我们当前应用架构的精简版：","text":"在较为彻底地理解了Docker原理后，我曾经天真地以为再花差不多的时间就可以同样掌握k8s。直到我看到一张k8s的核心概念关系图：Docker只是其中粉红色的那一小块（Container）的一部分。。。 我也曾考虑过按照我的理解总结k8s优于Docker Swarm之处，以及为什么k8s能赢下容器编排大战。但很快发现这意味着我还需要先去熟悉Docker Swarm，才能较为准确地进行分析。这又何苦呢？我们已经知道了结论：Kubernetes是容器编排之战的最终战胜者。就算Docker Swarm有再多的优点，我们也不会采用。 我还考虑过类似Docker系列那样，先从原理开始整理。但k8s涉及的原理范围更广，从各种存储介质到OSI七层原理，先研究透再写的话估计还得花一个月。 所以我们跳过枯燥的原理介绍，先进行最有趣的实战环节，做一个包含接近完整功能的POC（Proof of Concept，概念验证）。在POC的过程中，我们再来逐渐熟悉k8s的架构设计和原理。 POC目标我们的目标是把现在基于虚拟机的部署方式和基于脚本的打包方式，尝试用Docker容器+Kubernetes实现。部署的时候还需要考虑： 服务高可用 节点扩展与收缩 安全性 与代码版本管理平台（Gitlab）和持续集成系统（Gitlab CI&#x2F;Jenkins）的整合 多版本应用维护 日志收集 监控 数据备份 …其他等等 下面是一个我们当前应用架构的精简版： Spring Boot（Java）应用作为后端，暴露接口地址供集群外的App调用。应用高可用部署。 Nginx上同时host了WebPack打包的静态网站。静态网站也会调用Java应用的接口。Nginx高可用部署。 Spring Boot应用也同时调用后台的MySQL数据库。MySQL数据库的物理部署架构为Master-Slave形式。Master写入，Slave读取。MySQL的数据库进行每日的全量备份到指定外部硬件存储上。 Java应用的日志汇总到Elasticsearch中存储，并可通过kibana查看。 网络上，集群外可以访问静态网站，Java应用的接口，kibana页面。可以通过工具查询MySQL中的数据。 POC相对实际部署架构的调整： 暂时省略了Node.JS后台应用。 数据库暂时省略了MongoDB，并用MySQL替换了Oracle。 暂时省略了Spring Cloud的注册中心和配置中心。 代码版本管理平台&#x2F;持续集成系统&#x2F;Maven私服等服务暂时不部署在集群内，使用外部已有的实例。 暂时省略了Redis缓存和消息队列（RabbitMQ）。 暂时省略了ETL（Kettle）。 使用上一章中搭建完成的单主k8s集群进行部署。 POC中的一个原则我们在POC及后续实际使用Kubernetes的过程中需要保持一个原则：只通过YAML修改Kubernetes对象。换句话说，我们尽量避免像以往的运维那样，直接进入虚拟机进行操作。这样的操作是无法记录，不透明的。有可能我们下一次想部署同样的环境的时候遗漏了某个步骤，导致最终部署失败。即使我们写了操作手册也不能确保文字表达不会产生歧义。而YAML定义是透明且不会产生歧义的，且可以通过版本控制追溯历史的。 这也是我为什么会相信Kubernetes是未来的趋势的原因之一：以前的运维经验像口耳相传的秘笈，而未来基于Kubernetes的运维就是使用YAML编程。零散的运维知识点通过Kubernetes被整合成了体系化的知识。配合封装了底层各种优化的公有云，培训出一个合格运维的成本会大大降低。以前有些中小公司中，只有某个老资格的运维对服务器了如指掌，即使工作态度很差也不敢换人。但Kubernetes化运维时代，只需要部署的YAML配置在，交接时间会大大缩短。（当然网络、底层硬件和存储之类的依然需要不少时间交接。另外还有无法Kubernetes化的Windows服务器）Serverless和FaaS（Function as a Serivce）技术最近也火热发展中。或许以后都不需要运维了，开发只需要直接向云Kubernetes直接提交业务函数即可。不过目前这些技术还处于探索阶段。这对运维来说不能说是一个利好消息。但不跟上这个潮流的话，就只有等着被历史的车轮碾过淘汰。 当然上述只是我理想中的情况。现实永远是一个泥潭，会逼得我们做各种dirty workaround。但只要愿景是美好的，我们终能一步步接近。 好了，我们开始吧。","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://galaxyyao.github.io/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://galaxyyao.github.io/tags/k8s/"}]},{"title":"容器-5-kubeadm部署Kubernetes1.14.2集群踩坑记","slug":"容器-5-kubeadm部署Kubernetes1-14-2集群踩坑记","date":"2019-05-28T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/05/29/容器-5-kubeadm部署Kubernetes1-14-2集群踩坑记/","permalink":"https://galaxyyao.github.io/2019/05/29/%E5%AE%B9%E5%99%A8-5-kubeadm%E9%83%A8%E7%BD%B2Kubernetes1-14-2%E9%9B%86%E7%BE%A4%E8%B8%A9%E5%9D%91%E8%AE%B0/","excerpt":"一般情况下我不喜欢把部署手册放到blog里。绝大多数情况下官网已经足够详尽，而且blog很可能因为版本陈旧误人子弟。曾经我写过Nginx的二进制部署手册，早就被轻松愉快的yum安装扫进了废纸堆。而使用了Docker和K8s后yum安装方式也被迅速淘汰。Hadoop的部署也被Cloudera全自动化部署替代了。但kubernetes的部署由于涉及科学上网的问题，把原本几个命令就能解决的问题搞得相当复杂。所以希望这篇也能多少对还在被GFW恶心的人有些帮助。（当然可能更简单的方式是部署在墙外，比如AWS上） 0. 部署目标和硬件准备0.1 部署目标由于是测试目的，就不部署高可用了。高可用的部署可以参见最后的参考资料。物理拓扑结构是1 Master + 3 Worker（Worker数量可轻松扩展）。 0.2 硬件准备部署的Kubernetes版本是v1.14.2（截止2019&#x2F;5&#x2F;29的最新版本），Docker的版本是Docker CE 18.09.6（也是截止2019&#x2F;5&#x2F;29的最新版本）。服务器全是VMWare虚拟机。虚机的硬件和操作系统如下： HOSTNAME ip ROLES 硬件配置 操作系统 docker-4 10.16.34.54 master 4核CPU&#x2F;8GB内存&#x2F;100GB硬盘 CentOS 7.4 docker-5 10.16.34.57 worker 4核CPU&#x2F;8GB内存&#x2F;100GB硬盘 CentOS 7.4 docker-6 10.16.34.58 worker 4核CPU&#x2F;8GB内存&#x2F;100GB硬盘 CentOS 7.4 docker-7 10.16.34.59 worker 4核CPU&#x2F;8GB内存&#x2F;100GB硬盘 CentOS 7.4 下面所有的命令都是在root账号下执行的。","text":"一般情况下我不喜欢把部署手册放到blog里。绝大多数情况下官网已经足够详尽，而且blog很可能因为版本陈旧误人子弟。曾经我写过Nginx的二进制部署手册，早就被轻松愉快的yum安装扫进了废纸堆。而使用了Docker和K8s后yum安装方式也被迅速淘汰。Hadoop的部署也被Cloudera全自动化部署替代了。但kubernetes的部署由于涉及科学上网的问题，把原本几个命令就能解决的问题搞得相当复杂。所以希望这篇也能多少对还在被GFW恶心的人有些帮助。（当然可能更简单的方式是部署在墙外，比如AWS上） 0. 部署目标和硬件准备0.1 部署目标由于是测试目的，就不部署高可用了。高可用的部署可以参见最后的参考资料。物理拓扑结构是1 Master + 3 Worker（Worker数量可轻松扩展）。 0.2 硬件准备部署的Kubernetes版本是v1.14.2（截止2019&#x2F;5&#x2F;29的最新版本），Docker的版本是Docker CE 18.09.6（也是截止2019&#x2F;5&#x2F;29的最新版本）。服务器全是VMWare虚拟机。虚机的硬件和操作系统如下： HOSTNAME ip ROLES 硬件配置 操作系统 docker-4 10.16.34.54 master 4核CPU&#x2F;8GB内存&#x2F;100GB硬盘 CentOS 7.4 docker-5 10.16.34.57 worker 4核CPU&#x2F;8GB内存&#x2F;100GB硬盘 CentOS 7.4 docker-6 10.16.34.58 worker 4核CPU&#x2F;8GB内存&#x2F;100GB硬盘 CentOS 7.4 docker-7 10.16.34.59 worker 4核CPU&#x2F;8GB内存&#x2F;100GB硬盘 CentOS 7.4 下面所有的命令都是在root账号下执行的。 1. 检查和配置操作系统1.1 检查操作系统&#x2F;硬件配置&#x2F;网络连通性按照安装 kubeadm - Kubernetes检查操作系统&#x2F;硬件配置&#x2F;网络连通性。主要检查节点之中不可以有重复的主机名，MAC 地址，product_uuid。 1.2 配置hostname根据官方文档的kubeadm问题排查，需要确保hostname -i命令返回可路由的ip。我拿到的虚机默认只会返回127.0.0.1。这个可能导致了后续配置过程中Worker节点在join后一直NotReady的问题。所以以防万一还是在每个节点上配置一下比较保险。 1vi /etc/hosts 添加内容： 1234510.16.34.54 docker-410.16.34.57 docker-510.16.34.58 docker-610.16.34.59 docker-7 然后重启network 1systemctl restart network 1.3 禁用swap12sudo swapoff -asudo sed -i &#x27;/ swap / s/^\\(.*\\)$/#\\1/g&#x27; /etc/fstab kubelet在swap不禁用的情况下会报错： kubelet[2856]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set –fail-swap-on K8S这么设计的原因主要是性能考量：Kubernetes会把每个node实例尽量压榨到利用率100%，包括CPU和内存。而swap出来的虚拟内存的性能远比不上真实内存，会影响调度器对机器余力的判断。 1.4 禁用selinux123# 将 SELinux 设置为 permissive 模式(将其禁用)setenforce 0sed -i &#x27;s/^SELINUX=enforcing$/SELINUX=permissive/&#x27; /etc/selinux/config 禁用SELinux是因为kubelet还不支持。不然容器访问不了宿主机的文件系统，也就没法使用Pod网络。 1.5 RHEL&#x2F;CentOS7相关iptables配置123456cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOFsysctl --system 1.6 开启端口理论上需要开这些端口：Master 节点 123456sudo firewall-cmd --zone=public --permanent --add-port=6443/tcpsudo firewall-cmd --zone=public --permanent --add-port=2379-2380/tcpsudo firewall-cmd --zone=public --permanent --add-port=10250/tcpsudo firewall-cmd --zone=public --permanent --add-port=10251/tcpsudo firewall-cmd --zone=public --permanent --add-port=10252/tcpsudo firewall-cmd --reload Worker 节点 123sudo firewall-cmd --zone=public --permanent --add-port=10250/tcpsudo firewall-cmd --zone=public --permanent --add-port=30000-32767/tcpsudo firewall-cmd --reload 不过对于测试环境来说，为了以防未知的坑，还是直接关闭掉防火墙比较直接。之后在部署Rook的时候，apply -f operator.yaml后Pod的状态一直为CrashLoopBackOff或Error。查看Event日志得到了如下的错误信息： 12345State: Waiting Reason: CrashLoopBackOffLast State: Terminated Reason: Error Message: failed to get pod. Get https://10.96.0.1:443/api/v1/namespaces/rook-ceph/pods/rook-ceph-operator-765ff54667-njkn6: dial tcp 10.96.0.1:443: connect: no route to host 通过kubectl get svc -n&#x3D;kube-system 命令查询service，发现kube-dns还需要开启53&#x2F;UDP,53&#x2F;TCP,9153&#x2F;TCP这三个端口。kubernetes-dashboard也需要443端口。在关闭防火墙后，rook-ceph部署成功。综上所述，将本步骤修改为： 12systemctl stop firewalldsystemctl disable firewalld 2. 安装容器运行时(CRI)-Docker2.1 安装Docker123456yum -y install yum-utils device-mapper-persistent-data lvm2yum -y install wgetcd /etc/yum.repos.d/wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum clean allyum -y install docker-ce 2.2 启动Docker服务123systemctl daemon-reloadsystemctl enable dockersystemctl start docker 3. 安装Kubernetes需要在每台机器上都安装以下的软件包： kubeadm: 用来初始化集群的指令 kubelet: 在集群中的每个节点上用来启动pod和container等 kubectl: 用来与集群通信的命令行工具 3.1 准备repo这里开始和科学上网有关了。要把repo地址里的packages.cloud.google.com都替换成很阿里云的域名mirrors.aliyun.com&#x2F;kubernetes。gpgcheck可以保留为1，不过这里以防万一我改为了不check。 12345678910cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum clean all 3.2 开始安装kubelet&#x2F;kubeadm&#x2F;kubectl1yum -y install kubelet kubeadm kubectl --disableexcludes=kubernetes 有些blog里提到还需要yum install kubernetes-cni。实际发现执行完上面的命令已经安装好了。大概最新版的kubeadm已经包含了kubernetes-cni。有些部署手册里依赖的是比较早版本的Kubernetes，可以在安装的时候指定版本： 123yum install kubelet=1.11.3-00yum install kubectl=1.11.3-00yum install kubeadm=1.11.3-00 3.3 启动kubelet服务12systemctl daemon-reloadsystemctl enable kubelet &amp;&amp; systemctl start kubelet 由于上一个步骤里yum安装的时候没有指定版本，这时候就可以通过kubectl version查到yum安装的Kubernetes版本。 123[root@docker-4 ~]# kubectl versionClient Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;14&quot;, GitVersion:&quot;v1.14.2&quot;, GitCommit:&quot;66049e3b21efe110454d67df4fa62b08ea79a19b&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-05-16T16:23:09Z&quot;, GoVersion:&quot;go1.12.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;The connection to the server localhost:8080 was refused - did you specify the right host or port? connection refused的报错信息可以先无视。 3.4 在Master节点上创建kubeadm init的配置文件kubeadm.yaml可以把kubernetes的YAML配置文件放在任何路径下。我这里是放到root的HOME目录&#x2F;root&#x2F;下。 1cd ~ 然后创建一份kubeadm init的配置文件kubeadm.yaml如下： 1234567891011apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationcontrollerManager: extraArgs: horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot; horizontal-pod-autoscaler-sync-period: &quot;10s&quot; node-monitor-grace-period: &quot;10s&quot;apiServer: extraArgs: runtime-config: &quot;api/all=true&quot;kubernetesVersion: &quot;stable-1.14&quot; 对于旧版本（例如1.11），apiVersion是kubeadm.k8s.io&#x2F;v1alpha1： 123456789apiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationcontrollerManagerExtraArgs: horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot; horizontal-pod-autoscaler-sync-period: &quot;10s&quot; node-monitor-grace-period: &quot;10s&quot;apiServerExtraArgs: runtime-config: &quot;api/all=true&quot;kubernetesVersion: &quot;stable-1.11&quot; 3.5 确定拉取的镜像版本如果服务器在墙外，那么就可以kubeadm init –config kubeadm.yaml，然后去泡杯茶慢慢等着了。但如果不是的话，你会看到如下的错误： 12345678910111213141516error execution phase preflight: [preflight] Some fatal errors occurred: [ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-apiserver:v1.14.2: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1 [ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-controller-manager:v1.14.2: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1 [ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-scheduler:v1.14.2: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1 [ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-proxy:v1.14.2: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1 [ERROR ImagePull]: failed to pull image k8s.gcr.io/pause:3.1: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1 [ERROR ImagePull]: failed to pull image k8s.gcr.io/etcd:3.3.10: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1 [ERROR ImagePull]: failed to pull image k8s.gcr.io/coredns:1.3.1: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` 原因就是国内连不上gcr.io。 如果你清楚知道kubeadm init使用的每个镜像的版本，那么你可以直接去按下一节的步骤去拉取镜像。但如果你不确定的话，还是先执行一遍kubeadm init命令，从错误信息里获取当前版本Kubernetes使用的各镜像的版本，以便下一节的pullimages.sh脚本中指定。 kubeadm需要的镜像包括：kube-proxy&#x2F;kube-scheduler&#x2F;kube-controller-manager&#x2F;kube-apiserver&#x2F;etcd&#x2F;coredns&#x2F;pause。对于v1.14.2，具体版本如下：kube-proxy:v1.14.2 kube-scheduler:v1.14.2 kube-controller-manager:v1.14.2 kube-apiserver:v1.14.2 etcd:3.3.10 coredns:1.3.1 pause:3.1 3.6 拉取镜像这个步骤是最麻烦的。如上一节所示，直接pull的话会失败。网上大多数文章中推荐docker hub上的一个个人的镜像站：anjia0532&#x2F;gcr.io_mirror:。但这个镜像站已经被Travis CI标记为疑似滥用，所以最新的几个版本都没有同步了。 所以现在推荐使用的是Azure中国的镜像站。就是对于从k8s.gcr.io拉取的docker pull命令，从gcr.azk8s.cn&#x2F;google-containers拉取。举个具体的例子。比如要拉取kubernetes dashboard v1.10.1，原本的命令为： 1docker pull k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 现在改为： 1docker pull gcr.azk8s.cn/google-containers/kubernetes-dashboard-amd64:v1.10.1 然后还可以打一个标记，覆盖k8s.gcr.io的同名镜像。 对于kubeadm需要的镜像，可以通过如下的脚本一次性获取 12cd ~vi pullimages.sh 添加内容： 1234567#!/bin/bashimages=(kube-proxy:v1.14.2 kube-scheduler:v1.14.2 kube-controller-manager:v1.14.2 kube-apiserver:v1.14.2 etcd:3.3.10 coredns:1.3.1 pause:3.1 )for imageName in $&#123;images[@]&#125; ; dodocker pull gcr.azk8s.cn/google-containers/$imageNamedocker tag gcr.azk8s.cn/google-containers/$imageName k8s.gcr.io/$imageNamedocker rmi gcr.azk8s.cn/google-containers/$imageNamedone （最后一句rmi的意义暂时没搞懂，为啥最后要把Azure的镜像删除掉。。。但的确能work，所以姑且按照网上的脚本来） 执行脚本： 12chmod +x pullimages.sh./pullimages.sh 不太确定的一点是要不要在所有的Worker Node上都执行pullimages.sh。如果遇到Worker Node一直是NotReady的话，可以在服务器上也执行一下。 PS. 也可以用阿里云的镜像，例如 1docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1 3.7 在Master节点上执行kubeadm init先配置停用 1vi /etc/sysconfig/kubelet 将内容修改为： 1KUBELET_EXTRA_ARGS=&quot;--fail-swap-on=false&quot; 然后就可以执行kubeadm init命令了。具体执行时间看网速，我这里大概总共3分钟。 1kubeadm init --config kubeadm.yaml 如果成功的话会显示如下内容： 12345678910111213141516Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 10.16.34.54:6443 --token hfzcd2.xhqca62fjjbmq7xh \\ --discovery-token-ca-cert-hash sha256:29a90fa653aaffd384259867c02e046a7b81a354838059f97f2053533faacbd9 然后按照提示在Master节点上执行剩下的命令： 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 保存好那条kubeadm join的命令。注意只有这个token只有当天有效。隔了24小时之后就需要kubectl create token重新创建token。 3.8 部署网络插件Weave在进行接下来的步骤之前先不要急，要先确认所有的node状态和pod状态。依次检查健康状态 1kubectl get cs 节点状态 1kubectl get nodes 如果是测试用的单节点部署，需要运行以下命令，去掉master节点的污点： 1kubectl taint nodes --all node-role.kubernetes.io/master- 系统Pod状态 1kubectl get pods -n kube-system 当前由于没有部署网络插件，所以coredns的Pod的状态还是Pending。 确保除了coredns之外的Pod都是running后，部署Weave插件： 1kubectl apply -f https://git.io/weave-kube-1.6 通过如下命令，等待Weave的Pod也正常running状态后，才能继续后续的kubeadm join操作 1kubectl get pods -n kube-system 3.9 Worker节点加入在每个Worker节点上执行1.1到3.3，以及3.6步骤后，执行join命令： 12kubeadm join 10.16.34.54:6443 --token hfzcd2.xhqca62fjjbmq7xh \\ --discovery-token-ca-cert-hash sha256:29a90fa653aaffd384259867c02e046a7b81a354838059f97f2053533faacbd9 在Master上观察各节点状态，直到全部Ready。 1kubectl get nodes 3.10 设置Worker角色通过kubeadm join加入的节点的默认角色为none，需要再标记为worker： 123kubectl label node docker-5 node-role.kubernetes.io/worker=workerkubectl label node docker-6 node-role.kubernetes.io/worker=workerkubectl label node docker-7 node-role.kubernetes.io/worker=worker 最终节点状态： 123456[root@docker-4 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONdocker-4 Ready master 24h v1.14.2docker-5 Ready worker 24h v1.14.2docker-6 Ready worker 24h v1.14.2docker-7 Ready worker 24h v1.14.2 最终系统Pod状态： 12345678910111213141516[root@docker-4 ~]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-fb8b8dccf-5h9kk 1/1 Running 0 24hcoredns-fb8b8dccf-p6kh8 1/1 Running 0 24hetcd-docker-4 1/1 Running 0 24hkube-apiserver-docker-4 1/1 Running 0 24hkube-controller-manager-docker-4 1/1 Running 0 24hkube-proxy-7xfbp 1/1 Running 0 24hkube-proxy-dw4l5 1/1 Running 0 24hkube-proxy-lhmrq 1/1 Running 0 24hkube-proxy-zmhql 1/1 Running 0 24hkube-scheduler-docker-4 1/1 Running 0 24hweave-net-g2w9p 2/2 Running 1 24hweave-net-hh6p2 2/2 Running 1 24hweave-net-qgk82 2/2 Running 0 24hweave-net-vgdnf 2/2 Running 0 24h 有些时候状态没Ready不要急，先泡杯茶去。有些操作要花一些时间的。包括之后kubectl的一些操作，拍下回车后有时候会没有UI反馈内容。如果这个时候没有耐心地Ctrl+C中止，可能产生一些不可知的后遗症。在apply多个yaml的时候，也最好在每个步骤结束后确认全部的Pod状态是Running，再进行下一个步骤。如果泡完茶依然有问题，再按照下一章的排查步骤来排查。 3.11 验证可以通过部署一个Nginx的Pod来进行验证。 12cd ~vi nginx-deployment.yaml 输入内容： 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 然后执行 1kubectl apply -f nginx-deployment.yaml 最后验证Pod状态： 1234[root@docker-4 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-deployment-5cbdcb76f7-8shbf 1/1 Running 0 17hnginx-deployment-5cbdcb76f7-g2gkt 1/1 Running 0 17h 验证完如果不需要的话可以删除： 1kubectl delete -f nginx-deployment.yaml 4. 问题排查安装的过程肯定不可能一帆风顺。知道怎么排查很重要。 4.1 排查节点问题如果怀疑是节点问题，可以通过如下的方式来查看节点状态： 1kubectl describe nodes 重点看Conditions下的Message。 4.2 排查Pod如果是系统Pod，可以通过如下命令首先查看Pod状态： 1kubectl get pods -n kube-system 然后describe节点查看 1kubectl describe pod &lt;Pod名&gt; -n kube-system 如果是普通Pod，就把命令最后的-n kube-system去掉： 1kubectl get pods 然后describe节点查看 1kubectl describe pod &lt;Pod名&gt; 重点都是看最后的Events。 4.3 其他日志有些时候Events比较简略，就需要查看日志。特别如果问题是在Worker Node上，没法执行kubectl命令，只能查看日志。查看方式有两种： 1journalctl -l -u kubelet 或者 1tail -f /var/log/messages 还可以通过grep来缩小排查范围。 4.4 重置状态在尝试解决网络插件问题的时候，我曾经病急乱投医地装了个flannel。但Pod状态始终处于ContainerCreating状态。后来Weave恢复后尝试通过kubectl delete命令删除flannel，遇到了Pod一直terminating但删除不掉的症状。雪上加霜的是还出现了硬件的告警：“kernel:NMI watchdog: BUG: soft lockup - CPU#1 stuck for 22s”。尝试reboot服务器居然发生了超时：“Failed to start reboot.target: Connection timed out”。万策已尽，只能请运维直接干掉虚机重装了。换了台机器继续装Master节点。但几个Worker Node已经join了原Master。这时候就要靠reset命令重置： 1kubeadm reset 需要注意reset完可能需要执行以下命令： 1echo 1 &gt; /proc/sys/net/ipv4/ip_forward 要不然可能会遇到以下的报错信息： 1[ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1 Master节点如果不是到我遇到的这个情况也可以reset。 5. 尚未确认的问题下面是一些我遇到过的问题。在解决过程中进行了不少操作，不太确定到底是其中具体哪个操作起到了决定性作用。所以姑且把我做过的事情都记录一下。 runtime network not ready: NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized在Worker节点加入后一直显示NotReady。查看node状态，在message里看到了如上的消息。尝试过关闭防火墙，怀疑过虚拟机网卡问题。怀疑可能有两个措施可能最终产生效果： 按照1.2配置&#x2F;etc&#x2F;hosts 在每个Worker Node上也执行pullimages.sh拉取镜像 极客时间的评论里有人因为多网卡而失败过，也摘抄一下备忘吧。 2、卡在多网卡的问题上。 2.1、我的环境是virtual box上虚拟的两个ubuntu，网络设置为nat+host only，集群搭建好之后，死活无法启动dashboard、ceph的容器（好多老外也是这么弄的啊），各种查各种试，也没解决问题。在kubernetes的官网上只说了“If you have more than one network adapter, and your Kubernetes components are not reachable on the default route, we recommend you add IP route(s) so Kubernetes cluster addresses go via the appropriate adapter.”。哪位大神按照这种方式弄好的清指点下，很是困惑啊啊啊啊，谁能解救我下……………… 2.2、放弃了2.1的nat+host only，改为了桥接的网络方式，只保留一个network interface，成功。 rpc error: code &#x3D; DeadlineExceeded异常，导致Pod持续处于ContainerCreating状态在部署完后发生过所有Node都已经Ready，但apply的Pod（包括系统插件的kubernetes dashboard和自定义的nginx）一直处于ContainerCreating状态的情况。在Worker上看到的日志中报rpc error: code &#x3D; DeadlineExceeded： 12May 29 01:51:10 docker-7 kubelet: E0529 01:51:10.545968 21276 kuberuntime_manager.go:693] createPodSandbox for pod &quot;kubernetes-dashboard-5f7b999d65-5jwpl_kube-system(a3e6ab24-81d4-11e9-935a-00505695705b)&quot; failed: rpc error: code = DeadlineExceeded desc = context deadline exceededMay 29 01:51:10 docker-7 kubelet: E0529 01:51:10.546252 21276 pod_workers.go:190] Error syncing pod a3e6ab24-81d4-11e9-935a-00505695705b (&quot;kubernetes-dashboard-5f7b999d65-5jwpl_kube-system(a3e6ab24-81d4-11e9-935a-00505695705b)&quot;), skipping: failed to &quot;CreatePodSandbox&quot; for &quot;kubernetes-dashboard-5f7b999d65-5jwpl_kube-system(a3e6ab24-81d4-11e9-935a-00505695705b)&quot; with CreatePodSandboxError: &quot;CreatePodSandbox for pod \\&quot;kubernetes-dashboard-5f7b999d65-5jwpl_kube-system(a3e6ab24-81d4-11e9-935a-00505695705b)\\&quot; failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot; 这个问题网上信息非常有限。采取了很多措施，最后也不知道哪个起效了。怀疑是又执行了一遍“1.5 RHEL&#x2F;CentOS7相关iptables配置”产生了效果。 CrashLoopBackOff状态装Minikube的时候还遇到过core-dns一直处于CrashLoopBackOff状态，也记录一下： 12345[root@docker-1 ~]# kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-59ffb8b4c-vtj5r 0/1 CrashLoopBackOff 20 78mkube-system coredns-59ffb8b4c-xj47w 0/1 CrashLoopBackOff 20 78mkube-system coredns-d5947d4b-g9hrd 0/1 CrashLoopBackOff 21 83m message如下： 1Error restarting cluster: wait: waiting for component=kube-apiserver: timed out waiting for the condition 靠停用防火墙后删除重装minikube解决了。暂时不确定是否和防火墙有关。 12345minikube stopminikube deletesystemctl disable firewalldsystemctl rebootminikube start --vm-driver=none --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers 最终感谢阿里云提供repo的镜像，微软Azure云提供Docker镜像。F*ck GFW 6. 参考资料如果是个人学习目的的话，Minikube就已经够用了，安装比上述步骤简单不少。当然科学上网的问题还是要解决。Install Minikube - Kubernetes 这篇是基于极客时间课程的搭建步骤，也是相对比较完整的。centos7快速搭建Kubernetes 1.11.1单机集群-data羊 官方文档的CRI和kubeadm安装手册。如果服务器在墙外直接照着操作就行。安装 kubeadm - KubernetesCRI installation - Kubernetes kubeadm高可用部署的官方文档。Creating Highly Available Clusters with kubeadm - Kubernetes 如果没有访问外网的话可以参考这篇。但我部署的时候真不想遇到这种情况。。。kubeadm init - 在没有互联网连接的情况下运行 kubeadm 补充一个Ansible部署K8S的开源项目（尚未试用）easzlab&#x2F;kubeasz: 使用Ansible脚本安装K8S集群，介绍组件交互原理，方便直接，不受国内网络环境影响 还有一个部署生产级别K8S的开源项目（尚未试用）kubernetes-sigs&#x2F;kubespray: Deploy a Production Ready Kubernetes Cluster","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://galaxyyao.github.io/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://galaxyyao.github.io/tags/k8s/"}]},{"title":"容器-4-Docker的意义","slug":"容器-4-Docker的意义","date":"2019-05-26T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/05/27/容器-4-Docker的意义/","permalink":"https://galaxyyao.github.io/2019/05/27/%E5%AE%B9%E5%99%A8-4-Docker%E7%9A%84%E6%84%8F%E4%B9%89/","excerpt":"在介绍完Docker的原理后，我们再回过头来看Docker的意义。 Docker的意义事实上，Cgroups是2007年就被合并到Linux内核的功能。那个时候结合了Cgroups的资源管理能力+Linux Namespace的视图隔离能力的LXC（Linux Container）就已经产生了。但LXC的视角还是操作系统和服务器，目标是打造一个相比虚拟机更轻量级的系统容器。而Docker从理念上就截然不同，将目标中心转到了应用。 不要小看这个理念上的差异。以应用为中心意味着两件事情的彻底改变： 构建和部署测试环境和生产环境的构建和部署都是以应用为粒度的。而一个操作系统上可能部署着不同测试阶段的应用。不可能那么凑巧让上面所有的应用恰好同一周期完毕，然后将整个操作系统从测试搬到生产。这个不符合正常的开发测试流程。而Docker直接将一个应用运行所需的完整环境，即整个操作系统的文件系统也打包了进去。只要这个应用的Docker镜像测试完成，就可以单独发布上生产。还可以利用现有的构建工具来辅助，例如Jenkins&#x2F;Ansible等。遇到性能瓶颈需要横向扩展时，也可以针对单应用迅速部署启动。在性能压力消除后也可以快速回收。在Docker之前，也已经有Cloud Foundry等PaaS项目开始以应用为中心。但相比做完一个镜像就可以随处运行的Docker，它们的便利性和适应性差了不少。 版本化和共享制作一个操作系统容器是一件私有的事情。你制作的操作系统容器一般只会使用在你自己的团队，顶多扩展到全公司内部。别人看不到你是具体怎么做的系统容器，不清楚你是否在里面埋了雷。借我十个胆子也不敢用。而Docker在容器镜像的制作上引入了“层（Layer）”的概念。这种基于“层”的实现借鉴了Git的思想，使容器的创建变得透明。每个人都可以审查应用容器在原始操作系统的容器上做了哪些修改。类似在Github上开源代码，当每个人和每个公司都可以参与到全世界的应用容器分发过程中时，Docker的爆发也在情理之中了。","text":"在介绍完Docker的原理后，我们再回过头来看Docker的意义。 Docker的意义事实上，Cgroups是2007年就被合并到Linux内核的功能。那个时候结合了Cgroups的资源管理能力+Linux Namespace的视图隔离能力的LXC（Linux Container）就已经产生了。但LXC的视角还是操作系统和服务器，目标是打造一个相比虚拟机更轻量级的系统容器。而Docker从理念上就截然不同，将目标中心转到了应用。 不要小看这个理念上的差异。以应用为中心意味着两件事情的彻底改变： 构建和部署测试环境和生产环境的构建和部署都是以应用为粒度的。而一个操作系统上可能部署着不同测试阶段的应用。不可能那么凑巧让上面所有的应用恰好同一周期完毕，然后将整个操作系统从测试搬到生产。这个不符合正常的开发测试流程。而Docker直接将一个应用运行所需的完整环境，即整个操作系统的文件系统也打包了进去。只要这个应用的Docker镜像测试完成，就可以单独发布上生产。还可以利用现有的构建工具来辅助，例如Jenkins&#x2F;Ansible等。遇到性能瓶颈需要横向扩展时，也可以针对单应用迅速部署启动。在性能压力消除后也可以快速回收。在Docker之前，也已经有Cloud Foundry等PaaS项目开始以应用为中心。但相比做完一个镜像就可以随处运行的Docker，它们的便利性和适应性差了不少。 版本化和共享制作一个操作系统容器是一件私有的事情。你制作的操作系统容器一般只会使用在你自己的团队，顶多扩展到全公司内部。别人看不到你是具体怎么做的系统容器，不清楚你是否在里面埋了雷。借我十个胆子也不敢用。而Docker在容器镜像的制作上引入了“层（Layer）”的概念。这种基于“层”的实现借鉴了Git的思想，使容器的创建变得透明。每个人都可以审查应用容器在原始操作系统的容器上做了哪些修改。类似在Github上开源代码，当每个人和每个公司都可以参与到全世界的应用容器分发过程中时，Docker的爆发也在情理之中了。 Docker对于开发人员的意义曾经我也觉得Docker是运维的事情。对开发来说，吃鸡蛋难道还要管鸡蛋是怎么下的么？但在现在微服务化趋势越来越明显的现在，使用Docker也会带来开发上的很大优势：之前我们开发和SIT测试环境的基础组件和微服务基本是公用的。这会带来一系列问题： A为了某个开发中的特性改动了数据库某个字段，导致其他开发和测试环境直接崩溃 A始终收不到消息队列中的消息，最后发现是B本地启动的应用把消息给消费掉了 CI&#x2F;CD被触发导致X应用自动重新部署，但A开发中的功能依赖于X应用，于是只能等自动部署完毕后才能继续开发而Docker可以使公用的中间件&#x2F;数据库&#x2F;微服务在本地按需启动。每个人独享自己的开发环境，不再受到其他开发人员和测试环境的影响。 此外Docker也进一步降低了生产部署的风险和时间。“我本地运行正常啊”这样的问题出现的概率会降低。也可以将上线的时间进一步压缩（虽然我们现在的一键部署脚本基本也可以一分钟内完成打包发布了），使开发同学能按时回家吃饭。 Docker也大大降低了尝试新技术和新软件的成本。我现在手头虚拟机资源还算相对充沛，如果想搭个jenkins，gitlab或区块链玩玩，申请几台新的虚机就行。但当初我也饱尝过没有机器可供随便玩的受限感。即使好不容易搞到一台虚机，还要研究个半天怎么安装。安装的时候谨慎再谨慎，就怕不小心装错了搞坏了操作系统，还要陪着笑麻烦运维删掉虚机重装。而现在大部分技术都提供了镜像，本地一句docker run命令，就可以直接开始体验了。玩坏了删除容器重新来一遍。完全没有任何心理负担。 即使只是对个人接个项目赚赚外快，Docker也带来不少便利。当你本地开发调试完，需要部署到客户本地。选项一是跑到客户现场，或远程到客户内网，手动装一堆环境依赖，可能还会遇到信息安全的限制。选项二是让客户自己运行pull + run两个命令，分分钟部署好。怎么选择毫无悬念。 参考资料极客时间的这篇专栏非常之推荐，讲得非常通俗易懂循序渐进，绝对值回票价。看到第10章搭建kubeadm的时候可能会卡一下，不过结合实际环境操作演练一下就可以跨过去了。深入剖析Kubernetes Docker全系列Namespace：楚门的世界Docker存储引擎Cgroups的计划经济Docker的意义","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"docker","slug":"docker","permalink":"https://galaxyyao.github.io/tags/docker/"}]},{"title":"容器-3-Cgroups的计划经济","slug":"容器-3-Cgroups的计划经济","date":"2019-05-24T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/05/25/容器-3-Cgroups的计划经济/","permalink":"https://galaxyyao.github.io/2019/05/25/%E5%AE%B9%E5%99%A8-3-Cgroups%E7%9A%84%E8%AE%A1%E5%88%92%E7%BB%8F%E6%B5%8E/","excerpt":"Cgroups-限制可用资源如果应用优化得不够好，直接把CPU或内存吃光也完全不是新鲜事。但我们肯定不能容忍容器无限制地挤占宿主机的资源，甚至把宿主机搞down掉。所以我们可以通过Linux的Cgroups（Control Groups，控制组）对某个容器可以使用的各种硬件资源设置配额。 根据wiki，Cgroups的功能包括： 资源限制（Resource Limitation）：进程组使用的内存的上限，也包括文件系统与物理内存交换用的页缓存（Page Cache） 优先级分配（Prioritization）：控制分配的CPU时间片数量及硬盘IO吞吐，实际上就相当于控制了进程运行的优先级 资源统计（Accounting）：统计资源使用量，如CPU使用时长、内存用量等，主要用于计费 进程控制（Control）：执行挂起、恢复进程组的操作 Cgroups的设置一般分为三个主要步骤： 创建cgroup 设置cgroup配额（将cpu&#x2F;内存等各种子系统添加到该cgroup中） 将进程添加为cgroup的任务 Docker容器在启动时候会动态创建Cgroup，并在容器终止的时候删除。","text":"Cgroups-限制可用资源如果应用优化得不够好，直接把CPU或内存吃光也完全不是新鲜事。但我们肯定不能容忍容器无限制地挤占宿主机的资源，甚至把宿主机搞down掉。所以我们可以通过Linux的Cgroups（Control Groups，控制组）对某个容器可以使用的各种硬件资源设置配额。 根据wiki，Cgroups的功能包括： 资源限制（Resource Limitation）：进程组使用的内存的上限，也包括文件系统与物理内存交换用的页缓存（Page Cache） 优先级分配（Prioritization）：控制分配的CPU时间片数量及硬盘IO吞吐，实际上就相当于控制了进程运行的优先级 资源统计（Accounting）：统计资源使用量，如CPU使用时长、内存用量等，主要用于计费 进程控制（Control）：执行挂起、恢复进程组的操作 Cgroups的设置一般分为三个主要步骤： 创建cgroup 设置cgroup配额（将cpu&#x2F;内存等各种子系统添加到该cgroup中） 将进程添加为cgroup的任务 Docker容器在启动时候会动态创建Cgroup，并在容器终止的时候删除。 容器的Cgroups相比虚拟机的优势主要在两方面： 资源利用率 性能损耗首先从资源利用率方面来说，一部分虚拟化技术只能静态分配资源，一台物理机上也装不了几个虚拟机。如果虚拟机闲置，分配给虚拟机的硬件资源也无法分配给其他虚拟机。也有一部分虚拟化技术可以实现一定程度上的“动态分配”。但这种“动态分配”有各种各样的缺陷，比如回收速度慢（Balloon技术），影响性能（内存压缩），或即慢又影响性能（透明页共享）。虚拟化技术也有很大的性能损耗。例如为了虚拟CPU，Hypervisor需要为每个虚拟的CPU创建一个数据结构，以模拟CPU的寄存器；为了虚拟内存，需要通过一个shadow page table，在物理内存和虚拟机内存之间增加一层虚拟的物理内存。而且操作系统本身的资源损耗是无论如何无法避免的。 相比之下，Cgroups虽然也会带来一些性能损耗。但通过一些测试可以发现，相比虚拟机接近50%的损耗，容器的性能损耗微乎其微（CPU密集场景下是5%）。此外Docker还节省了操作系统运行的资源损耗。此外，容器在启动时间上也有巨大的优势。 当然Cgroups也有自己的问题。比如高压力下容器与容器之间，以及容器和操作系统之间抢占资源的问题。有兴趣的话可以参考这篇，或者自己做个实验试一下。 Docker原理总结从以上的介绍你可以看到，其实Docker engine并没有使用什么特别深奥的原理。甚至你可以通过shell脚本自己来实现一个docker engine（事实上github上就有这么一个开源项目，使用100行bash实现了精简版的docker）。所以事实上真正Docker的架构类似下图： 常见问题建议在物理机上还是虚拟机上运行Docker？当我要开始搭容器测试环境的时候，最纠结的其实是到底该搭在虚拟机上还是物理机上。万一搞错了还要铲掉重来。按照我的经验，先到官方文档里找最佳实践，但翻了半天也没找到。在了解了原理后，自然会得出结论：物理机上部署Docker的性能约等于直接在物理机上部署应用。所以物理机上部署肯定有性能和延迟优势。不过Docker的博客上也提到了虚拟机化的几个好处： 方便上云 可以利用成熟且已有的虚拟化经验，例如：灾备，监控和自动化 节省虚拟机License 国内也有人测试过具体的性能差异，IO和CPU方面物理机快25%-30%。运行Docker：物理机vs虚拟机，五方面详细对比理 在我们的场景下，性能远不及其他几个优点重要。于是最终还是选择了虚拟机上部署。 容器的操作系统是否能和宿主机不一样？可以分成三种情况讨论： 容器的操作系统版本比宿主机高或低 操作系统是不同发行版本的（例如宿主机是CentOS，容器是Ubuntu） 操作系统完全不同（例如宿主机是Windows，容器是CentOS） 其实1和2都差不多。不同版本和发行版本的Linux内核的差别不那么大。容器只与主机共享一个内核。操作系统&#x3D;内核+文件系统&#x2F;库镜像&#x3D;文件系统&#x2F;库这也是为什么不能在Linux宿主机上运行Windows容器。内核根本就不一样。 但为什么我们可以在Windows 10上运行容器？这多亏了Hyper-V。（回想起来Hyper-V最初发布的时候还是在微软内部看到消息并试用的。。。）Hyper-V的技术细节就不多提了。与Docker相关的可以参见这张架构图：与之原理类似，MacOS上运行Docker是通过虚拟化技术xhyve或者virtualbox来实现。 更多可以参见这篇：Understanding Docker “Container Host” vs. “Container OS” for Linux and Windows Containers Docker全系列Namespace：楚门的世界Docker存储引擎Cgroups的计划经济Docker的意义","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"docker","slug":"docker","permalink":"https://galaxyyao.github.io/tags/docker/"}]},{"title":"容器-2-Docker存储引擎","slug":"容器-2-Docker存储引擎","date":"2019-05-23T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/05/24/容器-2-Docker存储引擎/","permalink":"https://galaxyyao.github.io/2019/05/24/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/","excerpt":"容器与容器间的文件系统必须相互隔离。如果一个容器能不受限制地访问到宿主机或另一个容器里的文件，那必定会引起严重的安全风险。所以对于docker中的进程，必须限制其能够访问的文件系统。我们先来看一种简易版的实现方式：chroot。 chroot-监狱chroot，即change root的缩写。它是一个 UNIX 操作系统上的系统调用，用于将一个进程及其子进程的根目录改变到文件系统中的一个新位置。我们知道root根目录（&#x2F;）是Linux的顶层目录。这里的顶层，换句话说就是没有办法访问比根目录更高一层级的目录。但对每个进程，可以通过chroot来“欺骗”，将指定的目录骗他们认定为根目录。 chroot经常和一个单词结合在一起说：jail（监狱）。可以很形象地理解为chroot就是给每个docker容器划了一个监狱房间。Docker在每个监狱里配套放置了一套文件系统。 chroot的基本语法如下： 12# 将某个进程chroot /path/to/new/root command chroot看起来挺不错，但也存在两个问题：","text":"容器与容器间的文件系统必须相互隔离。如果一个容器能不受限制地访问到宿主机或另一个容器里的文件，那必定会引起严重的安全风险。所以对于docker中的进程，必须限制其能够访问的文件系统。我们先来看一种简易版的实现方式：chroot。 chroot-监狱chroot，即change root的缩写。它是一个 UNIX 操作系统上的系统调用，用于将一个进程及其子进程的根目录改变到文件系统中的一个新位置。我们知道root根目录（&#x2F;）是Linux的顶层目录。这里的顶层，换句话说就是没有办法访问比根目录更高一层级的目录。但对每个进程，可以通过chroot来“欺骗”，将指定的目录骗他们认定为根目录。 chroot经常和一个单词结合在一起说：jail（监狱）。可以很形象地理解为chroot就是给每个docker容器划了一个监狱房间。Docker在每个监狱里配套放置了一套文件系统。 chroot的基本语法如下： 12# 将某个进程chroot /path/to/new/root command chroot看起来挺不错，但也存在两个问题： 监狱里需要备齐所有需要的文件，有几个容器就需要备几份。 有办法可以越狱。方式之一就是在chroot里运行chroot。事实上Docker存储引擎之一的VFS就是每个容器的存储完全独立（有时会在排查问题的时候使用），所以空间占用最大。但我们总希望能对空间利用能进一步优化。 Docker镜像原理写时复制Linux刚启动的时候会加载bootfs。当boot成功，kernel被加载到内存之后，bootfs就被umount了。我们平时能看到的&#x2F;bin，&#x2F;lib等目录是rootfs，处于bootfs上一层。 假设我们有两个centos容器。对于这两个容器来说，像&#x2F;bin，&#x2F;lib等rootfs目录里的内容完全一致，同时保存两份会浪费存储空间。那么我们是否可以让两个容器共享这些相同的文件？我们知道Linux里有软链接的概念。可以在不同的目录里创建软链接，指向同一个实际文件&#x2F;目录。但软链接方案有一个很显而易见的问题：改动会互相影响。我们无法接受在一个容器里修改了文件之后影响到另一个容器。很自然，我们就会考虑做一个只读的基准版本。每个容器对这个只读版本的修改分别独立保存。这个只读的基准版本就是镜像，可修改的就是容器。 Docker存储引擎对文件的修改使用到了写时复制（COW，Copy on Write）技术。即当要对镜像中某个文件进行写操作时，将文件复制到文件系统中，对副本进行修改，而不会对image里的源文件进行修改。多个容器操作同一个文件的时候会创建多个副本。 写时复制使得同一台机器上可以部署的Docker容器数量远超过可以部署的虚拟机数量：假设一个虚拟机的大小是10GB，那么创建并启动10个虚拟机需要多少空间？视使用的虚拟技术而定，可能是100GB，可能小于100GB。但假设操作系统占了2GB，容量不会小于20GB。假设一个容器镜像的大小是10GB，那么创建并启动10个容器需要多少空间？依然只需要10GB。 Union Mount假设原始镜像中有一个文件a.txt。当要修改该文件的内容时，通过“写时复制”，创建出来了一份a.txt的副本。我们希望最终在容器中查看文件系统时，除了a.txt之外的所有文件都读取镜像，只有a.txt这个文件读取副本。这依赖的是Union Mount（联合挂载）技术。我们已Docker存储引擎之一：OverlayFS为例，看一下联合挂载的特征。 如上图所示，在OverlayFS中存在Lower和Upper两个层次。Lower层就是镜像层，Upper层就是容器层。最终的效果类似从正上方俯视，Upper中的文件会覆盖Lower层中的文件。我们以下四种情况，看一下不同的文件系统操作的实际原理： 创建文件&#x2F;目录：在Upper层中直接创建 修改文件&#x2F;目录：从Lower层复制到Upper层，然后在Upper层中修改 删除文件&#x2F;目录：在Upper层创建一个Whiteout文件&#x2F;目录。Whiteout文件&#x2F;目录在Merge后不显示。 删除目录后创建同名目录：Upper层新目录为opaque目录，屏蔽Lower层目录和里面的文件 Layer假设我们现在有一批基于centos镜像的容器，这些容器里都需要装JDK。当然我们可以在每个容器的Upper层里都包含一份JDK文件。但既然每个容器里的JDK文件都一样，那么是不是可以考虑把JDK也做成共通的？ 有一种方案是把centos和JDK放在一起打包成一个独立的镜像。但接下来你就会面对组合的极速膨胀，比如： centos + JDK centos + python 2 centos + python 3 centos + JDK + tomcat centos + apache …光是镜像的大小就会变得很可观。 我们上一节提到过overlay层 &#x3D; 容器层 + 镜像层，那么自然可以联想到，这个模式也可以扩展成：overlay层 &#x3D; 容器层 + JDK镜像层 + centos镜像层推而广之： centos + JDK + tomcat &#x3D; 容器层 + tomcat镜像层 + JDK镜像层 + centos镜像层 centos + python 3 &#x3D; 容器层 + python 3层 + centos镜像层 …这样就只需要维护一份centos的镜像文件，以及基于centos镜像的增量改动即可。 容器层的改动也可以通过docker commit固化成镜像。于是只要把应用打包成镜像，分发到任何服务器上docker run，就达到了“一次打包，到处运行”的效果。 我认为这个设计还有一个顺带的好处：下载镜像的时候可以并发下载多个层，加快镜像的下载速度。每一层下载完后自行解压缩。下面是下载Gitlab镜像的实时状态： 1234567891011latest: Pulling from gitlab/gitlab-ce9ff7e2e5f967: Downloading [==================================&gt; ] 30.1MB/43.77MB59856638ac9f: Download complete 6f317d6d954b: Download complete a9dde5e2a643: Download complete 23e292690057: Downloading [==========&gt; ] 5.717MB/26.26MB49ec625ca43f: Download complete 15260c60bf0e: Download complete b62bd915894c: Waiting 8dc41372b526: Waiting 8d1e09653c32: Waiting Docker存储引擎的选择Docker的存储引擎有多种实现方式，并还在不断改进中。除了上文中提到的VFS和OverlayFS外，还有Device Mapper&#x2F;AUFS&#x2F;Overlay2等。当前最新版本推荐使用Overlay2。对于Overlay2引擎，镜像保存在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2路径下，镜像和层的元数据保存在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;image&#x2F;overlay2&#x2F;路径下。 参考资料一个便于深入理解chroot效果的动手实验技术|Linux &#x2F; Unix：chroot 命令实例讲解 容器Layer相关的图来自于这个博客Docker Getting Start: Related Knowledge OverlayFS的一些动手试验OverlayFS | Programster’s Blog Docker引擎原理的官方文档介绍About storage drivers | Docker Documentation 对Overlay&#x2F;Overlay2具体实现的分析Docker存储驱动—Overlay&#x2F;Overlay2「译」 | Arking Docker全系列Namespace：楚门的世界Docker存储引擎Cgroups的计划经济Docker的意义","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"docker","slug":"docker","permalink":"https://galaxyyao.github.io/tags/docker/"}]},{"title":"crontab ntpdate同步失败与Linux环境变量","slug":"crontab-ntpdate同步失败与Linux环境变量","date":"2019-05-22T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/05/23/crontab-ntpdate同步失败与Linux环境变量/","permalink":"https://galaxyyao.github.io/2019/05/23/crontab-ntpdate%E5%90%8C%E6%AD%A5%E5%A4%B1%E8%B4%A5%E4%B8%8ELinux%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/","excerpt":"问题症状与解决方法Oracle服务器由于无法上外网，所以做了个crontab的定时任务，每天定时和内部的一台ntp服务器同步。但没过两周，时间又不准了，差了近10秒。首先排除了这个时间差是在凌晨同步完后的几小时内造成的。以“ntpdate crontab”作为关键字搜索，很容易找到了原因：坑爹的crontab重置了PATH环境变量，所以执行ntpdate命令的时候报“command not found”。解决方法也很简单：通过whereis ntpdate的命令查出ntpdate的位置，改为完整路径调用，即“&#x2F;usr&#x2F;sbin&#x2F;ntpdate”。另外作为以防万一，也将同步间隔从1天缩小到1小时。 为了避免下次栽坑，我们需要知道crontab到底设置了哪些PATH环境变量。通过“vi &#x2F;etc&#x2F;crontab”打开文件，可以看到如下的内容： 1234567891011121314SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=root# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed 可以看到PATH里明明是包含&#x2F;usr&#x2F;sbin，也就是ntpdate的路径。那么为什么依然会报“&#x2F;bin&#x2F;sh: ntpdate command not found”的错误？（另外一个疑点是看起来是用&#x2F;bin&#x2F;bash执行的，为什么报&#x2F;bin&#x2F;sh） crontab ≠ crontab这里是比较容易混淆的地方： crontab -e是用户任务调度的命令 &#x2F;etc&#x2F;crontab是系统任务调度的配置文件","text":"问题症状与解决方法Oracle服务器由于无法上外网，所以做了个crontab的定时任务，每天定时和内部的一台ntp服务器同步。但没过两周，时间又不准了，差了近10秒。首先排除了这个时间差是在凌晨同步完后的几小时内造成的。以“ntpdate crontab”作为关键字搜索，很容易找到了原因：坑爹的crontab重置了PATH环境变量，所以执行ntpdate命令的时候报“command not found”。解决方法也很简单：通过whereis ntpdate的命令查出ntpdate的位置，改为完整路径调用，即“&#x2F;usr&#x2F;sbin&#x2F;ntpdate”。另外作为以防万一，也将同步间隔从1天缩小到1小时。 为了避免下次栽坑，我们需要知道crontab到底设置了哪些PATH环境变量。通过“vi &#x2F;etc&#x2F;crontab”打开文件，可以看到如下的内容： 1234567891011121314SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=root# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed 可以看到PATH里明明是包含&#x2F;usr&#x2F;sbin，也就是ntpdate的路径。那么为什么依然会报“&#x2F;bin&#x2F;sh: ntpdate command not found”的错误？（另外一个疑点是看起来是用&#x2F;bin&#x2F;bash执行的，为什么报&#x2F;bin&#x2F;sh） crontab ≠ crontab这里是比较容易混淆的地方： crontab -e是用户任务调度的命令 &#x2F;etc&#x2F;crontab是系统任务调度的配置文件 用户任务调度每个用户可以通过crontab -e创建自己的定时任务调度。创建的任务会放到&#x2F;var&#x2F;spool&#x2F;cron&#x2F;{用户名}的文件里。系统在启动的时候会由&#x2F;etc&#x2F;init.d启动crond守护进程（参考资料: daemon to execute scheduled commands - Linux man pagehttps://linux.die.net/man/8/crond)）。crond会将/var/spool/cron目录下的crontab文件载入内存，并在每分钟检查是否有需要执行的任务。（我怀疑就是这个机制导致Linux的定时任务的最小间隔是分钟而不是秒。crond的任务如果执行完没关闭的话，会残留下来很多crond进程。如果任务写得有问题的话可能会把进程数都吃光，导致无法ssh登录。）输出会通过系统内邮件发给对应的用户。 我们可以通过执行一个“echo $PATH”的定时任务，查看cron的PATH环境变量。结果如下： 1PATH=/usr/bin:/bin 没有ntpdate所在的&#x2F;usr&#x2F;sbin目录。这就能解释“command not found”这个报错的原因了。 系统任务调度&#x2F;etc&#x2F;crontab文件也是由crond守护进程扫描并调用的。但差别在于3点： &#x2F;etc&#x2F;crontab文件中额外增加了&#x2F;sbin和&#x2F;usr&#x2F;sbin两个目录 通过&#x2F;bin&#x2F;bash执行 可以执行的用户我理解&#x2F;etc&#x2F;crontab文件的意义在于： &#x2F;etc&#x2F;passwd中某些用户是不允许登录的系统账号（例如mysql用户）。虽然也可以通过crontab -u配置，但总不如汇总在一个文件里查看起来方便 方便添加公共环境变量。通过crontab -e执行需要制定环境变量的命令的时候，需要在命令前先添加一个export。而这些环境变量可以直接写在&#x2F;etc&#x2F;crontab文件的顶部 想起来之前sudo的时候也在环境变量的时候栽过坑，所以也总结一下环境变量相关知识。 环境变量层级对于每个进程来说，环境变量保存在&#x2F;proc&#x2F;$PID&#x2F;environ这个文件里（$PID是进程号）。每个环境变量的键值对之间是通过\\x0这个字符分割的，所以可以通过如下的命令打印当前进程用到的环境变量： 1sed &#x27;s:\\x0:\\n:g&#x27; /proc/$PID/environ 这些环境变量是由多个层级的环境变量值拼成的。 环境变量分为几个层级： 全局 用户级（Per User） 会话级（Per Session） 全局全局的环境变量主要在两个文件里： &#x2F;etc&#x2F;environment：推荐加在这个文件里 &#x2F;etc&#x2F;profile：只针对登录的shell有效此外，bash命令也会自带一些环境变量。&#x2F;etc&#x2F;locale.conf文件里也带有一个LANG&#x3D;”en_US.UTF-8”的环境变量。 之前遇到过一些登录SSH可以成功执行的命令，通过gitlab ci无法执行。原因主要也是因为依赖于一些只在&#x2F;etc&#x2F;profile中出现的变量。 用户级某个变量可能只有某个用户才需要。这种时候就需要用户级别的。用户级的变量保存在&#x2F;.bashrc和&#x2F;.bash_profile等文件中（表示用户的home目录）。另外也可以看到&#x2F;.bash_profile其实就是调用&#x2F;.bashrc。例如要在用户的PATH里添加一个目录&#x2F;home&#x2F;my_user&#x2F;bin，可以修改&#x2F;.bash_profile文件如下： 1export PATH=&quot;$&#123;PATH&#125;:/home/my_user/bin&quot; 然后通过source ~&#x2F;.bash_profile命令更新变量。 会话级有些时候可能只是想在某次登录会话期间，让环境变量临时生效。这时候就靠export命令： 1export PATH=&quot;$&#123;PATH&#125;:/home/my_user/tmp/usr/bin&quot; 像pwd命令读取的就是用户当前会话中所在路径。 sudosudo需要专门拎出来说，是由于之前遇到过一个坑：在一次pip安装lib的时候发生过，有一个命令在root下能成功执行的命令，普通用户sudo执行却会失败。 最终发现的原因是sudo下的PATH环境变量被重置成一个最小化的子集了。可以用文本编辑器打开&#x2F;etc&#x2F;sudoers文件，找到”secure_path”那一行： 1Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin 而那个命令是在&#x2F;usr&#x2F;local&#x2F;bin（忘了还是&#x2F;usr&#x2F;local&#x2F;sbin了）下，自然就执行失败了。更多可以参考这篇：技术|Linux有问必答：如何为sudo命令定义PATH环境变量。但我不推荐文中修改secure_path的做法。sudo的secure_path这么设置自然有其安全上的考量。改为完整路径调用或通过export临时添加环境变量即可。 参考资料这篇讲环境变量的比较全。虽然是ArchLinux的Wiki，但对于其他Linux的发行版也基本通用。Environment variables - ArchWiki 为什么虚拟机的时钟会产生偏差，这篇文章从原理上解释了原因。（话说Docker就不存在这个问题，容器里想改时间也不能改，要错一起错）奔跑在虚拟化大路上的你 请看一看路边的荆棘 - Netis","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://galaxyyao.github.io/tags/Linux/"}]},{"title":"容器-1-Namespace：楚门的世界","slug":"容器-1-Namespace：楚门的世界","date":"2019-05-16T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/05/17/容器-1-Namespace：楚门的世界/","permalink":"https://galaxyyao.github.io/2019/05/17/%E5%AE%B9%E5%99%A8-1-Namespace%EF%BC%9A%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C/","excerpt":"本文是一系列对Docker与Kubernetes的学习总结。源材料来源参见文中的链接和最后的参考资料。 容器的原理虽然有些人会把容器和虚拟机类比，称之为“轻量级的虚拟机”。刚开始接触Docker的时候大多看过下面这张图：忘记这张图吧。从上一节我们讨论的“容器的意义”就可以看到，容器和虚拟机关注的不是一个层面。 但虚拟机和容器也有共通之处：本质都是欺骗。虚拟机的原理是欺骗CPU&#x2F;物理内存&#x2F;物理IO，让硬件感觉自己还是在接收宿主机而非虚拟机的指令。而Docker的原理是欺骗一组进程，让这些进程以为自己活在另一个的操作系统里。用专业一点的术语，就是：容器的核心功能，就是通过约束进程和修改进程的动态表现，为进程创造出一组“边界”。 以电影作为比方，虚拟机就像《火星救援》里在火星上种土豆的宇航员，在火星上模拟地球的环境，连空气和水都需要自己从头开始制备，成本高昂。 而容器就是《楚门的世界》。男主（容器进程）生活在一个大型影棚里，所有能接触到的世界只有这个影棚。男主住的房子，吃的东西，呼吸的空气来自于地球，生产成本很低。但他所能阅读的报纸，乘坐的载具，观看的电视都是影棚工作人员提供给他的，受到了严格的限制。 对于Docker来说，修改进程的动态表现是通过Namespace，构成进程依赖的文件系统是通过union mount，约束进程是通过Cgroups。 Namespace-欺骗的6种手段","text":"本文是一系列对Docker与Kubernetes的学习总结。源材料来源参见文中的链接和最后的参考资料。 容器的原理虽然有些人会把容器和虚拟机类比，称之为“轻量级的虚拟机”。刚开始接触Docker的时候大多看过下面这张图：忘记这张图吧。从上一节我们讨论的“容器的意义”就可以看到，容器和虚拟机关注的不是一个层面。 但虚拟机和容器也有共通之处：本质都是欺骗。虚拟机的原理是欺骗CPU&#x2F;物理内存&#x2F;物理IO，让硬件感觉自己还是在接收宿主机而非虚拟机的指令。而Docker的原理是欺骗一组进程，让这些进程以为自己活在另一个的操作系统里。用专业一点的术语，就是：容器的核心功能，就是通过约束进程和修改进程的动态表现，为进程创造出一组“边界”。 以电影作为比方，虚拟机就像《火星救援》里在火星上种土豆的宇航员，在火星上模拟地球的环境，连空气和水都需要自己从头开始制备，成本高昂。 而容器就是《楚门的世界》。男主（容器进程）生活在一个大型影棚里，所有能接触到的世界只有这个影棚。男主住的房子，吃的东西，呼吸的空气来自于地球，生产成本很低。但他所能阅读的报纸，乘坐的载具，观看的电视都是影棚工作人员提供给他的，受到了严格的限制。 对于Docker来说，修改进程的动态表现是通过Namespace，构成进程依赖的文件系统是通过union mount，约束进程是通过Cgroups。 Namespace-欺骗的6种手段我们先想一下，对于一台宿主机上的虚拟机，它能看到和接触到的哪些信息必须和宿主机不一样？除了上一节提到的文件系统之外还能列出不少吧。虚拟化技术是通过Hypervisor + Guest OS实现的。而对于容器，要实现同样的效果是通过Namespace。 进程号不一样对于Linux系统来说，有几个进程是ID固定的： idle进程：pid&#x3D;0，系统创建的第一个进程，内核态 init进程：pid&#x3D;1，由0进程创建，用户态，系统中所有其它用户进程的祖先进程 kthreadd进程：pid&#x3D;2，管理和调度其他内核线程 《道德经》有云：道生一，一生二，二生三，三生万物。对于Linux进程要改一下，0生1和2，1和2生万物。 肯定不能让Docker容器接触到init进程（pid&#x3D;1），不然容器就能为所欲为了。但对于Docker里其他进程来说，如果自己不是由pid&#x3D;1的进程创建的，欺骗就出现了严重的漏洞。一山不容二虎，进程号不可重复。不可能创建出两个PID&#x3D;1的init进程。所以需要将一个fork出来的普通进程伪装成PID1的init进程，并骗容器里的其他进程相信这点。 通过PID NameSpace可以实现进程号唯一和进程视图隔离 下面是实际一个docker容器启动时，在容器里打印出来的进程 123PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 11820 1892 1512 S 0.0 0.0 0:00.15 bash 22 root 20 0 56212 2060 1452 R 0.0 0.0 0:00.01 top 而在宿主机用pstree打印出来的进程树如下： 123systemd─┬─containerd─┬─containerd-shim─┬─bash───top │ │ └─10*[&#123;containerd-shim&#125;] │ └─14*[&#123;containerd&#125;] 可以看到对于容器来说，bash是PID&#x3D;1的init进程。在这棵进程树上，containerd-shim可以理解为容器的pid&#x3D;0进程（当然实际上依然是用户态进程所以还是有很大差别）。至于为什么宿主机的init进程名是systemd，涉及sysvint和systemd的争议，在这里就不提了。有兴趣的话可以参考这篇。在这里你可以认为systemd是当前版本CentOS上的init进程实现。 主机名不一样每个容器最好有网络的独立性。这个包括主机名唯一，以及ip和端口不冲突等。先说主机名。 每个Docker容器的主机名等同于容器ID，用这种方式确保唯一。（同一个局域网上hostname重复其实也没大关系，但能做到唯一总更好一些吧） 12[root@269111b56ccd /]# hostname269111b56ccd UTS NameSpace可以实现主机名唯一 ip和端口不冲突每块网卡一个ip。每个容器有一个自己的ip，那么就要靠虚拟网卡veth。具体的架构可以参见下图： 所有容器的虚拟机网卡通过bridge桥接到宿主机的网卡上。我们可以在宿主机上打印出网络接口信息。其中的docker0就是桥接网卡，而veth开头的就是容器的虚拟网卡。 12345678910[root@mobilesit network-scripts]$ip li1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00:50:56:95:9d:68 brd ff:ff:ff:ff:ff:ff...6: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:c5:76:e1:b5 brd ff:ff:ff:ff:ff:ff8: vethf362a04@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether 5e:c3:81:ae:96:9f brd ff:ff:ff:ff:ff:ff link-netnsid 0 在容器里打印网络接口信息，除了lo这个本地环回接口（localhost）之外，就是虚拟网卡eth0了。 12345[root@269111b56ccd /]# ip li1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:007: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 有了虚拟网卡，端口映射也是小事一碟了。Network NameSpace可以实现独立虚拟网卡 容器间不可随意通信IPC是Linux下进程间通信的一种方式。两个独立容器间的进程应该是被隔离的，传小纸条这种行为需要被严格禁止。微服务间通信是靠HTTP请求和RPC，而进程间通信靠的是共享内存、信号量、消息队列等。 IPC NameSpace可以阻隔容器间通信 不同容器的同名用户互相独立我们期望在容器中可以任意创建用户，但不至于影响到宿主机。也期望在两个容器里建立同名用户的时候不互相影响。 在说到Docker的实现方式之前，先要提一下UID(User Identifier，用户id)和GID(Group Identifier，组id)。它们分别是用户和组在全系统里的唯一标识。整个Linux系统共用一套内核，内核只维护一套uid和gid。但朱丽叶也说过：“What’s in a name? That which we call a rose by ant other word would smell as sweet.”。同一个uid可以在不同的容器和宿主机之间显示不同的名字。我们可以做个实现：在Dockerfile里加上USER参数，以Daemon启动容器。然后分别在宿主机和容器查看进程。宿主机的结果是： 1234[root@mobilesit 12687]$ps -ef|grep sleeppolkitd 14126 14109 0 12:10 ? 00:00:00 sleep infinityroot 14680 786 0 12:39 ? 00:00:00 sleep 60root 14720 14234 0 12:39 pts/3 00:00:00 grep --color=auto sleep 而容器里的结果是： 1234UID PID PPID C STIME TTY TIME CMDtestuser 1 0 0 04:10 ? 00:00:00 sleep infinitytestuser 14 0 0 04:39 pts/0 00:00:00 /bin/bashtestuser 19 14 0 04:40 pts/0 00:00:00 ps -ef 一个是polkitd（Linux控制全局权限的Daemon进程），一个是testuser，看上去名字不相同。但我们再查询一下uid信息看看。宿主机的结果： 1uid=999(polkitd) gid=998(polkitd) groups=998(polkitd) 而容器里的结果为： 1uid=999(testuser) gid=998(testuser) groups=998(testuser) uid和gid完全相同，可见从本质上是同一个用户。 PS. 实验的时候还有三个未解的疑问，待以后有空再深入研究了。 如果用useradd在容器里创建用户，容器里能查到uid为1000的用户（如果继续建立，就会从1001开始累加）。但宿主机上的&#x2F;etc&#x2F;passwd上查不到uid&#x3D;1000的用户。不知道是不是对&#x2F;etc&#x2F;passwd隐藏了。 资料上说进程里通过&#x2F;proc&#x2F;&#x2F;uid_map做宿主机和容器间的uid映射。但我从dockerd到containerd-shim，各种进程的uid_map文件都找过了。只看到了root用户的映射，没找到testuser的映射。 在两个容器里分别建立了两个用户（无论是否同名），uid都是1000。但把其中一个容器中的用户删除后，另外一个容器里的用户不受影响。我理解删除的只是映射，但不确定的是如果给用户提权，不知道会不会两个容器都受到影响。 User NameSpace可以实现容器和宿主机之间的用户映射 挂载文件系统隔离每个容器的文件挂载之间应该是互相独立的。当某个容器里执行了挂载，我们期望其他容器不会也看到这个挂载点。 容器的实现方式是在每个进程中独立维护挂载信息。实际是维护在&#x2F;proc&#x2F;&#x2F;mounts，&#x2F;proc&#x2F;&#x2F;mountinfo和&#x2F;proc&#x2F;&#x2F;mountstats这三个文件中。 Mount NameSpace可以隔离挂载信息 我们会在下一章更详细地介绍Mount Namespace是怎么和Docker的存储引擎配合，创造出每个容器内独立的文件系统。 总结Docker里分别使用6种Namespace实现了各种资源的隔离，包括： Namespace 系统调用参数 隔离的资源 PID CLONE_NEWPID 进程号 UTS CLONE_NEWUTS 主机名与域名 Network CLONE_NEWNET 网络设备、网络栈、端口等等 IPC CLONE_NEWIPC 信号量、消息队列和共享内存 User CLONE_NEWUSER 用户和用户组 Mount CLONE_NEWNS 挂载点（文件系统） 对于容器的隔离性其实有不少需要考虑的。随便举个例子，比如系统时间和时区。如果在容器里改了系统时间，是不是宿主机也会受到影响？这些细节就待有兴趣或有需求的时候再来研究了。 另外在看了一部分Kubernetes后回来补充：容器之间的Namespace隔离也不是定死的。Kubernetes Pod内部的容器之间就可以共享Network Namespace，PID Namespace和IPC Namespace等。 参考资料docker进程号的动手实验谁是Docker容器的init(1)进程 | shareinto 更详细了解Docker Network Namespace的实现可以参考下文Docker 原理篇（七）Docker network namespace | 伤神的博客 Docker全系列Namespace：楚门的世界Docker存储引擎Cgroups的计划经济Docker的意义","categories":[],"tags":[{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"docker","slug":"docker","permalink":"https://galaxyyao.github.io/tags/docker/"}]},{"title":"重复的Sequence序列和MyBatis缓存","slug":"Java-重复的Sequence序列和MyBatis缓存","date":"2019-05-12T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/05/13/Java-重复的Sequence序列和MyBatis缓存/","permalink":"https://galaxyyao.github.io/2019/05/13/Java-%E9%87%8D%E5%A4%8D%E7%9A%84Sequence%E5%BA%8F%E5%88%97%E5%92%8CMyBatis%E7%BC%93%E5%AD%98/","excerpt":"1. 问题起因Java应用的某个功能里有个循环，每个循环中调用MyBatis的SQL来获取Oracle的序列Sequence，然后把序列值填充到实体中，调用jpa的save方法将实体保存到数据库。取序列号的sql没啥特殊的： 1select seq_name.nextval from dual 但实际保存到数据库的时候，发现所有循环保存的实体的序列值都相同。 2. 问题分析首先排除了Oracle数据库的问题。从这篇stackoverflow的回答，可以看到Oracle的序列实现是考虑很周到的。并发&#x2F;Oracle部署方式&#x2F;回滚都不会使序列重复。那么疑点就落在了MyBatis上。 以MyBatis为关键字，从网上能找到一些解答，例如如下这篇回答： 12345（该问题的原因）是因为其每次都会去取一级缓存中的值。1.拿出@Transactional，就不会出错。2.加上useCache=&quot;false&quot; flushCache=&quot;true&quot;,不保存在二级缓存中，并清空缓存3.mybatis.configuration.localCacheScope=STATEMENT,修改一级缓存的作用域4.mybatis.configuration.cacheEnabled = false,禁用一级和二级缓存","text":"1. 问题起因Java应用的某个功能里有个循环，每个循环中调用MyBatis的SQL来获取Oracle的序列Sequence，然后把序列值填充到实体中，调用jpa的save方法将实体保存到数据库。取序列号的sql没啥特殊的： 1select seq_name.nextval from dual 但实际保存到数据库的时候，发现所有循环保存的实体的序列值都相同。 2. 问题分析首先排除了Oracle数据库的问题。从这篇stackoverflow的回答，可以看到Oracle的序列实现是考虑很周到的。并发&#x2F;Oracle部署方式&#x2F;回滚都不会使序列重复。那么疑点就落在了MyBatis上。 以MyBatis为关键字，从网上能找到一些解答，例如如下这篇回答： 12345（该问题的原因）是因为其每次都会去取一级缓存中的值。1.拿出@Transactional，就不会出错。2.加上useCache=&quot;false&quot; flushCache=&quot;true&quot;,不保存在二级缓存中，并清空缓存3.mybatis.configuration.localCacheScope=STATEMENT,修改一级缓存的作用域4.mybatis.configuration.cacheEnabled = false,禁用一级和二级缓存 原因找到的没错：的确是MyBatis的一级缓存导致的。但你可能会疑惑：这4点都要做么？还是只做其中1-2点就有用了？从这篇解答的表述和后面的分析很容易就可以看到，作者对Mybatis的理解还是一团浆糊，并没有真正搞懂原理。所以我们先从原理开始讲。 3. MyBatis缓存的原理一级缓存与二级缓存正如大多数持久层框架一样，MyBatis 同样提供了一级缓存和二级缓存。一级缓存还有个别名Local Cache（本地缓存）。我觉得这个比较容易引起歧义，好像二级缓存就不放在本地了一样。事实上不管一级缓存还是二级缓存都是默认以HashMap的形式保存在本地内存Heap里的（虽然二级缓存也可以通过扩展保存到Memcached上）。一级缓存必定开启不能关闭，二级缓存默认不开启。一级缓存和二级缓存的差别在于作用域： 一级缓存默认基于SqlSession，可配置为基于Statement 二级缓存基于namespace，即可以跨SqlSession 如果开启二级缓存的话，先从二级缓存中查询，没有命中的话再查询一级缓存。流程图可以参考下图： 缓存命中的机制判断每个查询都会生成一个CacheKey对象。CacheKey对象包含了MappedStatement的Id、SQL的offset、SQL的limit、SQL本身以及SQL中的参数。全部匹配的Cache才会走缓存。 MyBatis Spring中缓存与事务的关系上面那段中的Statement和namespace没有什么特别需要解释的，看Mapper文件就可以了解。关于SqlSession可能需要解释一下。我们项目中引用的是mybatis-spring-boot-starter的1.3.2版本。从源代码可以看到是基于mybatis 3.4.6和mybatis-spring 1.3.2版本。除了上述介绍的MyBatis 3的原理之外，还允许MyBatis参与到Spring的事务管理中。原始的MyBatis 3的组件关系图如下：MyBatis Spring的组件关系图如下： MyBatis Spring中增加的SqlSessionTemplate是MyBatis Spring的核心。这个类负责管理MyBatis的SqlSession。从源代码中可以看到，如果如果Sql Session是在一个事务中，MyBatis不会急着提交。 1234567891011SqlSession sqlSession = getSqlSession(SqlSessionTemplate.this.sqlSessionFactory, SqlSessionTemplate.this.executorType, SqlSessionTemplate.this.exceptionTranslator);try &#123; Object result = method.invoke(sqlSession, args); if (!isSqlSessionTransactional(sqlSession, SqlSessionTemplate.this.sqlSessionFactory)) &#123; // force commit even on non-dirty sessions because some databases require // a commit/rollback before calling close() sqlSession.commit(true); &#125; return result;&#125; 而getSqlSession方法中有Spring的TransactionSynchronizationManager参与，增加一次sessionHolder的引用计数。 123456SqlSessionHolder holder = (SqlSessionHolder) TransactionSynchronizationManager.getResource(sessionFactory);SqlSession session = sessionHolder(executorType, holder);if (session != null) &#123; return session;&#125; 执行完关闭sqlsession的方法也会判断如果holder的引用计数减光了，那么就直接关闭session；如果还有引用计数，就只是减少引用计数，不关闭session。 12345678910111213public static void closeSqlSession(SqlSession session, SqlSessionFactory sessionFactory) &#123; notNull(session, NO_SQL_SESSION_SPECIFIED); notNull(sessionFactory, NO_SQL_SESSION_FACTORY_SPECIFIED); SqlSessionHolder holder = (SqlSessionHolder) TransactionSynchronizationManager.getResource(sessionFactory); if ((holder != null) &amp;&amp; (holder.getSqlSession() == session)) &#123; LOGGER.debug(() -&gt; &quot;Releasing transactional SqlSession [&quot; + session + &quot;]&quot;); holder.released(); &#125; else &#123; LOGGER.debug(() -&gt; &quot;Closing non transactional SqlSession [&quot; + session + &quot;]&quot;); session.close(); &#125;&#125; 当我们在Spring Bean的public方法上加了@Transactional注解，那么就会判断会话在事务中。所以在没有加事务的情况下，Mapper每次请求数据库，都会创建一个SqlSession，并在请求结束后关闭该SqlSession；如果加了事务，则会在事务里复用同一个SqlSession。（这里简化了一点逻辑，事实上还会进行executionType等的判断）之后做验证的时候也可以发现，如果我们循环获取序列的service方法上没有加@Transactional，那么每次获取的序列号是不同的，log里打印了多次请求的sql；而加上@Transactional，那么获取的序列号就是相同的，log里也只打印了一次请求sql。 缓存存储与刷新的机制缓存最底层的实现类是PerpetualCache类（Perpetual的意思是“永恒的”）。可以通过源代码看到它的实现非常简单： 123public class PerpetualCache implements Cache &#123; private final String id; private Map&lt;Object, Object&gt; cache = new HashMap&lt;&gt;(); 通过装饰器模式，在PerpetualCache类的基础上增加了日志、序列化、线程安全、清理等功能。装饰链如下：SynchronizedCache -&gt; LoggingCache -&gt; SerializedCache -&gt; LruCache -&gt; PerpetualCache缓存默认最大容量1024（参见源代码），使用LRU算法自动清理。在缓存作用域上调用任何修改语句（insert&#x2F;update&#x2F;delete）都会清空缓存，比较简单粗暴。 4. MyBatis缓存的特点MyBatis缓存的优点默认配置下的MyBatis缓存的作用比较有限，只会对同一事务中多次执行的同一SQL有优化效果。我们现在框架中的获取当前用户&#x2F;获取权限&#x2F;获取数据字典等经常被调用的方法，如果没有使用Spring Cache的话，至少MyBatis会避免一个方法查询100次数据库。 MyBatis缓存的坑还是那句听起来像玩笑话的名言：“计算机科学只存在两个难题：缓存失效和命名。”如果将MyBatis的一级缓存配置改为Statement级别，或开启MyBatis的二级缓存，问题就来了。 问题1：占用的内存大小如果我们在进行大数据量查询的时候没有加上分页条件，那么庞大的结果集会占用了大量内存，而且无法及时释放。 问题2：缓存失效的触发使statement或namespace里的缓存失效，只有两种方法： 触发LRU算法 在同一个应用上执行insert&#x2F;update&#x2F;delete 重启应用如果我们使用了分布式部署，在某一个节点上更新了数据，其他节点是不会得知数据有变更。 问题3：MyBatis+JPA的脏数据隐患我们现在使用的规范是单表情况下使用JPA进行CRUD，多表联合查询使用MyBatis。即使使用的是session级别的一级缓存，如果在同一个方法里包含了“MyBatis查询+JPA更新+MyBatis查询”三个步骤的逻辑，那么最后一个查询得到的就是更新前的结果。当然这个问题可以靠良好的代码规范部分解决，即“一个方法做一件事”。换句话说，不在一个方法里即做更新又做查询。 即使不使用JPA，全部用MyBatis来做CRUD，使用二级缓存也会有隐患。如果某个多表查询使用到的某几张表不在同一个namespace下，那么当这些表里的数据进行了修改，也会引发脏数据问题。 5. 我们场景下最终采取的方案从上述MyBatis的隐患可以看到，在我们分布式部署+使用JPA做单表CRUD的技术方案下，不适用开启statement级别的一级缓存，也不适宜开启二级缓存。 现在我们回过来看之前的那个解答：拿出@Transactional，就不会出错：【不完全对】这样的确可以解决问题，但不采用事务会引入其他风险。倒洗澡水连带着把孩子也倒掉了。 加上useCache&#x3D;”false” flushCache&#x3D;”true”,不保存在二级缓存中，并清空缓存【不完全对】这两个参数的含义可以看官方文档 12flushCache 将其设置为 true 后，只要语句被调用，都会导致本地缓存和二级缓存被清空，默认值：false。useCache 将其设置为 true 后，将会导致本条语句的结果被二级缓存缓存起来，默认值：对 select 元素为 true。 我们不使用二级缓存，所以useCache可以不用加。只需要加flushCache&#x3D;”true”就可以了。 mybatis.configuration.localCacheScope&#x3D;STATEMENT,修改一级缓存的作用域【不对】如果将一级缓存改为statement级别，获取sequence的语句还是会命中缓存，问题依然会存在。 mybatis.configuration.cacheEnabled &#x3D; false,禁用一级和二级缓存【不对】从源代码可以看到，cacheEnabled只控制CachingExecutor，即只能关闭二级缓存。而二级缓存本来就是默认关闭的。所以这么改毫无意义。 结论所以对于我们的场景的最终结论是：在获取序列的SQL语句的XML上，增加flushCache&#x3D;”true”。 参考资料MyBatis组件关系的架构图就是从这个博客里拿的，感谢日本同僚。虽然也只看得懂图。。。データベースアクセス（MyBatis3編） 作者写了很多的demo，结合源代码把MyBatis缓存讲得很透彻。聊聊MyBatis缓存机制 - 美团技术团队 MyBatis的官方文档mybatis – MyBatis 3 | XML 映射文件mybatis-spring – MyBatis-Spring | 事务","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://galaxyyao.github.io/tags/Java/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://galaxyyao.github.io/tags/MyBatis/"}]},{"title":"分布式配置中心 - 2. Spring Cloud Config评估","slug":"分布式配置中心-2-Spring-Cloud-Config评估","date":"2019-03-28T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/03/29/分布式配置中心-2-Spring-Cloud-Config评估/","permalink":"https://galaxyyao.github.io/2019/03/29/%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83-2-Spring-Cloud-Config%E8%AF%84%E4%BC%B0/","excerpt":"Spring Cloud Config架构图 从上图可以看到，Spring Cloud Config（Git版）的架构还是非常简单的。简单的代价就是缺少可视化配置界面，无法实现灰度发布和自动回滚。要实现比较复杂和细粒度的权限控制也比较困难。如果只使用Spring Cloud Config也没法实现配置自动刷新，还要依赖于Spring Cloud Bus。 技术资料Spring-Cloud-Config-官方文档-serverSpring-Cloud-Config-官方文档-clientSpring-Cloud-Config-配置中心-git 对配置中心的期望在具体评估技术之前，可以先列一下我们希望该技术能实现的目标，然后进行逐一确认是否能实现。对于配置中心，我们的期望如下： 从代码和项目管理角度","text":"Spring Cloud Config架构图 从上图可以看到，Spring Cloud Config（Git版）的架构还是非常简单的。简单的代价就是缺少可视化配置界面，无法实现灰度发布和自动回滚。要实现比较复杂和细粒度的权限控制也比较困难。如果只使用Spring Cloud Config也没法实现配置自动刷新，还要依赖于Spring Cloud Bus。 技术资料Spring-Cloud-Config-官方文档-serverSpring-Cloud-Config-官方文档-clientSpring-Cloud-Config-配置中心-git 对配置中心的期望在具体评估技术之前，可以先列一下我们希望该技术能实现的目标，然后进行逐一确认是否能实现。对于配置中心，我们的期望如下： 从代码和项目管理角度 清晰地按项目划分配置 可以将项目间共通的配置提取公共配置 可以支持非Java应用获取配置 从信息安全角度 生产和测试配置有分别的权限控制 可以对生产的密码配置进行不可逆的加密 从版本管理角度 可以支持多个不同分支同时并行开发 支持自动化持续集成 从运维角度 当配置修改后可以动态刷新更新配置 部署上可以实现高可用，当一个节点崩溃时不影响正常使用。 当配置中心不可用时，应用不会立刻崩溃 有可视化的界面，方便维护 Spring Cloud Config各种特性的具体实现按目录划分不同应用&#x2F;不同环境&#x2F;不同版本可以通过search-path实现。下面是按目录划分应用的范例： 1234567spring: cloud: config: server: git: uri: http://gitlab.anxintrust.com/config/java-project-config.git search-paths: &#x27;&#123;application&#125;&#x27; 共享公共配置可以在根目录下建立common&#x2F;application.yml文件实现 123search-paths: - &#x27;&#123;application&#125;&#x27; - common 动态刷新可以通过&#x2F;actuator&#x2F;refresh加上@RefreshScope注解实现。这个时候专门的@Configuration配置类的好处就体现出来了。只要给配置类加上@RefreshScope就可以了。而如果使用@Value获取配置的话，就需要在所有用到配置的类上加注解了。 加密通过JCE(Java Cryptography Extension)实现。具体可以参见：Encryption and Decryption 测试和生产使用不同的仓库为了信息安全考虑，生产和测试可以通过使用不同的Eureka服务，注册到不同的仓库 版本管理&#x2F;多分支并行开发多分支并行开发可以使用git的分支来实现。假设有两个人分别在branch1和branch2上开发，而这两个分支可能会产生冲突。那么可以将配置的git项目建立分支，然后在客户端改为如下配置 1234cloud: config: uri: http://127.0.0.1:20001/ label: branch1 高可用如果在client的配置里写死server，会导致两个副作用： 当ip修改后所有配置都需要修改 无法做到高可用所以配合Eureka，将多个config server注册到eureka server上。具体实现可以参见：配置中心Config和Eureka结合 · SpringCloud入门指南 · 看云即将client配置改为：1234567cloud: config: label: master fail-fast: true discovery: enabled: true service-id: cloud-config-server 支持非Java应用获取配置非Java应用可以通过http://ip:port/{application}/{profile}/{label} 获取配置。例如：http://127.0.0.1:20001/cloud-config-client-demo/dev/master 例如：Node.js通过Sidecar获取Spring Cloud Config配置 常见问题配置文件优先级Spring Cloud Config的配置需要早于application.yml&#x2F;application.properties加载。不然就会从默认的http://localhost:8888/获取配置。所以需要将配置写在resources&#x2F;bootstrap.yml文件里。 源代码阅读参考为什么bootstrap会在application配置之前读取Spring Cloud BootstrapApplicationListener Spring容器的刷新过程SpringBoot源码分析之Spring容器的refresh过程 | Format’s Notes Spring Cloud的热更新机制Spring Cloud 是如何实现热更新的 · ScienJus’s Blog 其他参考资料spring cloud:config-server中@RefreshScope的”陷阱” - 菩提树下的杨过 - 博客园https://www.cnblogs.com/yjmyzz/p/8085530.html 后记原本想在评估完Spring Cloud Config之后开始研究和部署Apollo。但实际评估下来觉得对于我们目前的状况来说，Spring Cloud Config就已经足够了。我们当前的情况： 研发团队规模小 微服务节点少 配置更新频率低 后端基本都是Java服务 Apollo和Nacos在以下几方面有优势： 更好的管理配置UI 多层级的权限控制 团队开发语言多样 配置更新频繁 微服务节点多 有一篇Nacos社区committer写的评估比较文可以作为参考，相对比较中立（略偏向Nacos）：架构设计之微服务配置中心选型","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://galaxyyao.github.io/tags/Java/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://galaxyyao.github.io/tags/Spring-Cloud/"}]},{"title":"从Gitlab CI启动tomcat的坑，到tty与进程组","slug":"Gitlab-CI-pipeline启动tomcat中遇到的坑","date":"2019-03-27T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/03/28/Gitlab-CI-pipeline启动tomcat中遇到的坑/","permalink":"https://galaxyyao.github.io/2019/03/28/Gitlab-CI-pipeline%E5%90%AF%E5%8A%A8tomcat%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/","excerpt":"问题症状为了做一个Spring MVC的Java Web项目的CI，我写了个编译war包后启动tomcat的脚本。CI脚本很简单： 123456sit: script: - cd /root/ - ./test-publish-xxx-sit.sh only: - develop SSH到服务器手工执行脚本一切顺利。但通过gitlab-runner执行脚本，到最后一步执行.&#x2F;startup.sh启动tomcat的时候，遇到了两个很奇怪的现象： 和SSH下执行.&#x2F;startup.sh不同，没有打印环境变量（例如Using CATALINA_BASE:）。只显示了最后一句“Tomcat started.” 虽然打印了“Tomcat started.”，tomcat却没有正常启动。catalina.out里完全没有启动日志信息尝试过从权限和执行用户方向排查，都没有找到原因。 解决方法在gitlab的论坛看到有人回答需要部署为linux的service，或者加个setsid，才能启动。结果证明这两种方式都是可行的解决方案。 问题是解决了。但疑问还是没解决：","text":"问题症状为了做一个Spring MVC的Java Web项目的CI，我写了个编译war包后启动tomcat的脚本。CI脚本很简单： 123456sit: script: - cd /root/ - ./test-publish-xxx-sit.sh only: - develop SSH到服务器手工执行脚本一切顺利。但通过gitlab-runner执行脚本，到最后一步执行.&#x2F;startup.sh启动tomcat的时候，遇到了两个很奇怪的现象： 和SSH下执行.&#x2F;startup.sh不同，没有打印环境变量（例如Using CATALINA_BASE:）。只显示了最后一句“Tomcat started.” 虽然打印了“Tomcat started.”，tomcat却没有正常启动。catalina.out里完全没有启动日志信息尝试过从权限和执行用户方向排查，都没有找到原因。 解决方法在gitlab的论坛看到有人回答需要部署为linux的service，或者加个setsid，才能启动。结果证明这两种方式都是可行的解决方案。 问题是解决了。但疑问还是没解决： 为何同样的用户执行，打印的日志不一样？ 为何普通脚本可以成功执行，但执行tomcat的启动脚本startup.sh的时候就会出问题？前一个问题和tty有关，后一个问题和Linux进程组有关。 引申1：tty以前在python脚本排查的时候遇到过一个诡异的问题：sudo -i切换root下时可以正常执行的命令，到su - root切换到root下就执行失败了。最终发现问题和PATH环境变量有关。但这次明显不是这个原因，要不然也不会打印“Tomcat started.”的日志。在翻了tomcat的catalina.sh后，找到了这段： 123456789101112131415# Bugzilla 37848: only output this if we have a TTYif [ $have_tty -eq 1 ]; then echo &quot;Using CATALINA_BASE: $CATALINA_BASE&quot; echo &quot;Using CATALINA_HOME: $CATALINA_HOME&quot; echo &quot;Using CATALINA_TMPDIR: $CATALINA_TMPDIR&quot; if [ &quot;$1&quot; = &quot;debug&quot; ] ; then echo &quot;Using JAVA_HOME: $JAVA_HOME&quot; else echo &quot;Using JRE_HOME: $JRE_HOME&quot; fi echo &quot;Using CLASSPATH: $CLASSPATH&quot; if [ ! -z &quot;$CATALINA_PID&quot; ]; then echo &quot;Using CATALINA_PID: $CATALINA_PID&quot; fifi 而have_tty这个变量是执行tty后的结果： 1234have_tty=0if [ &quot;`tty`&quot; != &quot;not a tty&quot; ]; then have_tty=1fi SSH的时候执行tty的结果是&#x2F;dev&#x2F;pts&#x2F;{数字}，而gitlab-runner执行的结果是not a tty。tty的含义可以参见文末的参考资料，可以简单理解为终端。gitlab与gitlab-runner通信的时候是通过https请求，没有终端。所以按照tomcat启动脚本的逻辑不会输出环境变量。 引申2：进程组排查时最疑惑的点在于：输出日志里打印了“Tomcat started.”，表示tomcat的启动脚本已经跑完了。但为何tomcat的进程不存在，catalina.out里也完全没有相关日志？要解释这个问题，需要从进程组开始解释。 当开两个SSH连到Linux服务器上，执行ps auxf命令，可以得到如下结果： 1234567USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1101 0.0 0.1 106084 4136 ? Ss Mar20 0:00 /usr/sbin/sshd -Droot 14590 0.0 0.1 145788 5244 ? Ss 23:00 0:00 \\_ sshd: root@pts/0root 14594 0.0 0.0 115440 2028 pts/0 Ss+ 23:00 0:00 | \\_ -bashroot 14631 3.2 0.1 145788 5240 ? Ss 23:23 0:00 \\_ sshd: root@pts/1root 14635 0.2 0.0 115436 2084 pts/1 Ss 23:23 0:00 \\_ -bashroot 14651 0.0 0.0 151244 1928 pts/1 R+ 23:23 0:00 \\_ ps auxf 这个界面展现了ssh相关的进程和进程间的父子关系。TTY那一列中的pts&#x2F;0和pts&#x2F;1分别对应两个SSH终端。sshd对应下图中的ssh server。 bash是sshd进程创建的子进程。当在第二个终端上通过bash执行ps auxf命令时，由bash进程创建ps auxf子进程。 123456789101112131415161718192021222324252627282930+----------+ +------------+| Keyboard |------&gt;| |+----------+ | Terminal || Monitor |&lt;------| |+----------+ +------------+ | | ssh protocol | ↓ +------------+ | | | ssh server |--------------------------+ | | fork | +------------+ | | ↑ | | | | write | | read | | | | +-----|---|-------------------+ | | | | | ↓ | ↓ | +-------+ | +-------+ | +--------+ | pts/0 |&lt;----------&gt;| shell | | | | +-------+ | +-------+ | | ptmx |&lt;-&gt;| pts/1 |&lt;----------&gt;| shell | | | | +-------+ | +-------+ | +--------+ | pts/2 |&lt;----------&gt;| shell | | +-------+ | +-------+ | Kernel | +-----------------------------+ 状态列STAT中的加号“+”表示前台进程（可以通过man ps命令查看各种状态的详情）。第一个大S表示进程在中断睡眠，大R表示运行中。第二个小s表示是会话的session leader。每个SSH窗口对应一个session会话。一个会话可以由多个进程组构成。一个进程组成为会话的前台工作(foreground)，而其他的进程组是后台工作(background)。我们也可以执行命令的时候添加&amp;，使进程组成为后台进程组。 在熟悉了这些知识后，我们来回顾一下我们的gitlab-runner脚本。我们是通过.&#x2F;test-publish-xxx-sit.sh命令来调用脚本。在我修改之前，脚本是通过调用.&#x2F;startup.sh启动tomcat。.&#x2F;xxx.sh是fork调用，即从当前进程创建一个子进程来执行脚本。（另外两种是source和exec）startup.sh fork调用了catalina.sh。而catalina.sh通过 1java 【省略参数】 org.apache.catalina.startup.Bootstrap start 【省略参数】 &amp; 这条命令启动了tomcat。总结一下，父子关系大致如下： 123456/usr/lib/gitlab-runner/gitlab-runner run \\_ /bin/bash \\_ /bin/sh ./test-publish-xxx-sit.sh \\_ /bin/sh ./startup.sh \\_ /bin/sh ./catalina.sh \\_ /usr/bin/java org.apache.catalina.startup.Bootstrap start 通过gitlab runner的源代码可以看到，gitlab runner在执行完每条命令，对该命令的进程组执行KillProcessGroup操作。 1234567select &#123;case err = &lt;-waitCh: return errcase &lt;-cmd.Context.Done(): return s.killAndWait(c, waitCh)&#125; 1234567891011func (s *executor) killAndWait(cmd *exec.Cmd, waitCh chan error) error &#123; for &#123; s.Debugln(&quot;Aborting command...&quot;) helpers.KillProcessGroup(cmd) select &#123; case &lt;-time.After(time.Second): case err := &lt;-waitCh: return err &#125; &#125;&#125; 所以当gitlab-runner执行完&#x2F;bin&#x2F;sh .&#x2F;test-publish-xxx-sit.sh这个命令，杀掉进程组后，tomcat进程也被跟着一起杀掉了。这也解释了为什么tomcat部署为服务和setsid命令会起效。当setsid后，tomcat的启动脚本进程和原父进程脱离关系，成为了孤儿进程。当部署为服务后，tomcat成为了守护进程，自然也和gitlab-runner的进程没有了关系。 后记想起来当初刚开始玩Spring Boot的时候，在Linux服务器上用java -jar加上&amp;后台启动应用后，过了两小时后进程被自动杀掉了。一开始还以为是Spring Boot的Bug。。。在总结了发生规律后，才发现是和SSH session有关。改为了nohup+&amp;启动后问题解决。之后又改为优雅一些的注册为系统服务。但对原理的不甚了了，最终还是导致这次栽坑了。知其然，知其所以然。不过这次相比之前也有一点改进：总算储备的shell知识积累到有胆子去翻tomcat启动脚本了。这次顺带解答了我之前的一个疑惑：为什么启动tomcat的启动命令.&#x2F;startup.sh时不用后面加&amp;。这是因为启动脚本里已经带了： 12345678eval \\&#123; $_NOHUP &quot;\\&quot;$_RUNJAVA\\&quot;&quot; &quot;\\&quot;$LOGGING_CONFIG\\&quot;&quot; $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \\ -D$ENDORSED_PROP=&quot;\\&quot;$JAVA_ENDORSED_DIRS\\&quot;&quot; \\ -classpath &quot;\\&quot;$CLASSPATH\\&quot;&quot; \\ -Dcatalina.base=&quot;\\&quot;$CATALINA_BASE\\&quot;&quot; \\ -Dcatalina.home=&quot;\\&quot;$CATALINA_HOME\\&quot;&quot; \\ -Djava.io.tmpdir=&quot;\\&quot;$CATALINA_TMPDIR\\&quot;&quot; \\ org.apache.catalina.startup.Bootstrap &quot;$@&quot; start \\ 2\\&gt;\\&amp;1 \\&amp; echo \\$! \\&gt;\\&quot;$catalina_pid_file\\&quot; \\; \\&#125; $catalina_out_command &quot;&amp;&quot; 但NOHUP参数默认不加，所以还是会被父进程杀掉。 参考资料Attempting to restart tomcat 8 with gitlab-runner, pid file created, log empty, server not started - Server Fault感谢作者解决问题后补充的回答。要不然我还钻在Google里，想不到去看tomcat启动脚本和gitlab-runner的源代码。 Linux TTY&#x2F;PTS概述 - Linux程序员 - SegmentFault 思否非常生动形象地用ASCII图展现了TTY的原理。 Linux 技巧：让进程在后台可靠运行的几种方法解释了为什么setsid和disown命令可以起效。 终端断开导致Tomcat进程被kill问题分析 | El Psy Congrootomcat的另一种非正常死法，通过进程组实验的方式解释了原理。我没有产生过作者那样的疑问，主要是个人习惯太好了，从来不会做不退出脚本就直接关闭终端的行为（雾","categories":[],"tags":[{"name":"持续集成","slug":"持续集成","permalink":"https://galaxyyao.github.io/tags/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"}]},{"title":"分布式配置中心 - 1. 配置中心介绍","slug":"分布式配置中心-1-配置中心介绍","date":"2019-03-25T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/03/26/分布式配置中心-1-配置中心介绍/","permalink":"https://galaxyyao.github.io/2019/03/26/%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83-1-%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E4%BB%8B%E7%BB%8D/","excerpt":"实施计划 先以测试环境一个应用作为试点，实施配置中心 稳定运行至少两周后，在生产环境的该应用上实施配置中心 再稳定运行两周后，在测试和生产环境的所有应用都改为使用配置中心 配置中心先使用Spring Cloud Config。不够用的话再考虑评估Apollo和Nacos 选型的考量Spring Cloud Config的原理和架构相对比较容易理解，所以先以Spring Cloud Config作为一个具体的实例，介绍一个简化版本的配置中心。如果只是小团队的话，Spring Cloud Config已经足够用了。但对于大一些的团队来说，还是存在以下两个不足： UI只能依赖Gitlab的界面 依赖于Git的权限控制粒度比较粗，也很难扩展 什么是配置中心这里引用几篇配置中心的科普文。 第一篇","text":"实施计划 先以测试环境一个应用作为试点，实施配置中心 稳定运行至少两周后，在生产环境的该应用上实施配置中心 再稳定运行两周后，在测试和生产环境的所有应用都改为使用配置中心 配置中心先使用Spring Cloud Config。不够用的话再考虑评估Apollo和Nacos 选型的考量Spring Cloud Config的原理和架构相对比较容易理解，所以先以Spring Cloud Config作为一个具体的实例，介绍一个简化版本的配置中心。如果只是小团队的话，Spring Cloud Config已经足够用了。但对于大一些的团队来说，还是存在以下两个不足： UI只能依赖Gitlab的界面 依赖于Git的权限控制粒度比较粗，也很难扩展 什么是配置中心这里引用几篇配置中心的科普文。 第一篇动态调整的基础 —— 配置中心这篇应该是淘宝在2016年之前的方案。虽然相对于现在已经比较out-of-date，但其中提到的几点比较有意思： 配置动态化的需求发展历程 应用 - 平台 - 版本 - 模块的元信息模型。在spring cloud config里，对应{application}&#x2F;{profile}&#x2F;{label}的三个层次。在Apollo里，对应Namespace。 把配置中心拆分成网关、核心服务和界面系统三个部分。在Apollo里，也拆分为Meta Server&#x2F;Config Service&#x2F;Admin Service三块。去年的一次分享上听一个阿里的技术专家说，他进阿里第一个月做的事情就是写开关配置。光20多行的代码就加了5个开关。阿里的这个“不要写死”的要求，在加强了灵活性的同时，对配置管理也提出了很高的要求。 第二篇下面这篇提出了一个通用版的配置中心的架构图。为什么需要分布式配置中心？ - 徐刘根 第三篇一篇好TM长的关于配置中心的文章 | 阿里中间件团队博客顺便提一句，文中提到的淘宝的开源配置中心项目Diamond，但已经5年没有维护了。。。 配置中心的优点与缺点优点 避免敏感信息在源代码中暴露，安全性提升 可以实现不重启应用就动态刷新配置 可以在应用间共享配置。原本只能在应用内共享配置 配置集中化管理，易于全局管理所有配置 对配置进行权限管理 缺点 原本下载应用代码就可以启动，现在还增加依赖spring cloud config server服务&#x2F;Apollo服务 如果config服务挂了，应用会无法重启 不如配置放在项目中那么直观 多分支开发的时候，处理不当的话可能会互相影响","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://galaxyyao.github.io/tags/Java/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://galaxyyao.github.io/tags/Spring-Cloud/"}]},{"title":"Twelve-Factor 12要素12原则","slug":"Twelve-Factor-12要素12原则","date":"2019-03-21T16:00:00.000Z","updated":"2021-04-30T05:34:34.000Z","comments":true,"path":"2019/03/22/Twelve-Factor-12要素12原则/","permalink":"https://galaxyyao.github.io/2019/03/22/Twelve-Factor-12%E8%A6%81%E7%B4%A012%E5%8E%9F%E5%88%99/","excerpt":"概述英文版https://12factor.net/zh_cn/中文版https://12factor.net/ 这篇文章是Spring Cloud文档的总述部分提到的。12要素指的是构建SaaS应用的方法论的总结，一共有12条。当得知这12要素是在2011年就提出的时候，我不禁由衷地钦佩。想想11年的时候我[消音–]都在做些啥。。。从另外一个角度可以学习到的是如何从实践中提炼和总结经验，然后总结为理论。通过“实践-理论-实践-理论”这样的循环，实现螺旋式地提升。 意义类似“社会主义核心价值观”（手动狗头），12要素对我们的意义主要在对实现的指导性。我们有些时候会觉得当前的代码管理&#x2F;配置管理&#x2F;部署方案&#x2F;运维方案不够好，但又不知道怎么改进；有些时候看到了多种改进方案，但不知道哪一种更好。在这种时候可以参考12要素来决策。 12要素归类我认为12要素可以粗略分为几类： 代码管理","text":"概述英文版https://12factor.net/zh_cn/中文版https://12factor.net/ 这篇文章是Spring Cloud文档的总述部分提到的。12要素指的是构建SaaS应用的方法论的总结，一共有12条。当得知这12要素是在2011年就提出的时候，我不禁由衷地钦佩。想想11年的时候我[消音–]都在做些啥。。。从另外一个角度可以学习到的是如何从实践中提炼和总结经验，然后总结为理论。通过“实践-理论-实践-理论”这样的循环，实现螺旋式地提升。 意义类似“社会主义核心价值观”（手动狗头），12要素对我们的意义主要在对实现的指导性。我们有些时候会觉得当前的代码管理&#x2F;配置管理&#x2F;部署方案&#x2F;运维方案不够好，但又不知道怎么改进；有些时候看到了多种改进方案，但不知道哪一种更好。在这种时候可以参考12要素来决策。 12要素归类我认为12要素可以粗略分为几类： 代码管理I. 基准代码&#x2F;II. 依赖 配置管理III. 配置&#x2F;IV. 后端服务 部署V. 构建，发布，运行&#x2F;VI. 进程&#x2F;VII. 端口绑定&#x2F;VIII. 并发 线上运维IX. 易处理&#x2F;X. 开发环境与线上环境等价&#x2F;XI. 日志&#x2F;XII. 管理进程 12要素的修订和引申毕竟是在8年前的2011年提出的，随着技术的发展，也有对12要素提出了一些修订。比如如下这篇：MRA(Microservices Reference Architecture), Part 5: Adapting the Twelve Factor App for Microservices 主要有以下几点： 对于“II. 依赖”，不仅限于类库依赖，还包括部署环境依赖。非容器环境，使用运维管理工具（Chef, Puppet, Ansible）来安装系统依赖；对于容器环境，使用Dockerfile。后面也有几点是基于容器化方案的改进意见。 对于“VII. 端口绑定”，将原则扩展到数据隔离。即如果要获取另一个微服务的数据，只能通过API，而不能通过其他方式（例如读取数据库）另外要避免微服务之间有明显的依赖。 反思我们当前不符合12要素的实现配置现在的生产配置的加密方式还很粗糙。另外现在配置还没有实现从环境变量中读取。以上两点将在实施配置中心后改善。 服务间依赖部分微服务之间还有明显的依赖，无法避免雪崩效应。后续将引入服务发现和服务注册来改善。 进程无状态部分应用的登录接口还带有粘性session。这是12-Factor极力反对的。但避免粘性Session的一个必要条件是保持缓存服务的高可用。后续需要对Redis的高可用进行升级。 日志日志分析&#x2F;统计&#x2F;告警已经迈出了第一步，还有待后续改进措施的实施。","categories":[],"tags":[{"name":"cloud","slug":"cloud","permalink":"https://galaxyyao.github.io/tags/cloud/"}]}],"categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://galaxyyao.github.io/tags/%E7%BD%91%E7%BB%9C/"},{"name":"ai","slug":"ai","permalink":"https://galaxyyao.github.io/tags/ai/"},{"name":"hardware","slug":"hardware","permalink":"https://galaxyyao.github.io/tags/hardware/"},{"name":"review","slug":"review","permalink":"https://galaxyyao.github.io/tags/review/"},{"name":"share","slug":"share","permalink":"https://galaxyyao.github.io/tags/share/"},{"name":"大数据","slug":"大数据","permalink":"https://galaxyyao.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"IaC","slug":"IaC","permalink":"https://galaxyyao.github.io/tags/IaC/"},{"name":"金融","slug":"金融","permalink":"https://galaxyyao.github.io/tags/%E9%87%91%E8%9E%8D/"},{"name":"运维","slug":"运维","permalink":"https://galaxyyao.github.io/tags/%E8%BF%90%E7%BB%B4/"},{"name":"项目管理","slug":"项目管理","permalink":"https://galaxyyao.github.io/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"},{"name":"中台","slug":"中台","permalink":"https://galaxyyao.github.io/tags/%E4%B8%AD%E5%8F%B0/"},{"name":"容器","slug":"容器","permalink":"https://galaxyyao.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://galaxyyao.github.io/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://galaxyyao.github.io/tags/k8s/"},{"name":"Java","slug":"Java","permalink":"https://galaxyyao.github.io/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://galaxyyao.github.io/tags/Spring/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://galaxyyao.github.io/tags/Spring-Cloud/"},{"name":"Feign","slug":"Feign","permalink":"https://galaxyyao.github.io/tags/Feign/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://galaxyyao.github.io/tags/PostgreSQL/"},{"name":"pgsql","slug":"pgsql","permalink":"https://galaxyyao.github.io/tags/pgsql/"},{"name":"Maven","slug":"Maven","permalink":"https://galaxyyao.github.io/tags/Maven/"},{"name":"持续集成","slug":"持续集成","permalink":"https://galaxyyao.github.io/tags/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"name":"CI","slug":"CI","permalink":"https://galaxyyao.github.io/tags/CI/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://galaxyyao.github.io/tags/Jenkins/"},{"name":"数据库","slug":"数据库","permalink":"https://galaxyyao.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://galaxyyao.github.io/tags/MySQL/"},{"name":"IE","slug":"IE","permalink":"https://galaxyyao.github.io/tags/IE/"},{"name":"Windows","slug":"Windows","permalink":"https://galaxyyao.github.io/tags/Windows/"},{"name":"Nginx","slug":"Nginx","permalink":"https://galaxyyao.github.io/tags/Nginx/"},{"name":"docker","slug":"docker","permalink":"https://galaxyyao.github.io/tags/docker/"},{"name":"Linux","slug":"Linux","permalink":"https://galaxyyao.github.io/tags/Linux/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://galaxyyao.github.io/tags/MyBatis/"},{"name":"cloud","slug":"cloud","permalink":"https://galaxyyao.github.io/tags/cloud/"}]}