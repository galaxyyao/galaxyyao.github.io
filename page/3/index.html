<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Galaxy</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="姚皓的技术博客">
<meta property="og:type" content="website">
<meta property="og:title" content="Galaxy">
<meta property="og:url" content="https://galaxyyao.github.io/page/3/index.html">
<meta property="og:site_name" content="Galaxy">
<meta property="og:description" content="姚皓的技术博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="姚皓(Hao Yao)">
<meta property="article:tag" content="galaxyyao,姚皓">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Galaxy" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Galaxy</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">姚皓的技术博客-一杯咖啡，一首音乐，一台电脑，编程</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://galaxyyao.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-容器-8-Kubernetes实战-k8s核心概念之Node-Pod与Deployment" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/06/24/%E5%AE%B9%E5%99%A8-8-Kubernetes%E5%AE%9E%E6%88%98-k8s%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BNode-Pod%E4%B8%8EDeployment/" class="article-date">
  <time class="dt-published" datetime="2019-06-23T16:00:00.000Z" itemprop="datePublished">2019-06-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/06/24/%E5%AE%B9%E5%99%A8-8-Kubernetes%E5%AE%9E%E6%88%98-k8s%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BNode-Pod%E4%B8%8EDeployment/">容器-8-Kubernetes实战-k8s核心概念之Node,Pod与Deployment</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本篇我们开始用k8s的方式部署静态网站镜像，并通过这个过程了解k8s为什么会抽象出那么多概念。  </p>
<h2 id="1-Node"><a href="#1-Node" class="headerlink" title="1. Node"></a>1. Node</h2><p>k8s首先需要选定在哪一台或哪几台服务器上部署。<br>如我们在kubeadm部署k8s集群的时候就已知的，k8s集群是由1-N台Master节点和N个Worker节点组成的。<br><img src="/images/%E5%AE%B9%E5%99%A8-8-Kubernetes%E5%AE%9E%E6%88%98-k8s%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BNode-Pod%E4%B8%8EDeployment/k8s-physics-architecture.jpeg" alt="Kubernetes节点">  </p>
<p>k8s的设计原则之一就是不挑Worker节点的硬件配置。毕竟当初Google搭Borg集群的时候淘到的服务器硬件各式各样都有。（想起来当初搭Hadoop的时候也有人问过我是不是什么硬件都可以。。。以Hadoop MapReduce的吃硬盘和网络程度，实体机+专用万兆宽带跑出来的性能比虚拟机快出好几倍。当然配置低也的确能跑起来不死。）<br>你手头的机器可能是什么歪瓜裂枣都有：<br><img src="/images/%E5%AE%B9%E5%99%A8-8-Kubernetes%E5%AE%9E%E6%88%98-k8s%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BNode-Pod%E4%B8%8EDeployment/k8s-nodes-1.png" alt="Kubernetes Nodes物理硬件"><br>不管是实体机还是虚拟机，不管什么硬件配置，不管高的矮的胖的瘦的，k8s都将其一视同仁地抽象为一个Node（曾经也有一个专有名词minion）。<br><img src="/images/%E5%AE%B9%E5%99%A8-8-Kubernetes%E5%AE%9E%E6%88%98-k8s%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BNode-Pod%E4%B8%8EDeployment/k8s-nodes-2.png" alt="Kubernetes Nodes抽象">  </p>
<p>一个典型的Node信息如下（部分删减）：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">Name:               docker-7</span><br><span class="line">Roles:              worker</span><br><span class="line">Conditions:</span><br><span class="line">  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message</span><br><span class="line">  ----             ------  -----------------                 ------------------                ------                       -------</span><br><span class="line">  MemoryPressure   False   Fri, 21 Jun 2019 16:58:54 +0800   Fri, 21 Jun 2019 16:57:54 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available</span><br><span class="line">  DiskPressure     False   Fri, 21 Jun 2019 16:58:54 +0800   Fri, 21 Jun 2019 16:57:54 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure</span><br><span class="line">  PIDPressure      False   Fri, 21 Jun 2019 16:58:54 +0800   Fri, 21 Jun 2019 16:57:54 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available</span><br><span class="line">  Ready            True    Fri, 21 Jun 2019 16:58:54 +0800   Fri, 21 Jun 2019 16:57:54 +0800   KubeletReady                 kubelet is posting ready status</span><br><span class="line">Addresses:</span><br><span class="line">  InternalIP:  10.16.34.59</span><br><span class="line">  Hostname:    docker-7</span><br><span class="line">Capacity:</span><br><span class="line"> cpu:                4</span><br><span class="line"> ephemeral-storage:  101729776Ki</span><br><span class="line"> hugepages-2Mi:      0</span><br><span class="line"> memory:             8010576Ki</span><br><span class="line"> pods:               110</span><br><span class="line">Allocatable:</span><br><span class="line"> cpu:                4</span><br><span class="line"> ephemeral-storage:  93754161407</span><br><span class="line"> hugepages-2Mi:      0</span><br><span class="line"> memory:             7908176Ki</span><br><span class="line"> pods:               110</span><br><span class="line">Allocated resources:</span><br><span class="line">  (Total limits may be over 100 percent, i.e., overcommitted.)</span><br><span class="line">  Resource           Requests    Limits</span><br><span class="line">  --------           --------    ------</span><br><span class="line">  cpu                320m (8%)   300m (7%)</span><br><span class="line">  memory             150Mi (1%)  150Mi (1%)</span><br><span class="line">  ephemeral-storage  0 (0%)      0 (0%)</span><br></pre></td></tr></table></figure>

<p>Master节点不断轮询更新每个Node的状态信息：“你还活着么？压力大不大？CPU&#x2F;内存&#x2F;存储已经分配了多少？还剩下多少？”<br>这样当Master节点为了新的容器需求征兵时，就能很容易地知道哪几个节点还够压榨。  </p>
<h2 id="2-Pod"><a href="#2-Pod" class="headerlink" title="2. Pod"></a>2. Pod</h2><p>Pod是Kubernetes中的原子调度单位。最简单的Pod就等于一个容器。这么表述也就意味着Pod里也可以放多个容器。<br>看为什么Kubernetes会发明出来Pod这个概念？因为有些容器之间有共享网络和存储的需求。  </p>
<p>举个最典型的例子：日志收集器。例如我们的静态网站容器会在被访问的时候生成access.log文件。如果是在虚机里部署，那么就会在服务器上另外起一个logstash，收集这些日志文件后汇总到Elasticsearch。<br>如果logstash和Nginx不在同一台宿主机上部署，虽然也不是不可行，但就会很折腾：后文会提到挂载PV，可以在logstash和Nginx的Pod上都挂载同一个PV。但这个折腾毫无必要。日志文件并不那么重要，不值得永久存储，生命周期跟着容器即可。当容器被销毁时，日志也可以跟着被销毁。  </p>
<p>那么我们是不是可以把Nginx和logstash打包到同一个容器中呢？这会产生一个问题：当logstash进程挂掉时，k8s的监控怎么表示容器的状态？如果显示Failure然后重启容器，则无辜的Nginx进程也受到了牵连；如果显示正常，那么要单独重启容器里的某一个进程就会变得很麻烦。<br>所以我们需要将Nginx和logstash单独打包为容器，部署成一个Pod。k8s会将一个Pod里的容器都部署在同一台宿主机上。  </p>
<p><img src="/images/%E5%AE%B9%E5%99%A8-8-Kubernetes%E5%AE%9E%E6%88%98-k8s%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BNode-Pod%E4%B8%8EDeployment/k8s-pod.png" alt="Kubernetes Pod">  </p>
<p>k8s的<a target="_blank" rel="noopener" href="https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/">官方博客</a>将多容器Pod的类型列为三种：  </p>
<ul>
<li>Sidecar（边车）容器</li>
<li>Ambassador（大使）容器</li>
<li>Adapter（适配器）容器<br>k8s Pod的实现方式特别适合我们将一些控制平面的功能放到Sidecar中。近几年很知名的Service Mesh项目就是完全通过Sidecar模式支撑起来的。我们以后再详细讨论这个话题。</li>
</ul>
<p>其实对于Pod不用想得太复杂，可以认为它就是逻辑上的一台虚机的概念。只有必须部署在同一台虚机上的，才会被并到一个Pod里。<br>例如虽然MySQL和Java应用虽然也可以部署在同一台虚机上，但从best practice考量，一般不会这么部署，所以就不是一个Pod。MySQL的Master和Slave一般是部署在两台服务器上，所以也是两个独立的Pod。  </p>
<h3 id="2-1-Pod-YAML"><a href="#2-1-Pod-YAML" class="headerlink" title="2.1 Pod YAML"></a>2.1 Pod YAML</h3><p>要部署一个Pod，我们需要先写一个YAML描述文件，然后用k8s的命令部署。<br>我从Spring Boot的配置开始就已经接触了挺久的YAML，所以这边就不详细介绍YAML的语法了。如果有对语法部署的可以参考<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/YAML">Wiki</a>。<br>一个最简单的Nginx Pod部署YAML如下：  </p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-pod</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx:latest</span></span><br></pre></td></tr></table></figure>

<p>这个YAML第一层的元素有四个：apiVersion, kind, metadata和spec。<br>apiVersion的v1表示是稳定版。（像之前kubeadm的apiVersion就是kubeadm.k8s.io&#x2F;v1beta1，还在快速迭代中）。<br>kind就是想要创建的对象的类型。<br>metadata是用来识别对象的唯一性。<br>spec字段是对象规约，内容随着每个Kubernetes对象而不同。对于该Nginx Pod来说，需要指定使用的唯一镜像以及镜像的版本。关于镜像和镜像版本可以在<a target="_blank" rel="noopener" href="https://hub.docker.com/">Docker Hub</a>上查到。  </p>
<h3 id="2-2-部署命令"><a href="#2-2-部署命令" class="headerlink" title="2.2 部署命令"></a>2.2 部署命令</h3><p>我们可以使用如下的命令部署：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nginx-pod.yaml</span><br></pre></td></tr></table></figure>

<p>然后就可以通过如下命令查看Pod创建进展：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod</span><br></pre></td></tr></table></figure>

<p>以及通过如下命令查看Pod详细信息：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;Pod名&gt;</span><br></pre></td></tr></table></figure>

<p>想进入Pod，就是用exec -it命令：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it nginx-pod /bin/bash</span><br></pre></td></tr></table></figure>

<p>如果不需要这个Pod了，可以用以下命令取消部署：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f nginx-pod.yaml</span><br></pre></td></tr></table></figure>

<p>k8s的命令虽然很多，但都非常有规律。基本格式都是：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl 动作 对象类型</span><br></pre></td></tr></table></figure>

<p>或</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl 动作 [-参数] 对象类型 对象名</span><br></pre></td></tr></table></figure>

<p>对于非default namespace的对象，再加一个namespace参数：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl 动作 对象类型 对象名 -n namespace名</span><br></pre></td></tr></table></figure>

<p>基本不怎么需要特别记忆。  </p>
<p>另外提一下，在使用了k8s后，最好不要再打Docker命令了。当然自己本地开发机上跑容器的时候还是需要拍Docker命令。<br>在本次POC中，我们不将Nginx部署为Pod。原因在下一章中说明。  </p>
<h3 id="2-3-验证"><a href="#2-3-验证" class="headerlink" title="2.3 验证"></a>2.3 验证</h3><p>此时Pod已部署成功，但我们还没法从容器外部访问。想验证的话可以稍微修改一下YAML，增加一个验证的shell容器，然后共享PID Namespace：  </p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-pod</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">shareProcessNamespace:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shell</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">stdin:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">tty:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>上述配置在spec下增加了shareProcessNamespace: true，表示PID Namespace共享。最底下还增加一个镜像为busybox的容器。（busybox和alpine由于体积比较小，在k8s部署的时候有广泛的用途）<br>要部署之前，我们需要先kubectl delete -f nginx-pod.yaml把pod删除，不然会得到告警：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spec.containers: Forbidden: pod updates may not add or remove containers</span><br></pre></td></tr></table></figure>

<p>在重新apply后使用如下的命令进入Pod</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl attach -it nginx-pod -c shell</span><br></pre></td></tr></table></figure>

<p>进入busybox容器后执行ps aux，就可以看到Nginx的进程了：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/ # ps aux</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 root      0:00 /pause</span><br><span class="line">    6 root      0:00 nginx: master process nginx -g daemon off;</span><br><span class="line">   11 101       0:00 nginx: worker process</span><br><span class="line">   12 root      0:00 sh</span><br><span class="line">   17 root      0:00 ps aux</span><br></pre></td></tr></table></figure>

<h3 id="2-4-infra容器：pause"><a href="#2-4-infra容器：pause" class="headerlink" title="2.4 infra容器：pause"></a>2.4 infra容器：pause</h3><p>上一节进程的查询结果中有一个pause进程需要说明一下。<br>当我们使用docker ps查看容器时，会发现有相当多的pause容器在启动。该容器和我们前几节提到的多容器Pod有关。<br>以我们上面修改后的nginx-pod范例为例。当我们需要让两个容器共享Namespace（不仅仅是IPC，还包括Network等其他Namespace），方法之一是让一个容器先启动，然后将另外一个容器的Namespace设置为前一个容器的Namespace。从技术上可行，但这个会导致这两个容器之间的关系不再是对等拓扑关系。<br>所以需要有个中间容器，也就是pause容器存在。pause容器非常小，也基本干不了什么事。它唯一的作用就是在Pod启动的最开始就启动，然后在其他容器启动后和它们共享自己的Namespace。在所有的容器启动完成后就暂停自己，不再消耗资源。<br><img src="/images/%E5%AE%B9%E5%99%A8-8-Kubernetes%E5%AE%9E%E6%88%98-k8s%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BNode-Pod%E4%B8%8EDeployment/k8s-infra-container.png" alt="Kubernetes Infra Container">  </p>
<h2 id="3-Deployment"><a href="#3-Deployment" class="headerlink" title="3. Deployment"></a>3. Deployment</h2><p>Pod的副本实例数是在Deployment中定义的。<br>假设我们从负载和高可用的角度考虑，想要在k8s中部署2个Nginx的实例，那么只需要在Deployment的配置中定义replicas: 2即可。  </p>
<p>我们实际写k8s yaml配置的时候，不推荐直接部署为Pod。即使是只有一个副本，也推荐部署为replicas为1的Deployment。这是由于k8s的调度机制是通过Deployment来确保副本数量。万一Pod所在的服务器挂了，k8s会检测到副本数不足1，于是将Pod调度到健康的Node上。  </p>
<p><img src="/images/%E5%AE%B9%E5%99%A8-8-Kubernetes%E5%AE%9E%E6%88%98-k8s%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BNode-Pod%E4%B8%8EDeployment/k8s-deployment.png" alt="Kubernetes Deployment">  </p>
<h3 id="3-1-Deployment-YAML"><a href="#3-1-Deployment-YAML" class="headerlink" title="3.1 Deployment YAML"></a>3.1 Deployment YAML</h3><p>一个Nginx Deployment的精简版的YAML如下：  </p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:latest</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">web</span></span><br></pre></td></tr></table></figure>

<p>可以看到spec下有一个replicas，表述副本数量。  </p>
<p>看到这个YAML，我第一反应是有些晕：为什么spec套了一层spec，template下还有一层metadata。<br>这篇<a target="_blank" rel="noopener" href="https://medium.com/@zwhitchcox/matchlabels-labels-and-selectors-explained-in-detail-for-beginners-d421bdd05362">matchLabels, labels, and selectors explained in detail, for beginners</a>解释了为什么这个YAML有点绕的原因。关键点是这个Deployment YAML中的template其实是podTemplate。这样你就会发现template中的部分只要加上apiVersion和kind，基本就是Pod的YAML。<br><img src="/images/%E5%AE%B9%E5%99%A8-8-Kubernetes%E5%AE%9E%E6%88%98-k8s%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BNode-Pod%E4%B8%8EDeployment/k8s-deployment-pod-yaml.png" alt="Kubernetes YAML template">  </p>
<p>与template平级的还有一个selector选择器属性。这个selector选择器表示deployment部署的是label里带app的podTemplate。<br>label作为一个非唯一的标签属性，可以使Kubernetes的运维更加灵活。这个我们后续再详细研究。  </p>
<p>我们现在可以总结出一个最简单版的deployment yaml模板：  </p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">&lt;deployment-name&gt;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="string">&lt;replicas-number&gt;</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">&lt;app-name&gt;</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">&lt;app-name&gt;</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&lt;app-name&gt;</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">&lt;image-name&gt;:&lt;tag-name&gt;</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="string">&lt;container-port&gt;</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">&lt;container-port-name&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="3-2-部署命令"><a href="#3-2-部署命令" class="headerlink" title="3.2 部署命令"></a>3.2 部署命令</h3><p>部署命令和Pod基本一样：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nginx-deployment.yaml</span><br></pre></td></tr></table></figure>

<p>然后查看到ready的deployment了：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-4 pod]# kubectl get deployment</span><br><span class="line">NAME               READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   2/2     2            2           16h</span><br></pre></td></tr></table></figure>

<h3 id="3-3-部署POC-Nginx镜像"><a href="#3-3-部署POC-Nginx镜像" class="headerlink" title="3.3 部署POC Nginx镜像"></a>3.3 部署POC Nginx镜像</h3><p>要部署POC的Nginx镜像，只需要将image修改为私有仓库的镜像即可：  </p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">poc-web</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">poc-web</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">poc-web</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">poc-web</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">poc-web</span></span><br><span class="line">        <span class="attr">image:</span> <span class="number">10.16</span><span class="number">.34</span><span class="number">.197</span><span class="string">:5000/staticsite:1.0</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">web</span></span><br></pre></td></tr></table></figure>

<p>如果使用Docker Hub作为Docker Registry，则首先创建secret（什么是secret会在<a href="https://galaxyyao.github.io/2019/07/03/%E5%AE%B9%E5%99%A8-12-Kubernetes%E5%AE%9E%E6%88%98-%E9%9D%99%E6%80%81%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%961-ConfigMap-Secret%E4%B8%8ETLS/">容器-12-Kubernetes实战-静态网站部署优化1:ConfigMap,Secret与TLS | Galaxy</a> 中详细说明）：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create secret docker-registry regcred --docker-username=账号 --docker-password=密码 --docker-email=Docker邮箱 -n 命名空间默认default</span><br></pre></td></tr></table></figure>

<p>然后在pod定义的同一级加上imagePullSecrets，例如：  </p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">poc-web</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">poc-web</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">poc-web</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">poc-web</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">poc-web</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">galaxyyao/demosite:1.0</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">regcred</span></span><br></pre></td></tr></table></figure>

<h2 id="4-参考资料"><a href="#4-参考资料" class="headerlink" title="4. 参考资料"></a>4. 参考资料</h2><p>Kubernetes Node的官方文档概念介绍<br><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/architecture/nodes/">Nodes - Kubernetes</a>  </p>
<p>Kubernetes Pod的官方文档概念介绍<br><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">Pod Overview - Kubernetes</a>  </p>
<p>Kubernetes Deployment的官方文档概念介绍<br><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments - Kubernetes</a>  </p>
<p>这是一篇关于Pause容器的详细介绍<br><a target="_blank" rel="noopener" href="https://www.ianlewis.org/en/almighty-pause-container">The Almighty Pause Container - Ian Lewis</a>  </p>
<p>本篇部分有趣的图来自这篇。很简要易懂的科普文。<br><a target="_blank" rel="noopener" href="https://medium.com/@geraldcroes/kubernetes-traefik-101-when-simplicity-matters-957eeede2cf8">Kubernetes &amp; Traefik 101— When Simplicity Matters – Gérald Croës – Medium</a>  </p>
<p>如果想补习YAML的话可以参考这篇：<br><a target="_blank" rel="noopener" href="https://developer.ibm.com/tutorials/yaml-basics-and-usage-in-kubernetes/">YAML basics in Kubernetes – IBM Developer</a>  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://galaxyyao.github.io/2019/06/24/%E5%AE%B9%E5%99%A8-8-Kubernetes%E5%AE%9E%E6%88%98-k8s%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B9%8BNode-Pod%E4%B8%8EDeployment/" data-id="clzjpz1yj000q8v6ob4170oln" data-title="容器-8-Kubernetes实战-k8s核心概念之Node,Pod与Deployment" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/k8s/" rel="tag">k8s</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kubernetes/" rel="tag">kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8/" rel="tag">容器</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-容器-7-Kubernetes实战-私有仓库和打包镜像" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/06/18/%E5%AE%B9%E5%99%A8-7-Kubernetes%E5%AE%9E%E6%88%98-%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E5%92%8C%E6%89%93%E5%8C%85%E9%95%9C%E5%83%8F/" class="article-date">
  <time class="dt-published" datetime="2019-06-17T16:00:00.000Z" itemprop="datePublished">2019-06-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/06/18/%E5%AE%B9%E5%99%A8-7-Kubernetes%E5%AE%9E%E6%88%98-%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E5%92%8C%E6%89%93%E5%8C%85%E9%95%9C%E5%83%8F/">容器-7-Kubernetes实战-私有仓库和打包镜像</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>我们先从比较简单的部分开始做。静态网站是一个比较合适的开端。独立，有界面方便验证，而且无状态。  </p>
<h2 id="1-搭建Docker私有仓库"><a href="#1-搭建Docker私有仓库" class="headerlink" title="1. 搭建Docker私有仓库"></a>1. 搭建Docker私有仓库</h2><p>按照容器的思路，我们需要先做一个静态网站文件+Nginx的镜像，然后在服务器上把镜像拖下来后实例化为容器运行。  </p>
<p>从安全和网络速度上考虑，我们做的这个镜像不太适合放到公网的<a href="hub.docker.com">Docker Hub</a>上。所以要先搭个私有仓库。  </p>
<p>我们先在一台虚机上按照<a href="/2019/05/29/%E5%AE%B9%E5%99%A8-5-kubeadm%E9%83%A8%E7%BD%B2Kubernetes1-14-2%E9%9B%86%E7%BE%A4%E8%B8%A9%E5%9D%91%E8%AE%B0">kubeadm部署指南</a>中的“2.1 安装Docker”和“2.2 启动Docker服务”两节安装docker。然后关闭防火墙。最后执行一句docker run就解决了：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 5000:5000 --restart=always --name docker-registry registry</span><br></pre></td></tr></table></figure>

<p>一开始没有关闭防火墙，在启动的时候报错了。再次启动的时候提示容器已存在。这时候只需要将docker run命令改为docker start命令就可以了。<br>PS. 如果是要在Windows上运行，要把端口从5000映射到5001，不然会因为和Docker Desktop端口冲突而容器启动失败。<br>我把Docker Registry私有仓库部署在10.16.34.197服务器上，于是现在访问<a target="_blank" rel="noopener" href="http://10.16.34.197:5000/v2/_catalog">http://10.16.34.197:5000/v2/_catalog</a>，就可以看到当前还没有镜像：  </p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;repositories&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="2-镜像打包服务器配置"><a href="#2-镜像打包服务器配置" class="headerlink" title="2. 镜像打包服务器配置"></a>2. 镜像打包服务器配置</h2><p>在要打包和push镜像的服务器上（一般是Gitlab或Jenkins服务器吧），需要执行以下命令（要替换ip）：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;insecure-registries&quot;: [</span><br><span class="line">        &quot;10.16.34.197:5000&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>

<p>要不然之后push的时候就会遇到报错信息：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-4 vue-hello-world]# docker push 10.16.34.197:5000/staticsite</span><br><span class="line">The push refers to repository [10.16.34.197:5000/staticsite]</span><br><span class="line">Get https://10.16.34.197:5000/v2/: http: server gave HTTP response to HTTPS client</span><br></pre></td></tr></table></figure>

<p>另外也需要在k8s的每个master和worker节点上都执行。不然在部署的时候也会遇到报错信息。  </p>
<h2 id="3-打包网站镜像"><a href="#3-打包网站镜像" class="headerlink" title="3. 打包网站镜像"></a>3. 打包网站镜像</h2><p>略过不相关的web项目创建和提交代码过程，我们在Gitlab服务器上得到了需要部署的静态网站文件：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-4 dist]# pwd</span><br><span class="line">/git/vue-hello-world/dist</span><br><span class="line">[root@docker-4 dist]# ll</span><br><span class="line">total 12</span><br><span class="line">drwxr-xr-x. 2 root root   30 Jun 13 01:42 css</span><br><span class="line">-rw-r--r--. 1 root root 4286 Jun 13 01:42 favicon.ico</span><br><span class="line">drwxr-xr-x. 2 root root   31 Jun 13 01:42 img</span><br><span class="line">-rw-r--r--. 1 root root  730 Jun 13 01:42 index.html</span><br><span class="line">drwxr-xr-x. 2 root root  126 Jun 13 01:42 js</span><br><span class="line">[root@docker-4 dist]#</span><br></pre></td></tr></table></figure>

<h3 id="3-1-添加Nginx配置和Dockerfile"><a href="#3-1-添加Nginx配置和Dockerfile" class="headerlink" title="3.1 添加Nginx配置和Dockerfile"></a>3.1 添加Nginx配置和Dockerfile</h3><p>要将该网站host在Nginx上，只需要做两个步骤：  </p>
<ul>
<li>将dist里的文件复制到Nginx的&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html目录下</li>
<li>做一个conf配置，根目录&#x2F;指向&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html，并设置默认首页网页文件名index.html</li>
</ul>
<p>于是在网站下增加了一个default.conf文件：  </p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span>       <span class="number">80</span>;</span><br><span class="line">    <span class="attribute">server_name</span>  localhost;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">charset</span> utf-<span class="number">8</span>;</span><br><span class="line">    <span class="comment">#access_log  /var/log/nginx/log/host.access.log  main;</span></span><br><span class="line"></span><br><span class="line">    <span class="section">location</span> / &#123;</span><br><span class="line">        <span class="attribute">root</span>   /usr/share/nginx/html;</span><br><span class="line">        <span class="attribute">index</span>  index.html index.htm;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">#error_page  404              /404.html;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># redirect server error pages to the static page /50x.html</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="attribute">error_page</span>   <span class="number">500</span> <span class="number">502</span> <span class="number">503</span> <span class="number">504</span>  /50x.html;</span><br><span class="line">    <span class="section">location</span> = /50x.html &#123;</span><br><span class="line">        <span class="attribute">root</span>   /usr/share/nginx/html;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>另外增加了一个Dockerfile：  </p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> nginx:alpine</span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> default.conf /etc/nginx/conf.d/default.conf</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> ./dist /usr/share/nginx/html</span></span><br></pre></td></tr></table></figure>

<p>nginx:alpine是Nginx官方的轻量级镜像。  </p>
<h3 id="3-2-执行打包命令"><a href="#3-2-执行打包命令" class="headerlink" title="3.2 执行打包命令"></a>3.2 执行打包命令</h3><p>然后到Dockerfile所在目录构建镜像并推送到私有仓库：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker build -t 10.16.34.197:5000/staticsite:1.0 .</span><br><span class="line">docker push 10.16.34.197:5000/staticsite</span><br></pre></td></tr></table></figure>

<p>现在访问私有仓库的<a target="_blank" rel="noopener" href="http://10.16.34.197:5000/v2/_catalog">catalog目录</a>，就能看到repositories中已增加了一个静态网站的镜像：  </p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;repositories&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;staticsite&quot;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>也可以不指定镜像版本，那么打出来的版本就是latest：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t 10.16.34.197:5000/staticsite .</span><br></pre></td></tr></table></figure>

<p>可以通过如下的URL查看该镜像的所有可用版本：<br><a target="_blank" rel="noopener" href="http://10.16.34.197:5000/v2/staticsite/tags/list">http://10.16.34.197:5000/v2/staticsite/tags/list</a><br>（要访问其他镜像替换staticsite这个镜像名就可以了）  </p>
<h2 id="4-接下来的步骤"><a href="#4-接下来的步骤" class="headerlink" title="4. 接下来的步骤"></a>4. 接下来的步骤</h2><p>如果只用docker，那么执行如下的命令就结束了：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --name staticsite --publish 8080:80 10.16.34.197:5000/staticsite:1.0</span><br></pre></td></tr></table></figure>

<p>但到此为止的话，我们还是需要明确地指定在哪台机器上部署该容器。如果需要在多台机器上部署为高可用，就需要重复N遍同样的操作。为了保持高可用，还需要在部署容器的服务器前面额外部署负载均衡。<br>作为懒人，我很希望只要定义好需要的副本数量以及每个容器的硬件需求，就有人能替我把这些事情都包办了。万一哪个容器挂了还能迅速自动重启。<br>k8s就是这样的集群容器管家。我们下一章继续。  </p>
<p>PS. Private Registry也可以部署在k8s上（所以上一章的架构图中它也在虚线框内），但为了简化POC的流程，我们先把这个放在后面做。  </p>
<h2 id="5-使用Docker-Hub作为Docker-Registry"><a href="#5-使用Docker-Hub作为Docker-Registry" class="headerlink" title="5. 使用Docker Hub作为Docker Registry"></a>5. 使用Docker Hub作为Docker Registry</h2><p>自己Demo的时候也可以省掉这个步骤，直接将镜像上传到Docker Hub，然后K8S从Docker Hub上拉取镜像。<br>在注册Docker Hub账号之后，如果是Windows机器，需要打开Docker Desktop，然后输入<code>docker login</code>就可以直接登录了，不需要再输入密码。如果是Linux上，则需要在<code>docker login</code>命令后输入用户名和密码。<br>构建镜像和推送命令需要改为：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker build -t 账号/staticsite:1.0 .</span><br><span class="line">docker push 账号/staticsite</span><br></pre></td></tr></table></figure>

<p>在K8S集群里拉取镜像的步骤在下一章再介绍。  </p>
<p>更详细步骤可以参考<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-secret-in-the-cluster-that-holds-your-authorization-token">Pull an Image from a Private Registry - Kubernetes</a>  </p>
<h2 id="6-流程图"><a href="#6-流程图" class="headerlink" title="6. 流程图"></a>6. 流程图</h2><p>画了一下按角色划分的发布流程图。不管用的是k8s还是docker swarm，CI&#x2F;CD的流程还是基本类似的。<br><img src="/images/%E5%AE%B9%E5%99%A8-7-Kubernetes%E5%AE%9E%E6%88%98-%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E5%92%8C%E6%89%93%E5%8C%85%E9%95%9C%E5%83%8F/docker-cicd-process.jpg" alt="容器CI/CD流程">  </p>
<h2 id="7-参考资料"><a href="#7-参考资料" class="headerlink" title="7. 参考资料"></a>7. 参考资料</h2><p>这是本篇参考的一个最精简的静态网站的Docker配置<br><a target="_blank" rel="noopener" href="https://github.com/nishanttotla/DockerStaticSite">nishanttotla&#x2F;DockerStaticSite: A simple static website using Docker and Nginx</a>  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://galaxyyao.github.io/2019/06/18/%E5%AE%B9%E5%99%A8-7-Kubernetes%E5%AE%9E%E6%88%98-%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E5%92%8C%E6%89%93%E5%8C%85%E9%95%9C%E5%83%8F/" data-id="clzjpz1yj000o8v6ohp7q29q1" data-title="容器-7-Kubernetes实战-私有仓库和打包镜像" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/k8s/" rel="tag">k8s</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kubernetes/" rel="tag">kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8/" rel="tag">容器</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Nginx-替换response header中的Content-Disposition值" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/06/17/Nginx-%E6%9B%BF%E6%8D%A2response%20header%E4%B8%AD%E7%9A%84Content-Disposition%E5%80%BC/" class="article-date">
  <time class="dt-published" datetime="2019-06-16T16:00:00.000Z" itemprop="datePublished">2019-06-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/06/17/Nginx-%E6%9B%BF%E6%8D%A2response%20header%E4%B8%AD%E7%9A%84Content-Disposition%E5%80%BC/">Nginx-替换response header中的Content-Disposition值</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>我们有个需求要在打开合同PDF的时候，要将response的header里的Content-Disposition从  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">attachment;filename*=&quot;utf-8\&#x27; \&#x27;文件名&quot;</span><br></pre></td></tr></table></figure>
<p>改为  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inline;filename*=&quot;utf-8\&#x27; \&#x27;文件名&quot;</span><br></pre></td></tr></table></figure>
<p>这样文件就可以直接在浏览器里预览打开，而不是直接下载。<br>理论上最好的方式自然是从应用端解决。但我们提供文件的内容管理服务器不提供这个配置选项。虽然是开源软件，但我也不想为了这个修改源代码。除此之外，为了避免影响其他和文件相关的功能，减少回归测试量，我们也不想把全局修改这个header值。<br>那么剩下的办法就只有从Nginx反向代理层找解决方案了。理想的解决方案是对xxx.domain.com域名（内容管理服务器的域名），所有URL中带PDF关键字和“?inline&#x3D;1”参数的请求，修改header中Content-Disposition的值。（我们可以在前端请求的时候加?inline&#x3D;1这个path variable）<br>我模糊记得Nginx可以带if条件，所以原本估计就是个小case。事实证明我估计错得离谱【捂脸】。。。如果要直接看结论的请跳转到最后一节。  </p>
<h1 id="教训1：Nginx“基本”不支持if里多个条件"><a href="#教训1：Nginx“基本”不支持if里多个条件" class="headerlink" title="教训1：Nginx“基本”不支持if里多个条件"></a>教训1：Nginx“基本”不支持if里多个条件</h1><p>我先找到了一段匹配文件后缀的正则表达式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.*\.(后缀1|后缀2)$</span><br></pre></td></tr></table></figure>
<p>后缀替换成pdf后，就尝试写了如下的代码：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if ($request_filename ~* &quot;.*\.(pdf)&quot; &amp;&amp; $request_uri ~ &quot;(.*)inline=1&quot;) &#123;</span><br><span class="line">    # 修改header值</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然而很快我就发现，Nginx不支持if(condition1 &amp;&amp; condition2)的语法【捂脸】。。。<br>其实也有一些奇淫技巧可以实现AND和OR，比如<a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000002481473">这一篇</a>，通过拼字符串的方式：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">location = /test_and/ &#123;</span><br><span class="line">    default_type text/html;</span><br><span class="line">    set $a 0;</span><br><span class="line">    set $b 0;</span><br><span class="line">    if ( $remote_addr != &#x27;&#x27; )&#123;</span><br><span class="line">        set $a 1;</span><br><span class="line">    &#125;</span><br><span class="line">    if ( $http_x_forwarded_for != &#x27;&#x27; )&#123;</span><br><span class="line">        set $a 1$a;</span><br><span class="line">    &#125;</span><br><span class="line">    if ( $a = 11 )&#123;</span><br><span class="line">        set $b 1;</span><br><span class="line">    &#125;</span><br><span class="line">    echo $b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>根据Nginx企业官网的<a target="_blank" rel="noopener" href="https://www.nginx.com/resources/wiki/start/topics/depth/ifisevil/">一篇文章：If Is Evil</a>，平时应该尽量谨慎用if。<br>除此以外，Nginx中要实现if…else…的语法也需要费一番周折。这里就不详细展开了。  </p>
<h1 id="教训2：location不包含参数"><a href="#教训2：location不包含参数" class="headerlink" title="教训2：location不包含参数"></a>教训2：location不包含参数</h1><p>接下来尝试用正则表达式表现url中同时包含.pdf（不区分大小写）和“inline&#x3D;1”参数。<br>考虑到问号可能需要转义，就用.来替代。于是写了类似如下的正则表达式：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">location ~* &quot;.*\.(pdf).(inline=1)&quot;</span><br></pre></td></tr></table></figure>
<p>但结果发现死活匹配不到inline&#x3D;1的那段。反复尝试了多种正则表达式后，才想起来location不包含URI参数。。。<br>最终决定通过location匹配后缀，在location内用if匹配URI参数（inline&#x3D;1）：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">location ~* &quot;.*\.(pdf)$&quot; &#123;</span><br><span class="line">    # 省略其他</span><br><span class="line">    if ($args ~ inline=) &#123;</span><br><span class="line">        # 替换header值逻辑</span><br><span class="line">    &#125;</span><br><span class="line">    # proxy_pass逻辑</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="教训3：当location为正则表达式时，proxy-pass不能包含URI部分"><a href="#教训3：当location为正则表达式时，proxy-pass不能包含URI部分" class="headerlink" title="教训3：当location为正则表达式时，proxy_pass不能包含URI部分"></a>教训3：当location为正则表达式时，proxy_pass不能包含URI部分</h1><p>在写proxy_pass的时候，参考了“location &#x2F;”的那段逻辑，写成了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proxy_pass  http://docsvr/;</span><br></pre></td></tr></table></figure>
<p>nginx -s reload的时候报错：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@nginx-internal proxy]# nginx -s reload</span><br><span class="line">nginx: [emerg] &quot;proxy_pass&quot; cannot have URI part in location given by regular expression, or inside named location, or inside &quot;if&quot; statement, or inside &quot;limit_except&quot; block in /etc/nginx/conf.d/proxy/doc.conf:56</span><br></pre></td></tr></table></figure>
<p>查了之后才得知当location为正则表达式时，proxy_pass不能包含URI部分。在此处“&#x2F;”也是URI部分。所以去除了<a target="_blank" rel="noopener" href="http://docsvr/">http://docsvr/</a> 最后的斜杠，调整为：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">location ~* &quot;.*\.(pdf)$&quot; &#123;</span><br><span class="line">    # 省略其他</span><br><span class="line">    if ($args ~ inline=) &#123;</span><br><span class="line">        # 替换header值逻辑</span><br><span class="line">    &#125;</span><br><span class="line">    proxy_pass  http://docsvr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在location后使用~*是为了让后缀忽略大小写。  </p>
<h1 id="教训4：proxy-set-header不能包含在if语句中"><a href="#教训4：proxy-set-header不能包含在if语句中" class="headerlink" title="教训4：proxy_set_header不能包含在if语句中"></a>教训4：proxy_set_header不能包含在if语句中</h1><p>接下来就是要替换Content-Disposition值了。<br>我们先尝试将该值替换成其他任意值：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if ($args ~ inline=) &#123;</span><br><span class="line">    proxy_set_header  &#x27;Content-Disposition&#x27; &#x27;bbb&#x27;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后就在nginx -s reload的时候收到了报错：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nginx: [emerg] &quot;proxy_set_header&quot; directive is not allowed here in /etc/nginx/conf.d/proxy/doc.conf:32</span><br></pre></td></tr></table></figure>
<p>从这篇<a target="_blank" rel="noopener" href="https://agentzh.blogspot.com/2011/03/how-nginx-location-if-works.html">How nginx “location if” works</a>，我们可以知道Nginx实现if是通过一个嵌入的location。而不允许proxy_set_header很可能是因为嵌套的location不支持。<br>顺带提一句，除了proxy_set_header外，proxy_hide_header也不能包含在if语句中。  </p>
<p>看上去我们只能靠变量了。逻辑大概如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">set $is_inline_pdf 0</span><br><span class="line">set $content_disposition &#x27;attachment;filename*=&quot;utf-8\&#x27; \&#x27;attachement.pdf&quot;&#x27;;</span><br><span class="line">if ($args ~ inline=) &#123;</span><br><span class="line">    set $is_inline_pdf 1;</span><br><span class="line">    set $content_disposition &#x27;inline;filename*=&quot;utf-8\&#x27; \&#x27;inline.pdf&quot;&#x27;;</span><br><span class="line">&#125;</span><br><span class="line">proxy_set_header &#x27;Content-Disposition&#x27; $content_disposition;</span><br></pre></td></tr></table></figure>

<h1 id="教训5：proxy-set-header只能用来设置自定义header"><a href="#教训5：proxy-set-header只能用来设置自定义header" class="headerlink" title="教训5：proxy_set_header只能用来设置自定义header"></a>教训5：proxy_set_header只能用来设置自定义header</h1><p>上面那段配置测试后发现无效。事实上，不管proxy_set_header给Content-Disposition设置什么值都无效。<br>查询之后发现proxy_set_header可能只对自定义的header有效，但不能改非自定义的header。  </p>
<p>改用add_header替换proxy_set_header，会因为出现两个Content-Disposition而无法正常展现。在Chrome下会显示ERR_RESPONSE_HEADERS_MULTIPLE_CONTENT_DISPOSITION的报错。  </p>
<p>所以需要用proxy_hide_header + add_header，先隐藏后添加了。即：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">proxy_hide_header &#x27;Content-Disposition&#x27;;</span><br><span class="line">add_header &#x27;Content-Disposition&#x27; $content_disposition;</span><br></pre></td></tr></table></figure>

<h1 id="教训6：if语句内外的add-header不会同时生效"><a href="#教训6：if语句内外的add-header不会同时生效" class="headerlink" title="教训6：if语句内外的add_header不会同时生效"></a>教训6：if语句内外的add_header不会同时生效</h1><p>附带发现了一个很神奇的现象：当在命中if条件时，只有if条件内的add_header语句会执行。例如在下面的这个例子中：    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">add_header  &#x27;testa&#x27; &#x27;aaa&#x27;;</span><br><span class="line">if ($args ~ inline=) &#123;</span><br><span class="line">    add_header  &#x27;testb&#x27; &#x27;bbb&#x27;;</span><br><span class="line">&#125;</span><br><span class="line">add_header  &#x27;testc&#x27; &#x27;ccc&#x27;;</span><br></pre></td></tr></table></figure>
<p>按照我们其他语言中对if的理解，当符合条件($args ~ inline&#x3D;)这个条件时，应该是testa&#x2F;testb&#x2F;testc三个header都会显示。<br>但实际上，当符合($args ~ inline&#x3D;)这个条件时，只有testb这个header会显示；而如果不符合if条件时，testa和testc这两个header会显示。<br>原因应该也和<a target="_blank" rel="noopener" href="https://agentzh.blogspot.com/2011/03/how-nginx-location-if-works.html">How nginx “location if” works</a>这篇中介绍的原理有关。  </p>
<h1 id="最终成果"><a href="#最终成果" class="headerlink" title="最终成果"></a>最终成果</h1><p>最终语法如下：    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">set $is_inline_pdf 0;</span><br><span class="line">if ($args ~ inline=) &#123;</span><br><span class="line">    set $is_inline_pdf 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">proxy_hide_header &#x27;Content-Disposition&#x27;;</span><br><span class="line">if ($is_inline_pdf = 1) &#123;</span><br><span class="line">    add_header &#x27;Content-Disposition&#x27; &#x27;inline;filename*=&quot;utf-8\&#x27; \&#x27;inline.pdf&quot;&#x27;;</span><br><span class="line">    proxy_pass  http://docsvr;</span><br><span class="line">&#125;</span><br><span class="line">add_header &#x27;Content-Disposition&#x27; &#x27;attachment;filename*=&quot;utf-8\&#x27; \&#x27;attachement.pdf&quot;&#x27;;</span><br><span class="line"></span><br><span class="line">proxy_pass  http://docsvr;</span><br></pre></td></tr></table></figure>
<p>理论上要做的更好的话，可以用$request_filename或$request_uri中的文件名来替换Content-Disposition中的文件名。但实际发现Content-Disposition中的文件名不影响浏览器中显示，也不影响下载的文件名。而且要截取$request_filename中的filename所需要写的正则表达式有点变态，于是这个问题就先搁置不做优化了。  </p>
<p>最终的感想：Nginx对if的支持太有限了。。。应该是Nginx为了解析速度和性能所必要的代价吧。  </p>
<h1 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h1><p>在查资料的时候顺带查到一篇挺有意思的文章和一个挺有用的网站：  </p>
<p>通过正则表达式来DDOS还挺有创意。。。<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46294360">一个由正则表达式引发的血案（解决版）</a>  </p>
<p>看到知乎上尤雨溪推荐的JS正则可视化的工具，对理解复杂正则挺有帮助。<br><a target="_blank" rel="noopener" href="https://regexper.com/">Regexper</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://galaxyyao.github.io/2019/06/17/Nginx-%E6%9B%BF%E6%8D%A2response%20header%E4%B8%AD%E7%9A%84Content-Disposition%E5%80%BC/" data-id="clzjpz1yj000k8v6o2vxn4ypj" data-title="Nginx-替换response header中的Content-Disposition值" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Nginx/" rel="tag">Nginx</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-容器-6-Kubernetes实战-POC目标" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/06/13/%E5%AE%B9%E5%99%A8-6-Kubernetes%E5%AE%9E%E6%88%98-POC%E7%9B%AE%E6%A0%87/" class="article-date">
  <time class="dt-published" datetime="2019-06-12T16:00:00.000Z" itemprop="datePublished">2019-06-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/06/13/%E5%AE%B9%E5%99%A8-6-Kubernetes%E5%AE%9E%E6%88%98-POC%E7%9B%AE%E6%A0%87/">容器-6-Kubernetes实战-POC目标</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在较为彻底地理解了Docker原理后，我曾经天真地以为再花差不多的时间就可以同样掌握k8s。直到我看到一张k8s的核心概念关系图：<br><img src="/images/%E5%AE%B9%E5%99%A8-6-Kubernetes%E5%AE%9E%E6%88%98-POC%E7%9B%AE%E6%A0%87/Kubernetes-core-concept-relation.png" alt="Kubernetes核心概念关系"><br>Docker只是其中粉红色的那一小块（Container）的一部分。。。  </p>
<p>我也曾考虑过按照我的理解总结k8s优于Docker Swarm之处，以及为什么k8s能赢下容器编排大战。但很快发现这意味着我还需要先去熟悉Docker Swarm，才能较为准确地进行分析。这又何苦呢？我们已经知道了结论：Kubernetes是容器编排之战的最终战胜者。就算Docker Swarm有再多的优点，我们也不会采用。  </p>
<p>我还考虑过类似Docker系列那样，先从原理开始整理。但k8s涉及的原理范围更广，从各种存储介质到OSI七层原理，先研究透再写的话估计还得花一个月。  </p>
<p>所以我们跳过枯燥的原理介绍，先进行最有趣的实战环节，做一个包含接近完整功能的POC（Proof of Concept，概念验证）。在POC的过程中，我们再来逐渐熟悉k8s的架构设计和原理。  </p>
<h2 id="POC目标"><a href="#POC目标" class="headerlink" title="POC目标"></a>POC目标</h2><p>我们的目标是把现在基于虚拟机的部署方式和基于脚本的打包方式，尝试用Docker容器+Kubernetes实现。<br>部署的时候还需要考虑：  </p>
<ul>
<li>服务高可用</li>
<li>节点扩展与收缩</li>
<li>安全性</li>
<li>与代码版本管理平台（Gitlab）和持续集成系统（Gitlab CI&#x2F;Jenkins）的整合</li>
<li>多版本应用维护</li>
<li>日志收集</li>
<li>监控</li>
<li>数据备份</li>
<li>…其他等等</li>
</ul>
<p>下面是一个我们当前应用架构的精简版：  </p>
<ul>
<li>Spring Boot（Java）应用作为后端，暴露接口地址供集群外的App调用。应用高可用部署。  </li>
<li>Nginx上同时host了WebPack打包的静态网站。静态网站也会调用Java应用的接口。Nginx高可用部署。  </li>
<li>Spring Boot应用也同时调用后台的MySQL数据库。MySQL数据库的物理部署架构为Master-Slave形式。Master写入，Slave读取。MySQL的数据库进行每日的全量备份到指定外部硬件存储上。  </li>
<li>Java应用的日志汇总到Elasticsearch中存储，并可通过kibana查看。</li>
<li>网络上，集群外可以访问静态网站，Java应用的接口，kibana页面。可以通过工具查询MySQL中的数据。</li>
</ul>
<p><img src="/images/%E5%AE%B9%E5%99%A8-6-Kubernetes%E5%AE%9E%E6%88%98-POC%E7%9B%AE%E6%A0%87/k8s-poc-target.jpg" alt="POC目标架构">  </p>
<p>POC相对实际部署架构的调整：  </p>
<ul>
<li>暂时省略了Node.JS后台应用。  </li>
<li>数据库暂时省略了MongoDB，并用MySQL替换了Oracle。  </li>
<li>暂时省略了Spring Cloud的注册中心和配置中心。  </li>
<li>代码版本管理平台&#x2F;持续集成系统&#x2F;Maven私服等服务暂时不部署在集群内，使用外部已有的实例。  </li>
<li>暂时省略了Redis缓存和消息队列（RabbitMQ）。  </li>
<li>暂时省略了ETL（Kettle）。</li>
</ul>
<p>使用<a href="/2019/05/29/%E5%AE%B9%E5%99%A8-5-kubeadm%E9%83%A8%E7%BD%B2Kubernetes1-14-2%E9%9B%86%E7%BE%A4%E8%B8%A9%E5%9D%91%E8%AE%B0">上一章</a>中搭建完成的单主k8s集群进行部署。  </p>
<h2 id="POC中的一个原则"><a href="#POC中的一个原则" class="headerlink" title="POC中的一个原则"></a>POC中的一个原则</h2><p>我们在POC及后续实际使用Kubernetes的过程中需要保持一个原则：<strong>只通过YAML修改Kubernetes对象</strong>。<br>换句话说，我们尽量避免像以往的运维那样，直接进入虚拟机进行操作。这样的操作是无法记录，不透明的。有可能我们下一次想部署同样的环境的时候遗漏了某个步骤，导致最终部署失败。即使我们写了操作手册也不能确保文字表达不会产生歧义。而YAML定义是透明且不会产生歧义的，且可以通过版本控制追溯历史的。  </p>
<p>这也是我为什么会相信Kubernetes是未来的趋势的原因之一：以前的运维经验像口耳相传的秘笈，而未来基于Kubernetes的运维就是使用YAML编程。零散的运维知识点通过Kubernetes被整合成了体系化的知识。配合封装了底层各种优化的公有云，培训出一个合格运维的成本会大大降低。<br>以前有些中小公司中，只有某个老资格的运维对服务器了如指掌，即使工作态度很差也不敢换人。但Kubernetes化运维时代，只需要部署的YAML配置在，交接时间会大大缩短。（当然网络、底层硬件和存储之类的依然需要不少时间交接。另外还有无法Kubernetes化的Windows服务器）<br>Serverless和FaaS（Function as a Serivce）技术最近也火热发展中。或许以后都不需要运维了，开发只需要直接向云Kubernetes直接提交业务函数即可。不过目前这些技术还处于探索阶段。<br>这对运维来说不能说是一个利好消息。但不跟上这个潮流的话，就只有等着被历史的车轮碾过淘汰。  </p>
<p>当然上述只是我理想中的情况。现实永远是一个泥潭，会逼得我们做各种dirty workaround。但只要愿景是美好的，我们终能一步步接近。  </p>
<p>好了，我们开始吧。  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://galaxyyao.github.io/2019/06/13/%E5%AE%B9%E5%99%A8-6-Kubernetes%E5%AE%9E%E6%88%98-POC%E7%9B%AE%E6%A0%87/" data-id="clzjpz1yj000j8v6odfl809za" data-title="容器-6-Kubernetes实战-POC目标" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/k8s/" rel="tag">k8s</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kubernetes/" rel="tag">kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8/" rel="tag">容器</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-容器-5-kubeadm部署Kubernetes1-14-2集群踩坑记" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/05/29/%E5%AE%B9%E5%99%A8-5-kubeadm%E9%83%A8%E7%BD%B2Kubernetes1-14-2%E9%9B%86%E7%BE%A4%E8%B8%A9%E5%9D%91%E8%AE%B0/" class="article-date">
  <time class="dt-published" datetime="2019-05-28T16:00:00.000Z" itemprop="datePublished">2019-05-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/05/29/%E5%AE%B9%E5%99%A8-5-kubeadm%E9%83%A8%E7%BD%B2Kubernetes1-14-2%E9%9B%86%E7%BE%A4%E8%B8%A9%E5%9D%91%E8%AE%B0/">容器-5-kubeadm部署Kubernetes1.14.2集群踩坑记</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>一般情况下我不喜欢把部署手册放到blog里。绝大多数情况下官网已经足够详尽，而且blog很可能因为版本陈旧误人子弟。曾经我写过Nginx的二进制部署手册，早就被轻松愉快的yum安装扫进了废纸堆。而使用了Docker和K8s后yum安装方式也被迅速淘汰。Hadoop的部署也被Cloudera全自动化部署替代了。<br>但kubernetes的部署由于涉及科学上网的问题，把原本几个命令就能解决的问题搞得相当复杂。所以希望这篇也能多少对还在被GFW恶心的人有些帮助。（当然可能更简单的方式是部署在墙外，比如AWS上）  </p>
<h2 id="0-部署目标和硬件准备"><a href="#0-部署目标和硬件准备" class="headerlink" title="0. 部署目标和硬件准备"></a>0. 部署目标和硬件准备</h2><h3 id="0-1-部署目标"><a href="#0-1-部署目标" class="headerlink" title="0.1 部署目标"></a>0.1 部署目标</h3><p>由于是测试目的，就不部署高可用了。高可用的部署可以参见最后的参考资料。<br>物理拓扑结构是1 Master + 3 Worker（Worker数量可轻松扩展）。  </p>
<h3 id="0-2-硬件准备"><a href="#0-2-硬件准备" class="headerlink" title="0.2 硬件准备"></a>0.2 硬件准备</h3><p>部署的Kubernetes版本是v1.14.2（截止2019&#x2F;5&#x2F;29的最新版本），Docker的版本是Docker CE 18.09.6（也是截止2019&#x2F;5&#x2F;29的最新版本）。<br>服务器全是VMWare虚拟机。虚机的硬件和操作系统如下：  </p>
<table>
<thead>
<tr>
<th>HOSTNAME</th>
<th>ip</th>
<th>ROLES</th>
<th>硬件配置</th>
<th>操作系统</th>
</tr>
</thead>
<tbody><tr>
<td>docker-4</td>
<td>10.16.34.54</td>
<td>master</td>
<td>4核CPU&#x2F;8GB内存&#x2F;100GB硬盘</td>
<td>CentOS 7.4</td>
</tr>
<tr>
<td>docker-5</td>
<td>10.16.34.57</td>
<td>worker</td>
<td>4核CPU&#x2F;8GB内存&#x2F;100GB硬盘</td>
<td>CentOS 7.4</td>
</tr>
<tr>
<td>docker-6</td>
<td>10.16.34.58</td>
<td>worker</td>
<td>4核CPU&#x2F;8GB内存&#x2F;100GB硬盘</td>
<td>CentOS 7.4</td>
</tr>
<tr>
<td>docker-7</td>
<td>10.16.34.59</td>
<td>worker</td>
<td>4核CPU&#x2F;8GB内存&#x2F;100GB硬盘</td>
<td>CentOS 7.4</td>
</tr>
</tbody></table>
<p>下面所有的命令都是在root账号下执行的。  </p>
<h2 id="1-检查和配置操作系统"><a href="#1-检查和配置操作系统" class="headerlink" title="1. 检查和配置操作系统"></a>1. 检查和配置操作系统</h2><h3 id="1-1-检查操作系统-硬件配置-网络连通性"><a href="#1-1-检查操作系统-硬件配置-网络连通性" class="headerlink" title="1.1 检查操作系统&#x2F;硬件配置&#x2F;网络连通性"></a>1.1 检查操作系统&#x2F;硬件配置&#x2F;网络连通性</h3><p>按照<a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/setup/independent/install-kubeadm/">安装 kubeadm - Kubernetes</a>检查操作系统&#x2F;硬件配置&#x2F;网络连通性。主要检查节点之中不可以有重复的主机名，MAC 地址，product_uuid。  </p>
<h3 id="1-2-配置hostname"><a href="#1-2-配置hostname" class="headerlink" title="1.2 配置hostname"></a>1.2 配置hostname</h3><p>根据官方文档的<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/independent/troubleshooting-kubeadm/">kubeadm问题排查</a>，需要确保hostname -i命令返回可路由的ip。<br>我拿到的虚机默认只会返回127.0.0.1。这个可能导致了后续配置过程中Worker节点在join后一直NotReady的问题。所以以防万一还是在每个节点上配置一下比较保险。  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/hosts</span><br></pre></td></tr></table></figure>

<p>添加内容：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">10.16.34.54     docker-4</span><br><span class="line">10.16.34.57     docker-5</span><br><span class="line">10.16.34.58     docker-6</span><br><span class="line">10.16.34.59     docker-7</span><br></pre></td></tr></table></figure>

<p>然后重启network  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart network</span><br></pre></td></tr></table></figure>

<h3 id="1-3-禁用swap"><a href="#1-3-禁用swap" class="headerlink" title="1.3 禁用swap"></a>1.3 禁用swap</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo swapoff -a</span><br><span class="line">sudo sed -i &#x27;/ swap / s/^\(.*\)$/#\1/g&#x27; /etc/fstab</span><br></pre></td></tr></table></figure>

<p>kubelet在swap不禁用的情况下会报错：  </p>
<blockquote>
<p>kubelet[2856]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set –fail-swap-on</p>
</blockquote>
<p>K8S这么设计的原因主要是性能考量：Kubernetes会把每个node实例尽量压榨到利用率100%，包括CPU和内存。而swap出来的虚拟内存的性能远比不上真实内存，会影响调度器对机器余力的判断。  </p>
<h3 id="1-4-禁用selinux"><a href="#1-4-禁用selinux" class="headerlink" title="1.4 禁用selinux"></a>1.4 禁用selinux</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将 SELinux 设置为 permissive 模式(将其禁用)</span></span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &#x27;s/^SELINUX=enforcing$/SELINUX=permissive/&#x27; /etc/selinux/config</span><br></pre></td></tr></table></figure>

<p>禁用SELinux是因为kubelet还不支持。不然容器访问不了宿主机的文件系统，也就没法使用Pod网络。  </p>
<h3 id="1-5-RHEL-CentOS7相关iptables配置"><a href="#1-5-RHEL-CentOS7相关iptables配置" class="headerlink" title="1.5 RHEL&#x2F;CentOS7相关iptables配置"></a>1.5 RHEL&#x2F;CentOS7相关iptables配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">vm.swappiness=0</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br></pre></td></tr></table></figure>

<h3 id="1-6-开启端口"><a href="#1-6-开启端口" class="headerlink" title="1.6 开启端口"></a>1.6 开启端口</h3><p>理论上需要开这些端口：<br><strong>Master 节点</strong>  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo firewall-cmd --zone=public --permanent --add-port=6443/tcp</span><br><span class="line">sudo firewall-cmd --zone=public --permanent --add-port=2379-2380/tcp</span><br><span class="line">sudo firewall-cmd --zone=public --permanent --add-port=10250/tcp</span><br><span class="line">sudo firewall-cmd --zone=public --permanent --add-port=10251/tcp</span><br><span class="line">sudo firewall-cmd --zone=public --permanent --add-port=10252/tcp</span><br><span class="line">sudo firewall-cmd --reload</span><br></pre></td></tr></table></figure>

<p><strong>Worker 节点</strong>  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo firewall-cmd --zone=public --permanent --add-port=10250/tcp</span><br><span class="line">sudo firewall-cmd --zone=public --permanent --add-port=30000-32767/tcp</span><br><span class="line">sudo firewall-cmd --reload</span><br></pre></td></tr></table></figure>

<p>不过对于测试环境来说，为了以防未知的坑，还是直接关闭掉防火墙比较直接。之后在部署Rook的时候，apply -f operator.yaml后Pod的状态一直为CrashLoopBackOff或Error。<br>查看Event日志得到了如下的错误信息：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">State:       Waiting</span><br><span class="line">  Reason:    CrashLoopBackOff</span><br><span class="line">Last State:  Terminated</span><br><span class="line">  Reason:    Error</span><br><span class="line">  Message:   failed to get pod. Get https://10.96.0.1:443/api/v1/namespaces/rook-ceph/pods/rook-ceph-operator-765ff54667-njkn6: dial tcp 10.96.0.1:443: connect: no route to host</span><br></pre></td></tr></table></figure>
<p>通过kubectl get svc -n&#x3D;kube-system 命令查询service，发现kube-dns还需要开启53&#x2F;UDP,53&#x2F;TCP,9153&#x2F;TCP这三个端口。kubernetes-dashboard也需要443端口。<br>在关闭防火墙后，rook-ceph部署成功。<br>综上所述，将本步骤修改为：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<h2 id="2-安装容器运行时-CRI-Docker"><a href="#2-安装容器运行时-CRI-Docker" class="headerlink" title="2. 安装容器运行时(CRI)-Docker"></a>2. 安装容器运行时(CRI)-Docker</h2><h3 id="2-1-安装Docker"><a href="#2-1-安装Docker" class="headerlink" title="2.1 安装Docker"></a>2.1 安装Docker</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">yum -y install yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum -y install wget</span><br><span class="line">cd /etc/yum.repos.d/</span><br><span class="line">wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line">yum clean all</span><br><span class="line">yum -y install docker-ce</span><br></pre></td></tr></table></figure>
<h3 id="2-2-启动Docker服务"><a href="#2-2-启动Docker服务" class="headerlink" title="2.2 启动Docker服务"></a>2.2 启动Docker服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable docker</span><br><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure>

<h2 id="3-安装Kubernetes"><a href="#3-安装Kubernetes" class="headerlink" title="3. 安装Kubernetes"></a>3. 安装Kubernetes</h2><p>需要在每台机器上都安装以下的软件包：  </p>
<ul>
<li>kubeadm: 用来初始化集群的指令</li>
<li>kubelet: 在集群中的每个节点上用来启动pod和container等</li>
<li>kubectl: 用来与集群通信的命令行工具</li>
</ul>
<h3 id="3-1-准备repo"><a href="#3-1-准备repo" class="headerlink" title="3.1 准备repo"></a>3.1 准备repo</h3><p>这里开始和科学上网有关了。要把repo地址里的packages.cloud.google.com都替换成很阿里云的域名mirrors.aliyun.com&#x2F;kubernetes。<br>gpgcheck可以保留为1，不过这里以防万一我改为了不check。  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line">yum clean all</span><br></pre></td></tr></table></figure>

<h3 id="3-2-开始安装kubelet-kubeadm-kubectl"><a href="#3-2-开始安装kubelet-kubeadm-kubectl" class="headerlink" title="3.2 开始安装kubelet&#x2F;kubeadm&#x2F;kubectl"></a>3.2 开始安装kubelet&#x2F;kubeadm&#x2F;kubectl</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install kubelet kubeadm kubectl --disableexcludes=kubernetes</span><br></pre></td></tr></table></figure>
<p>有些blog里提到还需要yum install kubernetes-cni。实际发现执行完上面的命令已经安装好了。大概最新版的kubeadm已经包含了kubernetes-cni。<br>有些部署手册里依赖的是比较早版本的Kubernetes，可以在安装的时候指定版本：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install kubelet=1.11.3-00</span><br><span class="line">yum install kubectl=1.11.3-00</span><br><span class="line">yum install kubeadm=1.11.3-00</span><br></pre></td></tr></table></figure>

<h3 id="3-3-启动kubelet服务"><a href="#3-3-启动kubelet服务" class="headerlink" title="3.3 启动kubelet服务"></a>3.3 启动kubelet服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kubelet &amp;&amp; systemctl start kubelet</span><br></pre></td></tr></table></figure>
<p>由于上一个步骤里yum安装的时候没有指定版本，这时候就可以通过kubectl version查到yum安装的Kubernetes版本。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-4 ~]# kubectl version</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;14&quot;, GitVersion:&quot;v1.14.2&quot;, GitCommit:&quot;66049e3b21efe110454d67df4fa62b08ea79a19b&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-05-16T16:23:09Z&quot;, GoVersion:&quot;go1.12.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;</span><br><span class="line">The connection to the server localhost:8080 was refused - did you specify the right host or port?</span><br></pre></td></tr></table></figure>
<p>connection refused的报错信息可以先无视。  </p>
<h3 id="3-4-在Master节点上创建kubeadm-init的配置文件kubeadm-yaml"><a href="#3-4-在Master节点上创建kubeadm-init的配置文件kubeadm-yaml" class="headerlink" title="3.4 在Master节点上创建kubeadm init的配置文件kubeadm.yaml"></a>3.4 在Master节点上创建kubeadm init的配置文件kubeadm.yaml</h3><p>可以把kubernetes的YAML配置文件放在任何路径下。我这里是放到root的HOME目录&#x2F;root&#x2F;下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd ~</span><br></pre></td></tr></table></figure>

<p>然后创建一份kubeadm init的配置文件kubeadm.yaml如下：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">controllerManager:</span><br><span class="line">    extraArgs:</span><br><span class="line">        horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot;</span><br><span class="line">        horizontal-pod-autoscaler-sync-period: &quot;10s&quot;</span><br><span class="line">        node-monitor-grace-period: &quot;10s&quot;</span><br><span class="line">apiServer:</span><br><span class="line">    extraArgs:</span><br><span class="line">        runtime-config: &quot;api/all=true&quot;</span><br><span class="line">kubernetesVersion: &quot;stable-1.14&quot;</span><br></pre></td></tr></table></figure>

<p>对于旧版本（例如1.11），apiVersion是kubeadm.k8s.io&#x2F;v1alpha1：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1alpha1</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">controllerManagerExtraArgs:</span><br><span class="line">  horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot;</span><br><span class="line">  horizontal-pod-autoscaler-sync-period: &quot;10s&quot;</span><br><span class="line">  node-monitor-grace-period: &quot;10s&quot;</span><br><span class="line">apiServerExtraArgs:</span><br><span class="line">  runtime-config: &quot;api/all=true&quot;</span><br><span class="line">kubernetesVersion: &quot;stable-1.11&quot;</span><br></pre></td></tr></table></figure>

<h3 id="3-5-确定拉取的镜像版本"><a href="#3-5-确定拉取的镜像版本" class="headerlink" title="3.5 确定拉取的镜像版本"></a>3.5 确定拉取的镜像版本</h3><p>如果服务器在墙外，那么就可以kubeadm init –config kubeadm.yaml，然后去泡杯茶慢慢等着了。但如果不是的话，你会看到如下的错误：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">error execution phase preflight: [preflight] Some fatal errors occurred:</span><br><span class="line">        [ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-apiserver:v1.14.2: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">, error: exit status 1</span><br><span class="line">        [ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-controller-manager:v1.14.2: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">, error: exit status 1</span><br><span class="line">        [ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-scheduler:v1.14.2: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">, error: exit status 1</span><br><span class="line">        [ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-proxy:v1.14.2: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">, error: exit status 1</span><br><span class="line">        [ERROR ImagePull]: failed to pull image k8s.gcr.io/pause:3.1: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">, error: exit status 1</span><br><span class="line">        [ERROR ImagePull]: failed to pull image k8s.gcr.io/etcd:3.3.10: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">, error: exit status 1</span><br><span class="line">        [ERROR ImagePull]: failed to pull image k8s.gcr.io/coredns:1.3.1: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">, error: exit status 1</span><br><span class="line">[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`</span><br></pre></td></tr></table></figure>
<p>原因就是国内连不上gcr.io。  </p>
<p>如果你清楚知道kubeadm init使用的每个镜像的版本，那么你可以直接去按下一节的步骤去拉取镜像。<br>但如果你不确定的话，还是先执行一遍kubeadm init命令，从错误信息里获取当前版本Kubernetes使用的各镜像的版本，以便下一节的pullimages.sh脚本中指定。  </p>
<p>kubeadm需要的镜像包括：kube-proxy&#x2F;kube-scheduler&#x2F;kube-controller-manager&#x2F;kube-apiserver&#x2F;etcd&#x2F;coredns&#x2F;pause。<br>对于v1.14.2，具体版本如下：<br>kube-proxy:v1.14.2 kube-scheduler:v1.14.2 kube-controller-manager:v1.14.2 kube-apiserver:v1.14.2 etcd:3.3.10 coredns:1.3.1 pause:3.1</p>
<h2 id="3-6-拉取镜像"><a href="#3-6-拉取镜像" class="headerlink" title="3.6 拉取镜像"></a>3.6 拉取镜像</h2><p>这个步骤是最麻烦的。<br>如上一节所示，直接pull的话会失败。网上大多数文章中推荐docker hub上的一个个人的镜像站：<a target="_blank" rel="noopener" href="https://github.com/anjia0532/gcr.io_mirror">anjia0532&#x2F;gcr.io_mirror</a>:。但这个镜像站已经被Travis CI标记为疑似滥用，所以最新的几个版本都没有同步了。  </p>
<p>所以现在推荐使用的是Azure中国的镜像站。就是对于从k8s.gcr.io拉取的docker pull命令，从gcr.azk8s.cn&#x2F;google-containers拉取。<br>举个具体的例子。比如要拉取kubernetes dashboard v1.10.1，原本的命令为：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1</span><br></pre></td></tr></table></figure>
<p>现在改为：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull gcr.azk8s.cn/google-containers/kubernetes-dashboard-amd64:v1.10.1</span><br></pre></td></tr></table></figure>
<p>然后还可以打一个标记，覆盖k8s.gcr.io的同名镜像。  </p>
<p>对于kubeadm需要的镜像，可以通过如下的脚本一次性获取</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~</span><br><span class="line">vi pullimages.sh</span><br></pre></td></tr></table></figure>
<p>添加内容：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">images=(kube-proxy:v1.14.2 kube-scheduler:v1.14.2 kube-controller-manager:v1.14.2 kube-apiserver:v1.14.2 etcd:3.3.10 coredns:1.3.1 pause:3.1 )</span><br><span class="line">for imageName in $&#123;images[@]&#125; ; do</span><br><span class="line">docker pull gcr.azk8s.cn/google-containers/$imageName</span><br><span class="line">docker tag gcr.azk8s.cn/google-containers/$imageName k8s.gcr.io/$imageName</span><br><span class="line">docker rmi gcr.azk8s.cn/google-containers/$imageName</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<p>（最后一句rmi的意义暂时没搞懂，为啥最后要把Azure的镜像删除掉。。。但的确能work，所以姑且按照网上的脚本来）  </p>
<p>执行脚本：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x pullimages.sh</span><br><span class="line">./pullimages.sh</span><br></pre></td></tr></table></figure>

<p>不太确定的一点是要不要在所有的Worker Node上都执行pullimages.sh。如果遇到Worker Node一直是NotReady的话，可以在服务器上也执行一下。  </p>
<p>PS. 也可以用阿里云的镜像，例如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1</span><br></pre></td></tr></table></figure>

<h3 id="3-7-在Master节点上执行kubeadm-init"><a href="#3-7-在Master节点上执行kubeadm-init" class="headerlink" title="3.7 在Master节点上执行kubeadm init"></a>3.7 在Master节点上执行kubeadm init</h3><p>先配置停用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sysconfig/kubelet</span><br></pre></td></tr></table></figure>
<p>将内容修改为：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBELET_EXTRA_ARGS=&quot;--fail-swap-on=false&quot;</span><br></pre></td></tr></table></figure>

<p>然后就可以执行kubeadm init命令了。具体执行时间看网速，我这里大概总共3分钟。  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --config kubeadm.yaml</span><br></pre></td></tr></table></figure>
<p>如果成功的话会显示如下内容：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 10.16.34.54:6443 --token hfzcd2.xhqca62fjjbmq7xh \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:29a90fa653aaffd384259867c02e046a7b81a354838059f97f2053533faacbd9 </span><br></pre></td></tr></table></figure>
<p>然后按照提示在Master节点上执行剩下的命令：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure>
<p>保存好那条kubeadm join的命令。注意只有这个token只有当天有效。隔了24小时之后就需要kubectl create token重新创建token。  </p>
<h3 id="3-8-部署网络插件Weave"><a href="#3-8-部署网络插件Weave" class="headerlink" title="3.8 部署网络插件Weave"></a>3.8 部署网络插件Weave</h3><p>在进行接下来的步骤之前先不要急，要先确认所有的node状态和pod状态。<br>依次检查健康状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get cs</span><br></pre></td></tr></table></figure>
<p>节点状态  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get nodes</span><br></pre></td></tr></table></figure>
<p>如果是测试用的单节点部署，需要运行以下命令，去掉master节点的污点：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes --all node-role.kubernetes.io/master-</span><br></pre></td></tr></table></figure>

<p>系统Pod状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -n kube-system</span><br></pre></td></tr></table></figure>
<p>当前由于没有部署网络插件，所以coredns的Pod的状态还是Pending。  </p>
<p>确保除了coredns之外的Pod都是running后，部署Weave插件：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://git.io/weave-kube-1.6</span><br></pre></td></tr></table></figure>
<p>通过如下命令，等待Weave的Pod也正常running状态后，才能继续后续的kubeadm join操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -n kube-system</span><br></pre></td></tr></table></figure>

<h3 id="3-9-Worker节点加入"><a href="#3-9-Worker节点加入" class="headerlink" title="3.9 Worker节点加入"></a>3.9 Worker节点加入</h3><p>在每个Worker节点上执行1.1到3.3，以及3.6步骤后，执行join命令：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 10.16.34.54:6443 --token hfzcd2.xhqca62fjjbmq7xh \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:29a90fa653aaffd384259867c02e046a7b81a354838059f97f2053533faacbd9 </span><br></pre></td></tr></table></figure>
<p>在Master上观察各节点状态，直到全部Ready。  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get nodes</span><br></pre></td></tr></table></figure>

<h3 id="3-10-设置Worker角色"><a href="#3-10-设置Worker角色" class="headerlink" title="3.10 设置Worker角色"></a>3.10 设置Worker角色</h3><p>通过kubeadm join加入的节点的默认角色为none，需要再标记为worker：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl label node docker-5 node-role.kubernetes.io/worker=worker</span><br><span class="line">kubectl label node docker-6 node-role.kubernetes.io/worker=worker</span><br><span class="line">kubectl label node docker-7 node-role.kubernetes.io/worker=worker</span><br></pre></td></tr></table></figure>

<p>最终节点状态：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-4 ~]# kubectl get nodes</span><br><span class="line">NAME       STATUS   ROLES    AGE   VERSION</span><br><span class="line">docker-4   Ready    master   24h   v1.14.2</span><br><span class="line">docker-5   Ready    worker   24h   v1.14.2</span><br><span class="line">docker-6   Ready    worker   24h   v1.14.2</span><br><span class="line">docker-7   Ready    worker   24h   v1.14.2</span><br></pre></td></tr></table></figure>
<p>最终系统Pod状态：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-4 ~]# kubectl get pods -n kube-system</span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-fb8b8dccf-5h9kk                 1/1     Running   0          24h</span><br><span class="line">coredns-fb8b8dccf-p6kh8                 1/1     Running   0          24h</span><br><span class="line">etcd-docker-4                           1/1     Running   0          24h</span><br><span class="line">kube-apiserver-docker-4                 1/1     Running   0          24h</span><br><span class="line">kube-controller-manager-docker-4        1/1     Running   0          24h</span><br><span class="line">kube-proxy-7xfbp                        1/1     Running   0          24h</span><br><span class="line">kube-proxy-dw4l5                        1/1     Running   0          24h</span><br><span class="line">kube-proxy-lhmrq                        1/1     Running   0          24h</span><br><span class="line">kube-proxy-zmhql                        1/1     Running   0          24h</span><br><span class="line">kube-scheduler-docker-4                 1/1     Running   0          24h</span><br><span class="line">weave-net-g2w9p                         2/2     Running   1          24h</span><br><span class="line">weave-net-hh6p2                         2/2     Running   1          24h</span><br><span class="line">weave-net-qgk82                         2/2     Running   0          24h</span><br><span class="line">weave-net-vgdnf                         2/2     Running   0          24h</span><br></pre></td></tr></table></figure>
<p>有些时候状态没Ready不要急，先泡杯茶去。有些操作要花一些时间的。包括之后kubectl的一些操作，拍下回车后有时候会没有UI反馈内容。如果这个时候没有耐心地Ctrl+C中止，可能产生一些不可知的后遗症。在apply多个yaml的时候，也最好在每个步骤结束后确认全部的Pod状态是Running，再进行下一个步骤。<br>如果泡完茶依然有问题，再按照下一章的排查步骤来排查。  </p>
<h3 id="3-11-验证"><a href="#3-11-验证" class="headerlink" title="3.11 验证"></a>3.11 验证</h3><p>可以通过部署一个Nginx的Pod来进行验证。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~</span><br><span class="line">vi nginx-deployment.yaml</span><br></pre></td></tr></table></figure>
<p>输入内容：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.14.2</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure>
<p>然后执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nginx-deployment.yaml</span><br></pre></td></tr></table></figure>
<p>最后验证Pod状态：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-4 ~]# kubectl get pods</span><br><span class="line">NAME                                READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-deployment-5cbdcb76f7-8shbf   1/1     Running   0          17h</span><br><span class="line">nginx-deployment-5cbdcb76f7-g2gkt   1/1     Running   0          17h</span><br></pre></td></tr></table></figure>

<p>验证完如果不需要的话可以删除：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f nginx-deployment.yaml</span><br></pre></td></tr></table></figure>

<h2 id="4-问题排查"><a href="#4-问题排查" class="headerlink" title="4. 问题排查"></a>4. 问题排查</h2><p>安装的过程肯定不可能一帆风顺。知道怎么排查很重要。  </p>
<h3 id="4-1-排查节点问题"><a href="#4-1-排查节点问题" class="headerlink" title="4.1 排查节点问题"></a>4.1 排查节点问题</h3><p>如果怀疑是节点问题，可以通过如下的方式来查看节点状态：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe nodes</span><br></pre></td></tr></table></figure>
<p>重点看Conditions下的Message。  </p>
<h3 id="4-2-排查Pod"><a href="#4-2-排查Pod" class="headerlink" title="4.2 排查Pod"></a>4.2 排查Pod</h3><p>如果是系统Pod，可以通过如下命令首先查看Pod状态：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -n kube-system</span><br></pre></td></tr></table></figure>
<p>然后describe节点查看</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;Pod名&gt; -n kube-system</span><br></pre></td></tr></table></figure>

<p>如果是普通Pod，就把命令最后的-n kube-system去掉：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure>
<p>然后describe节点查看</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;Pod名&gt;</span><br></pre></td></tr></table></figure>

<p>重点都是看最后的Events。  </p>
<h3 id="4-3-其他日志"><a href="#4-3-其他日志" class="headerlink" title="4.3 其他日志"></a>4.3 其他日志</h3><p>有些时候Events比较简略，就需要查看日志。特别如果问题是在Worker Node上，没法执行kubectl命令，只能查看日志。<br>查看方式有两种：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -l -u kubelet</span><br></pre></td></tr></table></figure>
<p>或者  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail -f /var/log/messages</span><br></pre></td></tr></table></figure>
<p>还可以通过grep来缩小排查范围。  </p>
<h3 id="4-4-重置状态"><a href="#4-4-重置状态" class="headerlink" title="4.4 重置状态"></a>4.4 重置状态</h3><p>在尝试解决网络插件问题的时候，我曾经病急乱投医地装了个flannel。但Pod状态始终处于ContainerCreating状态。后来Weave恢复后尝试通过kubectl delete命令删除flannel，遇到了Pod一直terminating但删除不掉的症状。雪上加霜的是还出现了硬件的告警：“kernel:NMI watchdog: BUG: soft lockup - CPU#1 stuck for 22s”。尝试reboot服务器居然发生了超时：“Failed to start reboot.target: Connection timed out”。<br>万策已尽，只能请运维直接干掉虚机重装了。换了台机器继续装Master节点。但几个Worker Node已经join了原Master。这时候就要靠reset命令重置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm reset</span><br></pre></td></tr></table></figure>
<p>需要注意reset完可能需要执行以下命令：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /proc/sys/net/ipv4/ip_forward</span><br></pre></td></tr></table></figure>
<p>要不然可能会遇到以下的报错信息：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1</span><br></pre></td></tr></table></figure>
<p>Master节点如果不是到我遇到的这个情况也可以reset。  </p>
<h2 id="5-尚未确认的问题"><a href="#5-尚未确认的问题" class="headerlink" title="5. 尚未确认的问题"></a>5. 尚未确认的问题</h2><p>下面是一些我遇到过的问题。在解决过程中进行了不少操作，不太确定到底是其中具体哪个操作起到了决定性作用。所以姑且把我做过的事情都记录一下。  </p>
<h3 id="runtime-network-not-ready-NetworkReady-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized"><a href="#runtime-network-not-ready-NetworkReady-false-reason-NetworkPluginNotReady-message-docker-network-plugin-is-not-ready-cni-config-uninitialized" class="headerlink" title="runtime network not ready: NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized"></a>runtime network not ready: NetworkReady&#x3D;false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized</h3><p>在Worker节点加入后一直显示NotReady。查看node状态，在message里看到了如上的消息。<br>尝试过关闭防火墙，怀疑过虚拟机网卡问题。<br>怀疑可能有两个措施可能最终产生效果： </p>
<ul>
<li>按照1.2配置&#x2F;etc&#x2F;hosts</li>
<li>在每个Worker Node上也执行pullimages.sh拉取镜像</li>
</ul>
<p>极客时间的评论里有人因为多网卡而失败过，也摘抄一下备忘吧。  </p>
<blockquote>
<p>2、卡在多网卡的问题上。<br>  2.1、我的环境是virtual box上虚拟的两个ubuntu，网络设置为nat+host only，集群搭建好之后，死活无法启动dashboard、ceph的容器（好多老外也是这么弄的啊），各种查各种试，也没解决问题。在kubernetes的官网上只说了“If you have more than one network adapter, and your Kubernetes components are not reachable on the default route, we recommend you add IP route(s) so Kubernetes cluster addresses go via the appropriate adapter.”。哪位大神按照这种方式弄好的清指点下，很是困惑啊啊啊啊，谁能解救我下………………<br>  2.2、放弃了2.1的nat+host only，改为了桥接的网络方式，只保留一个network interface，成功。</p>
</blockquote>
<h3 id="rpc-error-code-DeadlineExceeded异常，导致Pod持续处于ContainerCreating状态"><a href="#rpc-error-code-DeadlineExceeded异常，导致Pod持续处于ContainerCreating状态" class="headerlink" title="rpc error: code &#x3D; DeadlineExceeded异常，导致Pod持续处于ContainerCreating状态"></a>rpc error: code &#x3D; DeadlineExceeded异常，导致Pod持续处于ContainerCreating状态</h3><p>在部署完后发生过所有Node都已经Ready，但apply的Pod（包括系统插件的kubernetes dashboard和自定义的nginx）一直处于ContainerCreating状态的情况。<br>在Worker上看到的日志中报rpc error: code &#x3D; DeadlineExceeded：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">May 29 01:51:10 docker-7 kubelet: E0529 01:51:10.545968   21276 kuberuntime_manager.go:693] createPodSandbox for pod &quot;kubernetes-dashboard-5f7b999d65-5jwpl_kube-system(a3e6ab24-81d4-11e9-935a-00505695705b)&quot; failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded</span><br><span class="line">May 29 01:51:10 docker-7 kubelet: E0529 01:51:10.546252   21276 pod_workers.go:190] Error syncing pod a3e6ab24-81d4-11e9-935a-00505695705b (&quot;kubernetes-dashboard-5f7b999d65-5jwpl_kube-system(a3e6ab24-81d4-11e9-935a-00505695705b)&quot;), skipping: failed to &quot;CreatePodSandbox&quot; for &quot;kubernetes-dashboard-5f7b999d65-5jwpl_kube-system(a3e6ab24-81d4-11e9-935a-00505695705b)&quot; with CreatePodSandboxError: &quot;CreatePodSandbox for pod \&quot;kubernetes-dashboard-5f7b999d65-5jwpl_kube-system(a3e6ab24-81d4-11e9-935a-00505695705b)\&quot; failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;</span><br></pre></td></tr></table></figure>
<p>这个问题网上信息非常有限。采取了很多措施，最后也不知道哪个起效了。怀疑是又执行了一遍“1.5 RHEL&#x2F;CentOS7相关iptables配置”产生了效果。  </p>
<h3 id="CrashLoopBackOff状态"><a href="#CrashLoopBackOff状态" class="headerlink" title="CrashLoopBackOff状态"></a>CrashLoopBackOff状态</h3><p>装Minikube的时候还遇到过core-dns一直处于CrashLoopBackOff状态，也记录一下：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-1 ~]# kubectl get pods --all-namespaces</span><br><span class="line">NAMESPACE     NAME                               READY   STATUS             RESTARTS   AGE</span><br><span class="line">kube-system   coredns-59ffb8b4c-vtj5r            0/1     CrashLoopBackOff   20         78m</span><br><span class="line">kube-system   coredns-59ffb8b4c-xj47w            0/1     CrashLoopBackOff   20         78m</span><br><span class="line">kube-system   coredns-d5947d4b-g9hrd             0/1     CrashLoopBackOff   21         83m</span><br></pre></td></tr></table></figure>
<p>message如下：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error restarting cluster: wait: waiting for component=kube-apiserver: timed out waiting for the condition</span><br></pre></td></tr></table></figure>
<p>靠停用防火墙后删除重装minikube解决了。暂时不确定是否和防火墙有关。  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">minikube stop</span><br><span class="line">minikube delete</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">systemctl reboot</span><br><span class="line">minikube start --vm-driver=none --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers</span><br></pre></td></tr></table></figure>

<p>最终感谢阿里云提供repo的镜像，微软Azure云提供Docker镜像。<br>F*ck GFW  </p>
<h2 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6. 参考资料"></a>6. 参考资料</h2><p>如果是个人学习目的的话，Minikube就已经够用了，安装比上述步骤简单不少。当然科学上网的问题还是要解决。<br><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/tools/install-minikube/">Install Minikube - Kubernetes</a>  </p>
<p>这篇是基于极客时间课程的搭建步骤，也是相对比较完整的。<br><a target="_blank" rel="noopener" href="https://www.datayang.com/article/45">centos7快速搭建Kubernetes 1.11.1单机集群-data羊</a>  </p>
<p>官方文档的CRI和kubeadm安装手册。如果服务器在墙外直接照着操作就行。<br><a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/setup/independent/install-kubeadm/">安装 kubeadm - Kubernetes</a><br><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/cri/#docker">CRI installation - Kubernetes</a>  </p>
<p>kubeadm高可用部署的官方文档。<br><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/independent/high-availability/">Creating Highly Available Clusters with kubeadm - Kubernetes</a></p>
<p>如果没有访问外网的话可以参考这篇。但我部署的时候真不想遇到这种情况。。。<br><a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#%E5%9C%A8%E6%B2%A1%E6%9C%89%E4%BA%92%E8%81%94%E7%BD%91%E8%BF%9E%E6%8E%A5%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E8%BF%90%E8%A1%8C-kubeadm">kubeadm init - 在没有互联网连接的情况下运行 kubeadm</a>  </p>
<p>补充一个Ansible部署K8S的开源项目（尚未试用）<br><a target="_blank" rel="noopener" href="https://github.com/easzlab/kubeasz">easzlab&#x2F;kubeasz: 使用Ansible脚本安装K8S集群，介绍组件交互原理，方便直接，不受国内网络环境影响</a>  </p>
<p>还有一个部署生产级别K8S的开源项目（尚未试用）<br><a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/kubespray">kubernetes-sigs&#x2F;kubespray: Deploy a Production Ready Kubernetes Cluster</a>  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://galaxyyao.github.io/2019/05/29/%E5%AE%B9%E5%99%A8-5-kubeadm%E9%83%A8%E7%BD%B2Kubernetes1-14-2%E9%9B%86%E7%BE%A4%E8%B8%A9%E5%9D%91%E8%AE%B0/" data-id="clzjpz1yi000g8v6od0uxflte" data-title="容器-5-kubeadm部署Kubernetes1.14.2集群踩坑记" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/k8s/" rel="tag">k8s</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kubernetes/" rel="tag">kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8/" rel="tag">容器</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-容器-4-Docker的意义" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/05/27/%E5%AE%B9%E5%99%A8-4-Docker%E7%9A%84%E6%84%8F%E4%B9%89/" class="article-date">
  <time class="dt-published" datetime="2019-05-26T16:00:00.000Z" itemprop="datePublished">2019-05-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/05/27/%E5%AE%B9%E5%99%A8-4-Docker%E7%9A%84%E6%84%8F%E4%B9%89/">容器-4-Docker的意义</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在介绍完Docker的原理后，我们再回过头来看Docker的意义。</p>
<h1 id="Docker的意义"><a href="#Docker的意义" class="headerlink" title="Docker的意义"></a>Docker的意义</h1><p>事实上，Cgroups是2007年就被合并到Linux内核的功能。那个时候结合了Cgroups的资源管理能力+Linux Namespace的视图隔离能力的LXC（Linux Container）就已经产生了。<br>但LXC的视角还是操作系统和服务器，目标是打造一个相比虚拟机更轻量级的系统容器。<br>而Docker从理念上就截然不同，将目标中心转到了应用。<br><img src="/images/%E5%AE%B9%E5%99%A8-4-Docker%E7%9A%84%E6%84%8F%E4%B9%89/lxc-vs-docker.jpg" alt="LXC vs Docker">  </p>
<p>不要小看这个理念上的差异。以应用为中心意味着两件事情的彻底改变：  </p>
<h3 id="构建和部署"><a href="#构建和部署" class="headerlink" title="构建和部署"></a>构建和部署</h3><p>测试环境和生产环境的构建和部署都是以应用为粒度的。而一个操作系统上可能部署着不同测试阶段的应用。不可能那么凑巧让上面所有的应用恰好同一周期完毕，然后将整个操作系统从测试搬到生产。这个不符合正常的开发测试流程。<br>而Docker直接将一个应用运行所需的完整环境，即整个操作系统的文件系统也打包了进去。只要这个应用的Docker镜像测试完成，就可以单独发布上生产。还可以利用现有的构建工具来辅助，例如Jenkins&#x2F;Ansible等。遇到性能瓶颈需要横向扩展时，也可以针对单应用迅速部署启动。在性能压力消除后也可以快速回收。<br>在Docker之前，也已经有Cloud Foundry等PaaS项目开始以应用为中心。但相比做完一个镜像就可以随处运行的Docker，它们的便利性和适应性差了不少。</p>
<h3 id="版本化和共享"><a href="#版本化和共享" class="headerlink" title="版本化和共享"></a>版本化和共享</h3><p>制作一个操作系统容器是一件私有的事情。你制作的操作系统容器一般只会使用在你自己的团队，顶多扩展到全公司内部。别人看不到你是具体怎么做的系统容器，不清楚你是否在里面埋了雷。借我十个胆子也不敢用。<br>而Docker在容器镜像的制作上引入了“层（Layer）”的概念。这种基于“层”的实现借鉴了Git的思想，使容器的创建变得透明。每个人都可以审查应用容器在原始操作系统的容器上做了哪些修改。<br>类似在Github上开源代码，当每个人和每个公司都可以参与到全世界的应用容器分发过程中时，Docker的爆发也在情理之中了。<br><img src="/images/%E5%AE%B9%E5%99%A8-4-Docker%E7%9A%84%E6%84%8F%E4%B9%89/docker-hub.png" alt="Docker hub">  </p>
<h1 id="Docker对于开发人员的意义"><a href="#Docker对于开发人员的意义" class="headerlink" title="Docker对于开发人员的意义"></a>Docker对于开发人员的意义</h1><p>曾经我也觉得Docker是运维的事情。对开发来说，吃鸡蛋难道还要管鸡蛋是怎么下的么？但在现在微服务化趋势越来越明显的现在，使用Docker也会带来开发上的很大优势：<br>之前我们开发和SIT测试环境的基础组件和微服务基本是公用的。这会带来一系列问题：  </p>
<ul>
<li>A为了某个开发中的特性改动了数据库某个字段，导致其他开发和测试环境直接崩溃</li>
<li>A始终收不到消息队列中的消息，最后发现是B本地启动的应用把消息给消费掉了</li>
<li>CI&#x2F;CD被触发导致X应用自动重新部署，但A开发中的功能依赖于X应用，于是只能等自动部署完毕后才能继续开发<br>而Docker可以使公用的中间件&#x2F;数据库&#x2F;微服务在本地按需启动。每个人独享自己的开发环境，不再受到其他开发人员和测试环境的影响。</li>
</ul>
<p>此外Docker也进一步降低了生产部署的风险和时间。“我本地运行正常啊”这样的问题出现的概率会降低。也可以将上线的时间进一步压缩（虽然我们现在的一键部署脚本基本也可以一分钟内完成打包发布了），使开发同学能按时回家吃饭。  </p>
<p>Docker也大大降低了尝试新技术和新软件的成本。<br>我现在手头虚拟机资源还算相对充沛，如果想搭个jenkins，gitlab或区块链玩玩，申请几台新的虚机就行。但当初我也饱尝过没有机器可供随便玩的受限感。即使好不容易搞到一台虚机，还要研究个半天怎么安装。安装的时候谨慎再谨慎，就怕不小心装错了搞坏了操作系统，还要陪着笑麻烦运维删掉虚机重装。<br>而现在大部分技术都提供了镜像，本地一句docker run命令，就可以直接开始体验了。玩坏了删除容器重新来一遍。完全没有任何心理负担。  </p>
<p>即使只是对个人接个项目赚赚外快，Docker也带来不少便利。当你本地开发调试完，需要部署到客户本地。选项一是跑到客户现场，或远程到客户内网，手动装一堆环境依赖，可能还会遇到信息安全的限制。选项二是让客户自己运行pull + run两个命令，分分钟部署好。怎么选择毫无悬念。  </p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>极客时间的这篇专栏非常之推荐，讲得非常通俗易懂循序渐进，绝对值回票价。看到第10章搭建kubeadm的时候可能会卡一下，不过结合实际环境操作演练一下就可以跨过去了。<br><a target="_blank" rel="noopener" href="https://time.geekbang.org/column/intro/116">深入剖析Kubernetes</a>  </p>
<h1 id="Docker全系列"><a href="#Docker全系列" class="headerlink" title="Docker全系列"></a>Docker全系列</h1><p><a href="/2019/05/17/%E5%AE%B9%E5%99%A8-1-Namespace%EF%BC%9A%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C">Namespace：楚门的世界</a><br><a href="/2019/05/24/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E">Docker存储引擎</a><br><a href="/2019/05/25/%E5%AE%B9%E5%99%A8-3-Cgroups%E7%9A%84%E8%AE%A1%E5%88%92%E7%BB%8F%E6%B5%8E">Cgroups的计划经济</a><br><a href="/2019/05/27/%E5%AE%B9%E5%99%A8-4-Docker%E7%9A%84%E6%84%8F%E4%B9%89">Docker的意义</a>  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://galaxyyao.github.io/2019/05/27/%E5%AE%B9%E5%99%A8-4-Docker%E7%9A%84%E6%84%8F%E4%B9%89/" data-id="clzjpz1yi000c8v6obghe0oeu" data-title="容器-4-Docker的意义" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/docker/" rel="tag">docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8/" rel="tag">容器</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-容器-3-Cgroups的计划经济" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/05/25/%E5%AE%B9%E5%99%A8-3-Cgroups%E7%9A%84%E8%AE%A1%E5%88%92%E7%BB%8F%E6%B5%8E/" class="article-date">
  <time class="dt-published" datetime="2019-05-24T16:00:00.000Z" itemprop="datePublished">2019-05-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/05/25/%E5%AE%B9%E5%99%A8-3-Cgroups%E7%9A%84%E8%AE%A1%E5%88%92%E7%BB%8F%E6%B5%8E/">容器-3-Cgroups的计划经济</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Cgroups-限制可用资源"><a href="#Cgroups-限制可用资源" class="headerlink" title="Cgroups-限制可用资源"></a>Cgroups-限制可用资源</h1><p>如果应用优化得不够好，直接把CPU或内存吃光也完全不是新鲜事。但我们肯定不能容忍容器无限制地挤占宿主机的资源，甚至把宿主机搞down掉。<br>所以我们可以通过Linux的Cgroups（Control Groups，控制组）对某个容器可以使用的各种硬件资源设置配额。</p>
<p><img src="/images/%E5%AE%B9%E5%99%A8-3-Cgroups%E7%9A%84%E8%AE%A1%E5%88%92%E7%BB%8F%E6%B5%8E/docker-cgroups.png" alt="cgroup">  </p>
<p>根据<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cgroups">wiki</a>，Cgroups的功能包括：</p>
<ul>
<li>资源限制（Resource Limitation）：进程组使用的内存的上限，也包括文件系统与物理内存交换用的页缓存（Page Cache）</li>
<li>优先级分配（Prioritization）：控制分配的CPU时间片数量及硬盘IO吞吐，实际上就相当于控制了进程运行的优先级</li>
<li>资源统计（Accounting）：统计资源使用量，如CPU使用时长、内存用量等，主要用于计费</li>
<li>进程控制（Control）：执行挂起、恢复进程组的操作</li>
</ul>
<p>Cgroups的设置一般分为三个主要步骤：</p>
<ol>
<li>创建cgroup</li>
<li>设置cgroup配额（将cpu&#x2F;内存等各种子系统添加到该cgroup中）</li>
<li>将进程添加为cgroup的任务</li>
</ol>
<p>Docker容器在启动时候会动态创建Cgroup，并在容器终止的时候删除。  </p>
<p>容器的Cgroups相比虚拟机的优势主要在两方面：</p>
<ul>
<li>资源利用率</li>
<li>性能损耗<br>首先从资源利用率方面来说，一部分虚拟化技术只能静态分配资源，一台物理机上也装不了几个虚拟机。如果虚拟机闲置，分配给虚拟机的硬件资源也无法分配给其他虚拟机。也有一部分虚拟化技术可以实现一定程度上的“动态分配”。但这种“动态分配”有各种各样的缺陷，比如回收速度慢（Balloon技术），影响性能（内存压缩），或即慢又影响性能（透明页共享）。<br>虚拟化技术也有很大的性能损耗。例如为了虚拟CPU，Hypervisor需要为每个虚拟的CPU创建一个数据结构，以模拟CPU的寄存器；为了虚拟内存，需要通过一个shadow page table，在物理内存和虚拟机内存之间增加一层虚拟的物理内存。而且操作系统本身的资源损耗是无论如何无法避免的。</li>
</ul>
<p>相比之下，Cgroups虽然也会带来一些性能损耗。但通过一些<a target="_blank" rel="noopener" href="https://www.kankanzhijian.com/2018/07/15/loss_in_docker/">测试</a>可以发现，相比虚拟机<a target="_blank" rel="noopener" href="https://www.linuxidc.com/Linux/2016-05/130991.htm">接近50%</a>的损耗，容器的性能损耗微乎其微（CPU密集场景下是5%）。此外Docker还节省了操作系统运行的资源损耗。此外，容器在启动时间上也有巨大的优势。  </p>
<p>当然Cgroups也有自己的问题。比如高压力下容器与容器之间，以及容器和操作系统之间抢占资源的问题。有兴趣的话可以参考<a target="_blank" rel="noopener" href="https://engineering.linkedin.com/blog/2016/08/don_t-let-linux-control-groups-uncontrolled">这篇</a>，或者自己做个实验试一下。  </p>
<h1 id="Docker原理总结"><a href="#Docker原理总结" class="headerlink" title="Docker原理总结"></a>Docker原理总结</h1><p>从以上的介绍你可以看到，其实Docker engine并没有使用什么特别深奥的原理。甚至你可以通过shell脚本自己来实现一个docker engine（事实上github上就有这么一个<a target="_blank" rel="noopener" href="https://github.com/p8952/bocker">开源项目</a>，使用100行bash实现了精简版的docker）。所以事实上真正Docker的架构类似下图：<br><img src="/images/%E5%AE%B9%E5%99%A8-3-Cgroups%E7%9A%84%E8%AE%A1%E5%88%92%E7%BB%8F%E6%B5%8E/docker-vs-vm-true.jpg" alt="容器-虚拟机vs容器（真）">  </p>
<h1 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h1><h3 id="建议在物理机上还是虚拟机上运行Docker？"><a href="#建议在物理机上还是虚拟机上运行Docker？" class="headerlink" title="建议在物理机上还是虚拟机上运行Docker？"></a>建议在物理机上还是虚拟机上运行Docker？</h3><p>当我要开始搭容器测试环境的时候，最纠结的其实是到底该搭在虚拟机上还是物理机上。万一搞错了还要铲掉重来。<br>按照我的经验，先到官方文档里找最佳实践，但翻了半天也没找到。<br>在了解了原理后，自然会得出结论：物理机上部署Docker的性能约等于直接在物理机上部署应用。所以物理机上部署肯定有性能和延迟优势。<br>不过Docker的<a target="_blank" rel="noopener" href="https://blog.docker.com/2018/08/containers-replacing-virtual-machines/">博客</a>上也提到了虚拟机化的几个好处：</p>
<ul>
<li>方便上云</li>
<li>可以利用成熟且已有的虚拟化经验，例如：灾备，监控和自动化</li>
<li>节省虚拟机License</li>
</ul>
<p>国内也有人测试过具体的性能差异，IO和CPU方面物理机快25%-30%。<br><a target="_blank" rel="noopener" href="http://www.ciotimes.com/cloud/153978.html">运行Docker：物理机vs虚拟机，五方面详细对比理</a></p>
<p>在我们的场景下，性能远不及其他几个优点重要。于是最终还是选择了虚拟机上部署。<br><img src="/images/%E5%AE%B9%E5%99%A8-3-Cgroups%E7%9A%84%E8%AE%A1%E5%88%92%E7%BB%8F%E6%B5%8E/docker-on-vm.png" alt="容器-虚拟机上部署docker">  </p>
<h3 id="容器的操作系统是否能和宿主机不一样？"><a href="#容器的操作系统是否能和宿主机不一样？" class="headerlink" title="容器的操作系统是否能和宿主机不一样？"></a>容器的操作系统是否能和宿主机不一样？</h3><p>可以分成三种情况讨论：</p>
<ul>
<li>容器的操作系统版本比宿主机高或低</li>
<li>操作系统是不同发行版本的（例如宿主机是CentOS，容器是Ubuntu）</li>
<li>操作系统完全不同（例如宿主机是Windows，容器是CentOS）</li>
</ul>
<p>其实1和2都差不多。不同版本和发行版本的Linux内核的差别不那么大。容器只与主机共享一个内核。<br>操作系统&#x3D;内核+文件系统&#x2F;库<br>镜像&#x3D;文件系统&#x2F;库<br>这也是为什么不能在Linux宿主机上运行Windows容器。内核根本就不一样。</p>
<p>但为什么我们可以在Windows 10上运行容器？这多亏了Hyper-V。（回想起来Hyper-V最初发布的时候还是在微软内部看到消息并试用的。。。）<br>Hyper-V的技术细节就不多提了。与Docker相关的可以参见这张架构图：<br><img src="/images/%E5%AE%B9%E5%99%A8-3-Cgroups%E7%9A%84%E8%AE%A1%E5%88%92%E7%BB%8F%E6%B5%8E/docker-on-windows.png" alt="容器-虚拟机上部署docker"><br>与之原理类似，MacOS上运行Docker是通过虚拟化技术xhyve或者virtualbox来实现。  </p>
<p>更多可以参见这篇：<br><a target="_blank" rel="noopener" href="http://www.floydhilton.com/docker/2017/03/31/Docker-ContainerHost-vs-ContainerOS-Linux-Windows.html">Understanding Docker “Container Host” vs. “Container OS” for Linux and Windows Containers</a>  </p>
<h1 id="Docker全系列"><a href="#Docker全系列" class="headerlink" title="Docker全系列"></a>Docker全系列</h1><p><a href="/2019/05/17/%E5%AE%B9%E5%99%A8-1-Namespace%EF%BC%9A%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C">Namespace：楚门的世界</a><br><a href="/2019/05/24/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E">Docker存储引擎</a><br><a href="/2019/05/25/%E5%AE%B9%E5%99%A8-3-Cgroups%E7%9A%84%E8%AE%A1%E5%88%92%E7%BB%8F%E6%B5%8E">Cgroups的计划经济</a><br><a href="/2019/05/27/%E5%AE%B9%E5%99%A8-4-Docker%E7%9A%84%E6%84%8F%E4%B9%89">Docker的意义</a>  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://galaxyyao.github.io/2019/05/25/%E5%AE%B9%E5%99%A8-3-Cgroups%E7%9A%84%E8%AE%A1%E5%88%92%E7%BB%8F%E6%B5%8E/" data-id="clzjpz1yh000b8v6o86v13lfs" data-title="容器-3-Cgroups的计划经济" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/docker/" rel="tag">docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8/" rel="tag">容器</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-容器-2-Docker存储引擎" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/05/24/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/" class="article-date">
  <time class="dt-published" datetime="2019-05-23T16:00:00.000Z" itemprop="datePublished">2019-05-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/05/24/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/">容器-2-Docker存储引擎</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>容器与容器间的文件系统必须相互隔离。如果一个容器能不受限制地访问到宿主机或另一个容器里的文件，那必定会引起严重的安全风险。所以对于docker中的进程，必须限制其能够访问的文件系统。<br>我们先来看一种简易版的实现方式：chroot。  </p>
<h1 id="chroot-监狱"><a href="#chroot-监狱" class="headerlink" title="chroot-监狱"></a>chroot-监狱</h1><p>chroot，即change root的缩写。它是一个 UNIX 操作系统上的系统调用，用于将一个进程及其子进程的根目录改变到文件系统中的一个新位置。<br>我们知道root根目录（&#x2F;）是Linux的顶层目录。这里的顶层，换句话说就是没有办法访问比根目录更高一层级的目录。但对每个进程，可以通过chroot来“欺骗”，将指定的目录骗他们认定为根目录。  </p>
<p>chroot经常和一个单词结合在一起说：jail（监狱）。可以很形象地理解为chroot就是给每个docker容器划了一个监狱房间。<br>Docker在每个监狱里配套放置了一套文件系统。  </p>
<p><img src="/images/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/docker-chroot.jpg" alt="chroot">  </p>
<p>chroot的基本语法如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将某个进程</span></span><br><span class="line">chroot /path/to/new/root command</span><br></pre></td></tr></table></figure>

<p>chroot看起来挺不错，但也存在两个问题：  </p>
<ul>
<li>监狱里需要备齐所有需要的文件，有几个容器就需要备几份。</li>
<li>有办法可以越狱。方式之一就是在chroot里运行chroot。<br>事实上Docker存储引擎之一的VFS就是每个容器的存储完全独立（有时会在排查问题的时候使用），所以空间占用最大。<br>但我们总希望能对空间利用能进一步优化。</li>
</ul>
<h1 id="Docker镜像原理"><a href="#Docker镜像原理" class="headerlink" title="Docker镜像原理"></a>Docker镜像原理</h1><h3 id="写时复制"><a href="#写时复制" class="headerlink" title="写时复制"></a>写时复制</h3><p>Linux刚启动的时候会加载bootfs。当boot成功，kernel被加载到内存之后，bootfs就被umount了。我们平时能看到的&#x2F;bin，&#x2F;lib等目录是rootfs，处于bootfs上一层。<br><img src="/images/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/docker-rootfs.png" alt="rootfs">  </p>
<p>假设我们有两个centos容器。对于这两个容器来说，像&#x2F;bin，&#x2F;lib等rootfs目录里的内容完全一致，同时保存两份会浪费存储空间。那么我们是否可以让两个容器共享这些相同的文件？<br>我们知道Linux里有软链接的概念。可以在不同的目录里创建软链接，指向同一个实际文件&#x2F;目录。但软链接方案有一个很显而易见的问题：改动会互相影响。我们无法接受在一个容器里修改了文件之后影响到另一个容器。<br>很自然，我们就会考虑做一个只读的基准版本。每个容器对这个只读版本的修改分别独立保存。<br>这个只读的基准版本就是镜像，可修改的就是容器。<br><img src="/images/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/docker-image-1.png" alt="镜像与容器">  </p>
<p>Docker存储引擎对文件的修改使用到了写时复制（COW，Copy on Write）技术。即当要对镜像中某个文件进行写操作时，将文件复制到文件系统中，对副本进行修改，而不会对image里的源文件进行修改。多个容器操作同一个文件的时候会创建多个副本。  </p>
<p>写时复制使得同一台机器上可以部署的Docker容器数量远超过可以部署的虚拟机数量：<br>假设一个虚拟机的大小是10GB，那么创建并启动10个虚拟机需要多少空间？<br>视使用的虚拟技术而定，可能是100GB，可能小于100GB。但假设操作系统占了2GB，容量不会小于20GB。<br>假设一个容器镜像的大小是10GB，那么创建并启动10个容器需要多少空间？<br>依然只需要10GB。  </p>
<h3 id="Union-Mount"><a href="#Union-Mount" class="headerlink" title="Union Mount"></a>Union Mount</h3><p>假设原始镜像中有一个文件a.txt。当要修改该文件的内容时，通过“写时复制”，创建出来了一份a.txt的副本。我们希望最终在容器中查看文件系统时，除了a.txt之外的所有文件都读取镜像，只有a.txt这个文件读取副本。这依赖的是Union Mount（联合挂载）技术。<br>我们已Docker存储引擎之一：OverlayFS为例，看一下联合挂载的特征。  </p>
<p><img src="/images/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/overlay-constructs.jpg" alt="镜像与容器">  </p>
<p>如上图所示，在OverlayFS中存在Lower和Upper两个层次。Lower层就是镜像层，Upper层就是容器层。最终的效果类似从正上方俯视，Upper中的文件会覆盖Lower层中的文件。<br>我们以下四种情况，看一下不同的文件系统操作的实际原理：</p>
<ul>
<li>创建文件&#x2F;目录：在Upper层中直接创建</li>
<li>修改文件&#x2F;目录：从Lower层复制到Upper层，然后在Upper层中修改</li>
<li>删除文件&#x2F;目录：在Upper层创建一个Whiteout文件&#x2F;目录。Whiteout文件&#x2F;目录在Merge后不显示。</li>
<li>删除目录后创建同名目录：Upper层新目录为opaque目录，屏蔽Lower层目录和里面的文件</li>
</ul>
<h3 id="Layer"><a href="#Layer" class="headerlink" title="Layer"></a>Layer</h3><p>假设我们现在有一批基于centos镜像的容器，这些容器里都需要装JDK。<br>当然我们可以在每个容器的Upper层里都包含一份JDK文件。但既然每个容器里的JDK文件都一样，那么是不是可以考虑把JDK也做成共通的？  </p>
<p>有一种方案是把centos和JDK放在一起打包成一个独立的镜像。但接下来你就会面对组合的极速膨胀，比如：</p>
<ul>
<li>centos + JDK</li>
<li>centos + python 2</li>
<li>centos + python 3</li>
<li>centos + JDK + tomcat</li>
<li>centos + apache</li>
<li>…<br>光是镜像的大小就会变得很可观。</li>
</ul>
<p>我们上一节提到过overlay层 &#x3D; 容器层 + 镜像层，那么自然可以联想到，这个模式也可以扩展成：overlay层 &#x3D; 容器层 + JDK镜像层 + centos镜像层<br>推而广之：</p>
<ul>
<li>centos + JDK + tomcat &#x3D; 容器层 + tomcat镜像层 + JDK镜像层 + centos镜像层</li>
<li>centos + python 3 &#x3D; 容器层 + python 3层 + centos镜像层</li>
<li>…<br>这样就只需要维护一份centos的镜像文件，以及基于centos镜像的增量改动即可。<br><img src="/images/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/docker-image-2.png" alt="镜像与容器"></li>
</ul>
<p>容器层的改动也可以通过docker commit固化成镜像。于是只要把应用打包成镜像，分发到任何服务器上docker run，就达到了“一次打包，到处运行”的效果。  </p>
<p>我认为这个设计还有一个顺带的好处：下载镜像的时候可以并发下载多个层，加快镜像的下载速度。每一层下载完后自行解压缩。下面是下载Gitlab镜像的实时状态：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">latest: Pulling from gitlab/gitlab-ce</span><br><span class="line">9ff7e2e5f967: Downloading [==================================&gt;                ]   30.1MB/43.77MB</span><br><span class="line">59856638ac9f: Download complete </span><br><span class="line">6f317d6d954b: Download complete </span><br><span class="line">a9dde5e2a643: Download complete </span><br><span class="line">23e292690057: Downloading [==========&gt;                                        ]  5.717MB/26.26MB</span><br><span class="line">49ec625ca43f: Download complete </span><br><span class="line">15260c60bf0e: Download complete </span><br><span class="line">b62bd915894c: Waiting </span><br><span class="line">8dc41372b526: Waiting </span><br><span class="line">8d1e09653c32: Waiting </span><br></pre></td></tr></table></figure>

<h1 id="Docker存储引擎的选择"><a href="#Docker存储引擎的选择" class="headerlink" title="Docker存储引擎的选择"></a>Docker存储引擎的选择</h1><p>Docker的存储引擎有多种实现方式，并还在不断改进中。除了上文中提到的VFS和OverlayFS外，还有Device Mapper&#x2F;AUFS&#x2F;Overlay2等。<br>当前最新版本推荐使用Overlay2。<br>对于Overlay2引擎，镜像保存在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2路径下，镜像和层的元数据保存在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;image&#x2F;overlay2&#x2F;路径下。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>一个便于深入理解chroot效果的动手实验<br><a target="_blank" rel="noopener" href="https://linux.cn/article-3068-1.html">技术|Linux &#x2F; Unix：chroot 命令实例讲解</a>  </p>
<p>容器Layer相关的图来自于这个博客<br><a target="_blank" rel="noopener" href="http://tiewei.github.io/cloud/Docker-Getting-Start/">Docker Getting Start: Related Knowledge</a>  </p>
<p>OverlayFS的一些动手试验<br><a target="_blank" rel="noopener" href="https://blog.programster.org/overlayfs">OverlayFS | Programster’s Blog</a></p>
<p>Docker引擎原理的官方文档介绍<br><a target="_blank" rel="noopener" href="https://docs.docker.com/storage/storagedriver/#images-and-layers">About storage drivers | Docker Documentation</a>  </p>
<p>对Overlay&#x2F;Overlay2具体实现的分析<br><a target="_blank" rel="noopener" href="https://arkingc.github.io/2017/05/05/2017-05-05-docker-filesystem-overlay/">Docker存储驱动—Overlay&#x2F;Overlay2「译」 | Arking</a>  </p>
<h1 id="Docker全系列"><a href="#Docker全系列" class="headerlink" title="Docker全系列"></a>Docker全系列</h1><p><a href="/2019/05/17/%E5%AE%B9%E5%99%A8-1-Namespace%EF%BC%9A%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C">Namespace：楚门的世界</a><br><a href="/2019/05/24/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E">Docker存储引擎</a><br><a href="/2019/05/25/%E5%AE%B9%E5%99%A8-3-Cgroups%E7%9A%84%E8%AE%A1%E5%88%92%E7%BB%8F%E6%B5%8E">Cgroups的计划经济</a><br><a href="/2019/05/27/%E5%AE%B9%E5%99%A8-4-Docker%E7%9A%84%E6%84%8F%E4%B9%89">Docker的意义</a>  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://galaxyyao.github.io/2019/05/24/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/" data-id="clzjpz1yh00088v6od5kz3ltr" data-title="容器-2-Docker存储引擎" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/docker/" rel="tag">docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8/" rel="tag">容器</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-crontab-ntpdate同步失败与Linux环境变量" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/05/23/crontab-ntpdate%E5%90%8C%E6%AD%A5%E5%A4%B1%E8%B4%A5%E4%B8%8ELinux%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/" class="article-date">
  <time class="dt-published" datetime="2019-05-22T16:00:00.000Z" itemprop="datePublished">2019-05-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/05/23/crontab-ntpdate%E5%90%8C%E6%AD%A5%E5%A4%B1%E8%B4%A5%E4%B8%8ELinux%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/">crontab ntpdate同步失败与Linux环境变量</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="问题症状与解决方法"><a href="#问题症状与解决方法" class="headerlink" title="问题症状与解决方法"></a>问题症状与解决方法</h1><p>Oracle服务器由于无法上外网，所以做了个crontab的定时任务，每天定时和内部的一台ntp服务器同步。但没过两周，时间又不准了，差了近10秒。<br>首先排除了这个时间差是在凌晨同步完后的几小时内造成的。以“ntpdate crontab”作为关键字<a target="_blank" rel="noopener" href="https://codingstandards.iteye.com/blog/1611790">搜索</a>，很容易找到了原因：坑爹的crontab重置了PATH环境变量，所以执行ntpdate命令的时候报“command not found”。<br>解决方法也很简单：通过whereis ntpdate的命令查出ntpdate的位置，改为完整路径调用，即“&#x2F;usr&#x2F;sbin&#x2F;ntpdate”。另外作为以防万一，也将同步间隔从1天缩小到1小时。  </p>
<p>为了避免下次栽坑，我们需要知道crontab到底设置了哪些PATH环境变量。<br>通过“vi &#x2F;etc&#x2F;crontab”打开文件，可以看到如下的内容：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">SHELL=/bin/bash</span><br><span class="line">PATH=/sbin:/bin:/usr/sbin:/usr/bin</span><br><span class="line">MAILTO=root</span><br><span class="line"></span><br><span class="line"># For details see man 4 crontabs</span><br><span class="line"></span><br><span class="line"># Example of job definition:</span><br><span class="line"># .---------------- minute (0 - 59)</span><br><span class="line"># |  .------------- hour (0 - 23)</span><br><span class="line"># |  |  .---------- day of month (1 - 31)</span><br><span class="line"># |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...</span><br><span class="line"># |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat</span><br><span class="line"># |  |  |  |  |</span><br><span class="line"># *  *  *  *  * user-name  command to be executed</span><br></pre></td></tr></table></figure>
<p>可以看到PATH里明明是包含&#x2F;usr&#x2F;sbin，也就是ntpdate的路径。那么为什么依然会报“&#x2F;bin&#x2F;sh: ntpdate command not found”的错误？<br>（另外一个疑点是看起来是用&#x2F;bin&#x2F;bash执行的，为什么报&#x2F;bin&#x2F;sh）  </p>
<h1 id="crontab-≠-crontab"><a href="#crontab-≠-crontab" class="headerlink" title="crontab ≠ crontab"></a>crontab ≠ crontab</h1><p>这里是比较容易混淆的地方：</p>
<ul>
<li>crontab -e是<strong>用户任务调度</strong>的命令</li>
<li>&#x2F;etc&#x2F;crontab是<strong>系统任务调度</strong>的配置文件</li>
</ul>
<h3 id="用户任务调度"><a href="#用户任务调度" class="headerlink" title="用户任务调度"></a>用户任务调度</h3><p>每个用户可以通过crontab -e创建自己的定时任务调度。创建的任务会放到&#x2F;var&#x2F;spool&#x2F;cron&#x2F;{用户名}的文件里。<br>系统在启动的时候会由&#x2F;etc&#x2F;init.d启动crond守护进程（<a href="crond(8">参考资料</a>: daemon to execute scheduled commands - Linux man page<br><a target="_blank" rel="noopener" href="https://linux.die.net/man/8/crond)%EF%BC%89%E3%80%82crond%E4%BC%9A%E5%B0%86/var/spool/cron%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84crontab%E6%96%87%E4%BB%B6%E8%BD%BD%E5%85%A5%E5%86%85%E5%AD%98%EF%BC%8C%E5%B9%B6%E5%9C%A8%E6%AF%8F%E5%88%86%E9%92%9F%E6%A3%80%E6%9F%A5%E6%98%AF%E5%90%A6%E6%9C%89%E9%9C%80%E8%A6%81%E6%89%A7%E8%A1%8C%E7%9A%84%E4%BB%BB%E5%8A%A1%E3%80%82%EF%BC%88%E6%88%91%E6%80%80%E7%96%91%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B8%AA%E6%9C%BA%E5%88%B6%E5%AF%BC%E8%87%B4Linux%E7%9A%84%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%9C%80%E5%B0%8F%E9%97%B4%E9%9A%94%E6%98%AF%E5%88%86%E9%92%9F%E8%80%8C%E4%B8%8D%E6%98%AF%E7%A7%92%E3%80%82crond%E7%9A%84%E4%BB%BB%E5%8A%A1%E5%A6%82%E6%9E%9C%E6%89%A7%E8%A1%8C%E5%AE%8C%E6%B2%A1%E5%85%B3%E9%97%AD%E7%9A%84%E8%AF%9D%EF%BC%8C%E4%BC%9A%E6%AE%8B%E7%95%99%E4%B8%8B%E6%9D%A5%E5%BE%88%E5%A4%9Acrond%E8%BF%9B%E7%A8%8B%E3%80%82%E5%A6%82%E6%9E%9C%E4%BB%BB%E5%8A%A1%E5%86%99%E5%BE%97%E6%9C%89%E9%97%AE%E9%A2%98%E7%9A%84%E8%AF%9D%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%8A%8A%E8%BF%9B%E7%A8%8B%E6%95%B0%E9%83%BD%E5%90%83%E5%85%89%EF%BC%8C%E5%AF%BC%E8%87%B4%E6%97%A0%E6%B3%95ssh%E7%99%BB%E5%BD%95%E3%80%82%EF%BC%89">https://linux.die.net/man/8/crond)）。crond会将/var/spool/cron目录下的crontab文件载入内存，并在每分钟检查是否有需要执行的任务。（我怀疑就是这个机制导致Linux的定时任务的最小间隔是分钟而不是秒。crond的任务如果执行完没关闭的话，会残留下来很多crond进程。如果任务写得有问题的话可能会把进程数都吃光，导致无法ssh登录。）</a><br>输出会通过系统内邮件发给对应的用户。  </p>
<p>我们可以通过执行一个“echo $PATH”的定时任务，查看cron的PATH环境变量。结果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PATH=/usr/bin:/bin</span><br></pre></td></tr></table></figure>
<p>没有ntpdate所在的&#x2F;usr&#x2F;sbin目录。这就能解释“command not found”这个报错的原因了。  </p>
<h3 id="系统任务调度"><a href="#系统任务调度" class="headerlink" title="系统任务调度"></a>系统任务调度</h3><p>&#x2F;etc&#x2F;crontab文件也是由crond守护进程扫描并调用的。但差别在于3点：</p>
<ul>
<li>&#x2F;etc&#x2F;crontab文件中额外增加了&#x2F;sbin和&#x2F;usr&#x2F;sbin两个目录</li>
<li>通过&#x2F;bin&#x2F;bash执行</li>
<li>可以执行的用户<br>我理解&#x2F;etc&#x2F;crontab文件的意义在于：  </li>
<li>&#x2F;etc&#x2F;passwd中某些用户是不允许登录的系统账号（例如mysql用户）。虽然也可以通过crontab -u配置，但总不如汇总在一个文件里查看起来方便</li>
<li>方便添加公共环境变量。通过crontab -e执行需要制定环境变量的命令的时候，需要在命令前先添加一个export。而这些环境变量可以直接写在&#x2F;etc&#x2F;crontab文件的顶部</li>
</ul>
<p>想起来之前sudo的时候也在环境变量的时候栽过坑，所以也总结一下环境变量相关知识。  </p>
<h1 id="环境变量层级"><a href="#环境变量层级" class="headerlink" title="环境变量层级"></a>环境变量层级</h1><p>对于每个进程来说，环境变量保存在&#x2F;proc&#x2F;$PID&#x2F;environ这个文件里（$PID是进程号）。每个环境变量的键值对之间是通过\x0这个字符分割的，所以可以通过如下的命令打印当前进程用到的环境变量：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &#x27;s:\x0:\n:g&#x27; /proc/$PID/environ</span><br></pre></td></tr></table></figure>
<p>这些环境变量是由多个层级的环境变量值拼成的。  </p>
<p>环境变量分为几个层级：  </p>
<ul>
<li>全局</li>
<li>用户级（Per User）</li>
<li>会话级（Per Session）</li>
</ul>
<h3 id="全局"><a href="#全局" class="headerlink" title="全局"></a>全局</h3><p>全局的环境变量主要在两个文件里：</p>
<ul>
<li>&#x2F;etc&#x2F;environment：推荐加在这个文件里</li>
<li>&#x2F;etc&#x2F;profile：只针对登录的shell有效<br>此外，bash命令也会自带一些环境变量。&#x2F;etc&#x2F;locale.conf文件里也带有一个LANG&#x3D;”en_US.UTF-8”的环境变量。</li>
</ul>
<p>之前遇到过一些登录SSH可以成功执行的命令，通过gitlab ci无法执行。原因主要也是因为依赖于一些只在&#x2F;etc&#x2F;profile中出现的变量。</p>
<h3 id="用户级"><a href="#用户级" class="headerlink" title="用户级"></a>用户级</h3><p>某个变量可能只有某个用户才需要。这种时候就需要用户级别的。用户级的变量保存在<del>&#x2F;.bashrc和</del>&#x2F;.bash_profile等文件中（<del>表示用户的home目录）。另外也可以看到</del>&#x2F;.bash_profile其实就是调用<del>&#x2F;.bashrc。<br>例如要在用户的PATH里添加一个目录&#x2F;home&#x2F;my_user&#x2F;bin，可以修改</del>&#x2F;.bash_profile文件如下：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=&quot;$&#123;PATH&#125;:/home/my_user/bin&quot;</span><br></pre></td></tr></table></figure>
<p>然后通过source ~&#x2F;.bash_profile命令更新变量。</p>
<h3 id="会话级"><a href="#会话级" class="headerlink" title="会话级"></a>会话级</h3><p>有些时候可能只是想在某次登录会话期间，让环境变量临时生效。这时候就靠export命令：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=&quot;$&#123;PATH&#125;:/home/my_user/tmp/usr/bin&quot;</span><br></pre></td></tr></table></figure>
<p>像pwd命令读取的就是用户当前会话中所在路径。  </p>
<h3 id="sudo"><a href="#sudo" class="headerlink" title="sudo"></a>sudo</h3><p>sudo需要专门拎出来说，是由于之前遇到过一个坑：<br>在一次pip安装lib的时候发生过，有一个命令在root下能成功执行的命令，普通用户sudo执行却会失败。  </p>
<p>最终发现的原因是sudo下的PATH环境变量被重置成一个最小化的子集了。可以用文本编辑器打开&#x2F;etc&#x2F;sudoers文件，找到”secure_path”那一行：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Defaults    secure_path = /sbin:/bin:/usr/sbin:/usr/bin</span><br></pre></td></tr></table></figure>
<p>而那个命令是在&#x2F;usr&#x2F;local&#x2F;bin（忘了还是&#x2F;usr&#x2F;local&#x2F;sbin了）下，自然就执行失败了。<br>更多可以参考这篇：<a target="_blank" rel="noopener" href="https://linux.cn/article-3737-1.html">技术|Linux有问必答：如何为sudo命令定义PATH环境变量</a>。但我不推荐文中修改secure_path的做法。sudo的secure_path这么设置自然有其安全上的考量。改为完整路径调用或通过export临时添加环境变量即可。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>这篇讲环境变量的比较全。虽然是ArchLinux的Wiki，但对于其他Linux的发行版也基本通用。<br><a target="_blank" rel="noopener" href="https://wiki.archlinux.org/index.php/Environment_variables">Environment variables - ArchWiki</a></p>
<p>为什么虚拟机的时钟会产生偏差，这篇文章从原理上解释了原因。（话说Docker就<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/24551592/how-to-make-sure-dockers-time-syncs-with-that-of-the-host">不存在这个问题</a>，容器里想改时间也不能改，要错一起错）<br><a target="_blank" rel="noopener" href="http://www.netis.com/247.html">奔跑在虚拟化大路上的你 请看一看路边的荆棘 - Netis</a>  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://galaxyyao.github.io/2019/05/23/crontab-ntpdate%E5%90%8C%E6%AD%A5%E5%A4%B1%E8%B4%A5%E4%B8%8ELinux%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/" data-id="clzjpz1yg00058v6oc7neg88p" data-title="crontab ntpdate同步失败与Linux环境变量" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-容器-1-Namespace：楚门的世界" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/05/17/%E5%AE%B9%E5%99%A8-1-Namespace%EF%BC%9A%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C/" class="article-date">
  <time class="dt-published" datetime="2019-05-16T16:00:00.000Z" itemprop="datePublished">2019-05-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2019/05/17/%E5%AE%B9%E5%99%A8-1-Namespace%EF%BC%9A%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C/">容器-1-Namespace：楚门的世界</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文是一系列对Docker与Kubernetes的学习总结。源材料来源参见文中的链接和最后的参考资料。  </p>
<h1 id="容器的原理"><a href="#容器的原理" class="headerlink" title="容器的原理"></a>容器的原理</h1><p>虽然有些人会把容器和虚拟机类比，称之为“轻量级的虚拟机”。刚开始接触Docker的时候大多看过下面这张图：<br><img src="/images/%E5%AE%B9%E5%99%A8-1-namespace-%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C/docker-vs-vm-false.png" alt="容器-虚拟机vs容器（伪）"><br>忘记这张图吧。从上一节我们讨论的“容器的意义”就可以看到，容器和虚拟机关注的不是一个层面。  </p>
<p>但虚拟机和容器也有共通之处：本质都是欺骗。虚拟机的原理是欺骗CPU&#x2F;物理内存&#x2F;物理IO，让硬件感觉自己还是在接收宿主机而非虚拟机的指令。而Docker的原理是<strong>欺骗一组进程</strong>，让这些进程以为自己活在另一个的操作系统里。<br>用专业一点的术语，就是：<strong>容器的核心功能，就是通过约束进程和修改进程的动态表现，为进程创造出一组“边界”。</strong>  </p>
<p>以电影作为比方，虚拟机就像《火星救援》里在火星上种土豆的宇航员，在火星上模拟地球的环境，连空气和水都需要自己从头开始制备，成本高昂。<br><img src="/images/%E5%AE%B9%E5%99%A8-1-namespace-%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C/docker-the-martian.jpg" alt="火星救援">    </p>
<p>而容器就是《楚门的世界》。男主（容器进程）生活在一个大型影棚里，所有能接触到的世界只有这个影棚。男主住的房子，吃的东西，呼吸的空气来自于地球，生产成本很低。但他所能阅读的报纸，乘坐的载具，观看的电视都是影棚工作人员提供给他的，受到了严格的限制。<br><img src="/images/%E5%AE%B9%E5%99%A8-1-namespace-%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C/docker-the-truman-show.jpg" alt="楚门的世界">  </p>
<p>对于Docker来说，修改进程的动态表现是通过<strong>Namespace</strong>，构成进程依赖的文件系统是通过<strong>union mount</strong>，约束进程是通过<strong>Cgroups</strong>。  </p>
<h1 id="Namespace-欺骗的6种手段"><a href="#Namespace-欺骗的6种手段" class="headerlink" title="Namespace-欺骗的6种手段"></a>Namespace-欺骗的6种手段</h1><p>我们先想一下，对于一台宿主机上的虚拟机，它能看到和接触到的哪些信息必须和宿主机不一样？除了上一节提到的文件系统之外还能列出不少吧。虚拟化技术是通过Hypervisor + Guest OS实现的。<br>而对于容器，要实现同样的效果是通过Namespace。  </p>
<h4 id="进程号不一样"><a href="#进程号不一样" class="headerlink" title="进程号不一样"></a>进程号不一样</h4><p>对于Linux系统来说，有几个进程是ID固定的：</p>
<ul>
<li>idle进程：pid&#x3D;0，系统创建的第一个进程，内核态</li>
<li>init进程：pid&#x3D;1，由0进程创建，用户态，系统中所有其它用户进程的祖先进程</li>
<li>kthreadd进程：pid&#x3D;2，管理和调度其他内核线程</li>
</ul>
<p>《道德经》有云：道生一，一生二，二生三，三生万物。对于Linux进程要改一下，0生1和2，1和2生万物。<br><img src="/images/%E5%AE%B9%E5%99%A8-1-namespace-%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C/linux-processes.png" alt="Network Namespace">   </p>
<p>肯定不能让Docker容器接触到init进程（pid&#x3D;1），不然容器就能为所欲为了。但对于Docker里其他进程来说，如果自己不是由pid&#x3D;1的进程创建的，欺骗就出现了严重的漏洞。<br>一山不容二虎，进程号不可重复。不可能创建出两个PID&#x3D;1的init进程。所以需要将一个fork出来的普通进程伪装成PID1的init进程，并骗容器里的其他进程相信这点。  </p>
<p><strong>通过PID NameSpace可以实现进程号唯一和进程视图隔离</strong>  </p>
<p>下面是实际一个docker容器启动时，在容器里打印出来的进程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                    </span><br><span class="line">  1 root      20   0   11820   1892   1512 S   0.0  0.0   0:00.15 bash                                                                                                       </span><br><span class="line"> 22 root      20   0   56212   2060   1452 R   0.0  0.0   0:00.01 top                                                                                                        </span><br></pre></td></tr></table></figure>
<p>而在宿主机用pstree打印出来的进程树如下：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemd─┬─containerd─┬─containerd-shim─┬─bash───top</span><br><span class="line">        │            │                 └─10*[&#123;containerd-shim&#125;]</span><br><span class="line">        │            └─14*[&#123;containerd&#125;]</span><br></pre></td></tr></table></figure>
<p>可以看到对于容器来说，bash是PID&#x3D;1的init进程。在这棵进程树上，containerd-shim可以理解为容器的pid&#x3D;0进程（当然实际上依然是用户态进程所以还是有很大差别）。<br>至于为什么宿主机的init进程名是systemd，涉及sysvint和systemd的争议，在这里就不提了。有兴趣的话可以参考<a target="_blank" rel="noopener" href="https://coolshell.cn/articles/17998.html">这篇</a>。在这里你可以认为systemd是当前版本CentOS上的init进程实现。  </p>
<h4 id="主机名不一样"><a href="#主机名不一样" class="headerlink" title="主机名不一样"></a>主机名不一样</h4><p>每个容器最好有网络的独立性。这个包括主机名唯一，以及ip和端口不冲突等。先说主机名。  </p>
<p>每个Docker容器的主机名等同于容器ID，用这种方式确保唯一。（同一个局域网上hostname重复其实也没大关系，但能做到唯一总更好一些吧）  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@269111b56ccd /]# hostname</span><br><span class="line">269111b56ccd</span><br></pre></td></tr></table></figure>
<p><strong>UTS NameSpace可以实现主机名唯一</strong>  </p>
<h4 id="ip和端口不冲突"><a href="#ip和端口不冲突" class="headerlink" title="ip和端口不冲突"></a>ip和端口不冲突</h4><p>每块网卡一个ip。每个容器有一个自己的ip，那么就要靠虚拟网卡veth。<br>具体的架构可以参见下图：<br><img src="/images/%E5%AE%B9%E5%99%A8-1-namespace-%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C/docker-namespace-network.png" alt="Network Namespace">  </p>
<p>所有容器的虚拟机网卡通过bridge桥接到宿主机的网卡上。<br>我们可以在宿主机上打印出网络接口信息。其中的docker0就是桥接网卡，而veth开头的就是容器的虚拟网卡。  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@mobilesit network-scripts]$ip li</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:50:56:95:9d:68 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">...</span><br><span class="line">6: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default </span><br><span class="line">    link/ether 02:42:c5:76:e1:b5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">8: vethf362a04@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default </span><br><span class="line">    link/ether 5e:c3:81:ae:96:9f brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br></pre></td></tr></table></figure>
<p>在容器里打印网络接口信息，除了lo这个本地环回接口（localhost）之外，就是虚拟网卡eth0了。  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@269111b56ccd /]# ip li</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">7: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default </span><br><span class="line">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br></pre></td></tr></table></figure>

<p>有了虚拟网卡，端口映射也是小事一碟了。<br><strong>Network NameSpace可以实现独立虚拟网卡</strong>  </p>
<h4 id="容器间不可随意通信"><a href="#容器间不可随意通信" class="headerlink" title="容器间不可随意通信"></a>容器间不可随意通信</h4><p>IPC是Linux下进程间通信的一种方式。两个独立容器间的进程应该是被隔离的，传小纸条这种行为需要被严格禁止。<br>微服务间通信是靠HTTP请求和RPC，而进程间通信靠的是共享内存、信号量、消息队列等。  </p>
<p><strong>IPC NameSpace可以阻隔容器间通信</strong>  </p>
<h4 id="不同容器的同名用户互相独立"><a href="#不同容器的同名用户互相独立" class="headerlink" title="不同容器的同名用户互相独立"></a>不同容器的同名用户互相独立</h4><p>我们期望在容器中可以任意创建用户，但不至于影响到宿主机。也期望在两个容器里建立同名用户的时候不互相影响。  </p>
<p>在说到Docker的实现方式之前，先要提一下UID(User Identifier，用户id)和GID(Group Identifier，组id)。它们分别是用户和组在全系统里的唯一标识。整个Linux系统共用一套内核，内核只维护一套uid和gid。<br>但朱丽叶也说过：“What’s in a name? That which we call a rose by ant other word would smell as sweet.”。同一个uid可以在不同的容器和宿主机之间显示不同的名字。<br>我们可以做个实现：在Dockerfile里加上USER参数，以Daemon启动容器。然后分别在宿主机和容器查看进程。<br>宿主机的结果是：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@mobilesit 12687]$ps -ef|grep sleep</span><br><span class="line">polkitd  14126 14109  0 12:10 ?        00:00:00 sleep infinity</span><br><span class="line">root     14680   786  0 12:39 ?        00:00:00 sleep 60</span><br><span class="line">root     14720 14234  0 12:39 pts/3    00:00:00 grep --color=auto sleep</span><br></pre></td></tr></table></figure>
<p>而容器里的结果是：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">testuser     1     0  0 04:10 ?        00:00:00 sleep infinity</span><br><span class="line">testuser    14     0  0 04:39 pts/0    00:00:00 /bin/bash</span><br><span class="line">testuser    19    14  0 04:40 pts/0    00:00:00 ps -ef</span><br></pre></td></tr></table></figure>
<p>一个是polkitd（Linux控制全局权限的Daemon进程），一个是testuser，看上去名字不相同。<br>但我们再查询一下uid信息看看。宿主机的结果：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uid=999(polkitd) gid=998(polkitd) groups=998(polkitd)</span><br></pre></td></tr></table></figure>
<p>而容器里的结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uid=999(testuser) gid=998(testuser) groups=998(testuser)</span><br></pre></td></tr></table></figure>
<p>uid和gid完全相同，可见从本质上是同一个用户。  </p>
<p>PS. 实验的时候还有三个未解的疑问，待以后有空再深入研究了。  </p>
<ul>
<li>如果用useradd在容器里创建用户，容器里能查到uid为1000的用户（如果继续建立，就会从1001开始累加）。但宿主机上的&#x2F;etc&#x2F;passwd上查不到uid&#x3D;1000的用户。不知道是不是对&#x2F;etc&#x2F;passwd隐藏了。</li>
<li>资料上说进程里通过&#x2F;proc&#x2F;<PID>&#x2F;uid_map做宿主机和容器间的uid映射。但我从dockerd到containerd-shim，各种进程的uid_map文件都找过了。只看到了root用户的映射，没找到testuser的映射。</li>
<li>在两个容器里分别建立了两个用户（无论是否同名），uid都是1000。但把其中一个容器中的用户删除后，另外一个容器里的用户不受影响。我理解删除的只是映射，但不确定的是如果给用户提权，不知道会不会两个容器都受到影响。</li>
</ul>
<p><strong>User NameSpace可以实现容器和宿主机之间的用户映射</strong>  </p>
<h4 id="挂载文件系统隔离"><a href="#挂载文件系统隔离" class="headerlink" title="挂载文件系统隔离"></a>挂载文件系统隔离</h4><p>每个容器的文件挂载之间应该是互相独立的。当某个容器里执行了挂载，我们期望其他容器不会也看到这个挂载点。  </p>
<p>容器的实现方式是在每个进程中独立维护挂载信息。实际是维护在&#x2F;proc&#x2F;<PID>&#x2F;mounts，&#x2F;proc&#x2F;<PID>&#x2F;mountinfo和&#x2F;proc&#x2F;<PID>&#x2F;mountstats这三个文件中。  </p>
<p><strong>Mount NameSpace可以隔离挂载信息</strong>  </p>
<p>我们会在下一章更详细地介绍Mount Namespace是怎么和Docker的存储引擎配合，创造出每个容器内独立的文件系统。  </p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Docker里分别使用6种Namespace实现了各种资源的隔离，包括：  </p>
<table>
<thead>
<tr>
<th>Namespace</th>
<th>系统调用参数</th>
<th>隔离的资源</th>
</tr>
</thead>
<tbody><tr>
<td>PID</td>
<td>CLONE_NEWPID</td>
<td>进程号</td>
</tr>
<tr>
<td>UTS</td>
<td>CLONE_NEWUTS</td>
<td>主机名与域名</td>
</tr>
<tr>
<td>Network</td>
<td>CLONE_NEWNET</td>
<td>网络设备、网络栈、端口等等</td>
</tr>
<tr>
<td>IPC</td>
<td>CLONE_NEWIPC</td>
<td>信号量、消息队列和共享内存</td>
</tr>
<tr>
<td>User</td>
<td>CLONE_NEWUSER</td>
<td>用户和用户组</td>
</tr>
<tr>
<td>Mount</td>
<td>CLONE_NEWNS</td>
<td>挂载点（文件系统）</td>
</tr>
</tbody></table>
<p>对于容器的隔离性其实有不少需要考虑的。随便举个例子，比如系统时间和时区。如果在容器里改了系统时间，是不是宿主机也会受到影响？这些细节就待有兴趣或有需求的时候再来研究了。  </p>
<p>另外在看了一部分Kubernetes后回来补充：容器之间的Namespace隔离也不是定死的。Kubernetes Pod内部的容器之间就可以共享Network Namespace，PID Namespace和IPC Namespace等。  </p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>docker进程号的动手实验<br><a target="_blank" rel="noopener" href="http://shareinto.github.io/2019/01/30/docker-init(1)/">谁是Docker容器的init(1)进程 | shareinto</a></p>
<p>更详细了解Docker Network Namespace的实现可以参考下文<br><a target="_blank" rel="noopener" href="https://www.shangyang.me/2017/01/08/docker-namespace-network/">Docker 原理篇（七）Docker network namespace | 伤神的博客</a></p>
<h1 id="Docker全系列"><a href="#Docker全系列" class="headerlink" title="Docker全系列"></a>Docker全系列</h1><p><a href="/2019/05/17/%E5%AE%B9%E5%99%A8-1-Namespace%EF%BC%9A%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C">Namespace：楚门的世界</a><br><a href="/2019/05/24/%E5%AE%B9%E5%99%A8-2-Docker%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E">Docker存储引擎</a><br><a href="/2019/05/25/%E5%AE%B9%E5%99%A8-3-Cgroups%E7%9A%84%E8%AE%A1%E5%88%92%E7%BB%8F%E6%B5%8E">Cgroups的计划经济</a><br><a href="/2019/05/27/%E5%AE%B9%E5%99%A8-4-Docker%E7%9A%84%E6%84%8F%E4%B9%89">Docker的意义</a>  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://galaxyyao.github.io/2019/05/17/%E5%AE%B9%E5%99%A8-1-Namespace%EF%BC%9A%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C/" data-id="clzjpz1yg00048v6od7wza0mm" data-title="容器-1-Namespace：楚门的世界" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/docker/" rel="tag">docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8/" rel="tag">容器</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">&laquo; 上一页</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/">下一页 &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CI/" rel="tag">CI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Feign/" rel="tag">Feign</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IE/" rel="tag">IE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IaC/" rel="tag">IaC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/" rel="tag">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jenkins/" rel="tag">Jenkins</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Maven/" rel="tag">Maven</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MyBatis/" rel="tag">MyBatis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySQL/" rel="tag">MySQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nginx/" rel="tag">Nginx</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PostgreSQL/" rel="tag">PostgreSQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spring/" rel="tag">Spring</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spring-Cloud/" rel="tag">Spring Cloud</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Windows/" rel="tag">Windows</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cloud/" rel="tag">cloud</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/k8s/" rel="tag">k8s</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kubernetes/" rel="tag">kubernetes</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pgsql/" rel="tag">pgsql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%B8%AD%E5%8F%B0/" rel="tag">中台</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%B9%E5%99%A8/" rel="tag">容器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/" rel="tag">持续集成</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%90%E7%BB%B4/" rel="tag">运维</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%87%91%E8%9E%8D/" rel="tag">金融</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/" rel="tag">项目管理</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/CI/" style="font-size: 10px;">CI</a> <a href="/tags/Feign/" style="font-size: 10px;">Feign</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IaC/" style="font-size: 10px;">IaC</a> <a href="/tags/Java/" style="font-size: 16.67px;">Java</a> <a href="/tags/Jenkins/" style="font-size: 10px;">Jenkins</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Maven/" style="font-size: 10px;">Maven</a> <a href="/tags/MyBatis/" style="font-size: 10px;">MyBatis</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Nginx/" style="font-size: 10px;">Nginx</a> <a href="/tags/PostgreSQL/" style="font-size: 10px;">PostgreSQL</a> <a href="/tags/Spring/" style="font-size: 10px;">Spring</a> <a href="/tags/Spring-Cloud/" style="font-size: 13.33px;">Spring Cloud</a> <a href="/tags/Windows/" style="font-size: 10px;">Windows</a> <a href="/tags/cloud/" style="font-size: 10px;">cloud</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/k8s/" style="font-size: 18.33px;">k8s</a> <a href="/tags/kubernetes/" style="font-size: 18.33px;">kubernetes</a> <a href="/tags/pgsql/" style="font-size: 10px;">pgsql</a> <a href="/tags/%E4%B8%AD%E5%8F%B0/" style="font-size: 10px;">中台</a> <a href="/tags/%E5%AE%B9%E5%99%A8/" style="font-size: 20px;">容器</a> <a href="/tags/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/" style="font-size: 11.67px;">持续集成</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 10px;">数据库</a> <a href="/tags/%E8%BF%90%E7%BB%B4/" style="font-size: 11.67px;">运维</a> <a href="/tags/%E9%87%91%E8%9E%8D/" style="font-size: 10px;">金融</a> <a href="/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/" style="font-size: 10px;">项目管理</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">八月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">三月 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/08/07/Terraform-IaC-%E5%AD%A6%E4%B9%A0Tips/">Terraform IaC 学习Tips</a>
          </li>
        
          <li>
            <a href="/2024/08/05/Resume%E4%BB%A5%E5%8F%8AARTS/">Resume以及ARTS</a>
          </li>
        
          <li>
            <a href="/2020/08/01/%E9%87%91%E8%9E%8D%E4%B8%9A%E5%8A%A1-%E4%B8%8D%E5%8A%A8%E4%BA%A7%E4%BF%9D%E7%90%86%E4%B8%9A%E5%8A%A1%E5%85%A5%E9%97%A8%E5%92%8C%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/">金融业务-不动产保理业务入门和系统设计</a>
          </li>
        
          <li>
            <a href="/2020/07/30/%E8%BF%90%E7%BB%B4-%E8%BF%90%E7%BB%B4%E4%BD%93%E7%B3%BB%E6%A0%87%E5%87%86%E5%8C%96%E4%B9%8B%E6%95%85%E9%9A%9C%E7%AE%A1%E7%90%86/">运维-运维体系标准化之故障管理</a>
          </li>
        
          <li>
            <a href="/2020/07/29/%E8%BF%90%E7%BB%B4-%E8%BF%90%E8%90%A5%E4%BD%93%E7%B3%BB%E6%A0%87%E5%87%86%E5%8C%96%E4%B9%8B%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86CMDB/">运维-运营体系标准化之配置管理CMDB</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 姚皓(Hao Yao)<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>